{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgvaGj2P7wU9"
   },
   "source": [
    "# GENERACIÓN DE TEXTO MEDIANTE UNA RED NEURONAL RECURRENTE DE TIPO LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trWtV1mf77zC"
   },
   "source": [
    "Un modelo LSTM es un tipo especial de redes recurrentes, se tiene en cuenta que las RNN tiene como característica principal que pueden persistir la información introduciendo bucles en el diagrama de la red, por lo que básicamente, pueden recordar estados previos y hacer uso de ellos para decidir cuál será el siguiente. Las LSTM tienen la ventaja de que pueden aprender dependencias largas, por lo que se podría decir que tienen una memoria a más largo plazo.\n",
    "\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del dataset\n",
    "\n",
    "\n",
    "El dataset empleado para este trabajo consiste en una compilación de juegos infantiles, los cuales serán la base para el entrenamiento y futura predicción por parte de la RNN. El archivo se encuentra con una extensión de texto plano, teniendo una combinación de datos alfanuméricos.\n",
    "\n",
    "Datasets de este tipo son empleados para entrenamiento de redes neuronales que tienen como finalidad el Procesamiento de Lenguaje Natural (o NLP por sus siglas en ingles), el cual estudia el uso de computadoras para la interpretación y análisis del lenguaje natural. Las aplicaciones del NLP son amplias, por ejemplo, a partir de varios blogs que hablan sobre el coronavirus, se puede aplicar aprendizaje automático para descubrir subtemas sobre los mismos artículos.\n",
    "\n",
    "- Ubicación del dataset: https://raw.githubusercontent.com/carlos-paezf/Deep_Learning/master/Tercer_Corte/datasets/cuentos_infantiles.txt\n",
    "- Cantidad de caracteres: 55.517\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO1t_J7g8Bio"
   },
   "source": [
    "## Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZPycRNm07wOR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "szMJ6CJh7vkY"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvwJGF5m8qkk"
   },
   "source": [
    "## Descarga y preprocesado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc1olDzf9odd"
   },
   "source": [
    "Obtener el dataset, en este caso es un archivo con cuentos infantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiEqfK8g7kQn",
    "outputId": "4a5337b7-b895-40f5-922e-a46f77f41f27"
   },
   "outputs": [],
   "source": [
    "fileDL = keras.utils.get_file('cuentos_infantiles.txt', 'https://raw.githubusercontent.com/carlos-paezf/Deep_Learning/master/Tercer_Corte/datasets/cuentos_infantiles.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1snGfv69xIR"
   },
   "source": [
    "Abrir el archivo en modo de `rb`, es decir modo de lectura binaria, luego se decodifica a un tipo `latin-1` para el alfabeto latino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mclmdtMf9L9G"
   },
   "outputs": [],
   "source": [
    "text = open(fileDL, 'rb').read().decode(encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGhwMcCd-KoW"
   },
   "source": [
    "Convertir el texto a minusculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LT0Y9O-19WWv"
   },
   "outputs": [],
   "source": [
    "raw_text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ENqIbTMT9eMC",
    "outputId": "0b5f6796-98bc-4657-87f2-3fb4289e08c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  caperucita roja\n",
      "habia una vez una dulce niã±a que queria mucho a su madre y a su abuela. les ayudaba en todo lo que podia y como era tan buena el dia de su cumpleaã±os su abuela le regalo una caperuza roja. como le gustaba tanto e iba con ella a todas partes, pronto todos empezaron a llamarla caperucita roja.\n",
      "un dia la abuela de caperucita, que vivia en el bosque, enfermo y la madre de caperucita le pidio que le llevara una cesta con una torta y un tarro de mantequilla. caperucita acepto encantada.\n",
      "  ten mucho cuidado caperucita, y no te entretengas en el bosque.\n",
      "  â¡si mama!\n",
      "la niã±a caminaba tranquilamente por el bosque cuando el lobo la vio y se acerco a ella.\n",
      "  â¿donde vas caperucita?\n",
      "  a casa de mi abuelita a llevarle esta cesta con una torta y mantequilla.\n",
      "  yo tambien queria ir a verlaâ¦. asi que, â¿por que no hacemos una carrera? tu ve por ese camino de aqui que yo ire por este otro.\n",
      "  â¡vale!\n",
      "el lobo mando a caperucita por el camino mas largo y llego antes que ella a casa de la abuelita. de modo que se hizo pasar por la pequeã±a y llamo a la puerta. aunque lo que no sabia es que un cazador lo habia visto llegar.\n",
      "  â¿quien es?, contesto la abuelita\n",
      "  soy yo, caperucita   dijo el lobo\n",
      "  que bien hija mia. pasa, pasa\n",
      "el lobo entro, se abalanzo sobre la abuelita y se la comio de un bocado. se puso su camison y se metio en la cama a esperar a que llegara caperucita.\n",
      "la pequeã±a se entretuvo en el bosque cogiendo avellanas y flores y por eso tardo en llegar un poco mas. al llegar llamo a la puerta.\n",
      "  â¿quien es?, contesto el lobo tratando de afinar su voz\n",
      "  soy yo, caperucita. te traigo una torta y un tarrito de mantequilla.\n",
      "  que bien hija mia. pasa, pasa\n",
      "cuando caperucita entro encontro diferente a la abuelita, aunque no supo bien porque.\n",
      "  â¡abuelita, que ojos mas grandes tienes!\n",
      "  si, son para verte mejor hija mia\n",
      "  â¡abuelita, que orejas tan grandes tienes!\n",
      "  claro, son para oirte mejorâ¦\n",
      "  pero abuelita, â¡que dientes mas grandes tienes!\n",
      "  â¡â¡son para comerte mejor!!\n",
      "en cuanto dijo esto el lobo se lanzo sobre caperucita y se la comio tambien. su estomago estaba tan lleno que el lobo se quedo dormido.\n",
      "                  hombre de jengibre\n",
      "erase una vez, una mujer viejecita que vivia en una casita vieja en la cima de una colina, rodeada de huertas doradas, bosques y arroyos. a la vieja le encantaba hornear, y un dia de navidad decidio hacer un hombre de jengibre. formo la cabeza y el cuerpo, los brazos y las piernas. agrego pasas jugosas para los ojos y la boca, y una fila en frente para los botones en su chaqueta. luego puso un caramelo para la nariz. al fin, lo puso en el horno.\n",
      "la cocina se lleno del olor dulce de especias, y cuando el hombre de jengibre estaba crujiente, la vieja abrio la puerta del horno. el hombre de jengibre salto del horno, y salio corriendo, cantando:\n",
      "  â¡corre, corre, tan pronto como puedas! no puedes alcanzarme. â¡soy el hombre de jengibre!\n",
      "la vieja corrio, pero el hombre de jengibre corrio mas rapido. el hombre de jengibre se encontro con un pato que dijo\n",
      "  â¡cua, cua! â¡hueles delicioso! â¡quiero comerte!\n",
      "pero el hombre de jengibre siguio corriendo. el pato lo persiguio balanceandose, pero el hombre de jengibre corrio mas rapido. cuando el hombre de jengibre corrio por las huertas doradas, se encontro con un cerdo que cortaba paja. el cerdo dijo:\n",
      "  â¡para, hombre de jengibre! â¡quiero comerte!\n",
      "pero el hombre de jengibre siguio corriendo. el cerdo lo persiguio brincando, pero el hombre de jengibre corrio mas rapido. en la sombra fresca del bosque, un cordero estaba picando hojas. cuando vio al hombre de jengibre, dijo:\n",
      "  â¡bee, bee! â¡para, hombre de jengibre! â¡quiero comerte!\n",
      "pero el hombre de jengibre siguio corriendo. el cordero lo persiguio saltando, pero el hombre de jengibre corrio mas rapido. mas alla, el hombre de jengibre podia ver un rio ondulante. miro hacia atras sobre el hombro y vio a todos los que estaban persiguiendole:\n",
      "  â¡paa! â¡paa!   exclamo la vieja\n",
      "  â¡cua, cua!   grazno el pato\n",
      "  â¡oink! â¡oink!   gruã±o el cerdo\n",
      "  â¡bee! â¡bee!   balo el cordero\n",
      "pero el hombre de jengibre se rio y continuo hacia el rio. al lado del rio, vio a un zorro. le dijo al zorro:\n",
      "  he huido de la vieja y el pato y el cerdo y el cordero. â¡puedo huir de ti tambien! â¡corre, corre, tan pronto como puedas! no puedes alcanzarme. â¡soy el hombre de jengibre!\n",
      "pero el zorro astuto sonrio y dijo:\n",
      "  espera, hombre de jengibre. â¡soy tu amigo! te ayudare a cruzar el rio. â¡echate encima de la cola!\n",
      "el hombre de jengibre echo un vistazo hacia atras y vio a la vieja, al pato, al cerdo y al cordero acercandose. se echo encima de la cola sedosa del zorro, y el zorro salio nadando en el rio. a mitad de camino, el zorro le pidio que se echara sobre su espalda para que no se mojara. y asi lo hizo. despues de unas brazadas mas, el zorro dijo:\n",
      "  hombre de jengibre, el agua es aun mas profunda. â¡echate encima de la cabeza!\n",
      "  â¡ja, ja! nunca me alcanzaran ahora rio el hombre de jengibre.\n",
      "  â¡tienes la razon! chillo el zorro.\n",
      "el zorro echo atras la cabeza, tiro al hombre de jengibre en el aire, y lo dejo caer en la boca. con un crujido fuerte, el zorro comio al hombre de jengibre.\n",
      "la vieja regreso a casa y decidio hornear un pastel de jengibre en su lugar.\n",
      "â¿te ha gustado este cuento? â¿sabias que existe una version en ingles, llamada gingerbread man? â¡no dejes de contarsela a tus hijos.\n",
      " pinocho\n",
      "erase una vez, un carpintero llamado gepetto que decidio construir un muã±eco de madera, al que llamo pinocho. con el, consiguio no sentirse tan solo como se habia sentido hasta aquel momento.\n",
      "  â¡que bien me ha quedado!  exclamo una vez acabado de construir y de pintar . â¡como me gustaria que tuviese vida y fuese un niã±o de verdad!\n",
      "como habia sido muy buen hombre a lo largo de la vida, y sus sentimientos eran sinceros. un hada decidio concederle el deseo y durante la noche dio vida a pinocho.\n",
      "al dia siguiente, cuando gepetto se dirigio a su taller, se llevo un buen susto al oir que alguien le saludaba:\n",
      "  â¡hola papa!  dijo pinocho.\n",
      "  â¿quien habla?  pregunto gepetto.\n",
      "  soy yo, pinocho. â¿no me conoces? â le pregunto.\n",
      "gepetto se dirigio al muã±eco.\n",
      "  â¿eres tu? â¡parece que estoy soã±ando!, â¡por fin tengo un hijo!\n",
      "gepetto queria cuidar a su hijo como habria hecho con cualquiera que no fuese de madera. pinocho tenia que ir al colegio, aprender y conocer a otros niã±os. pero el carpintero no tenia dinero, y tuvo que vender su abrigo para poder comprar una cartera y los libros.\n",
      "a partir de aquel dia, pinocho empezo a ir al colegio con la compaã±ia de un grillo, que le daba buenos consejos. pero, como la mayoria de los niã±os, pinocho preferia ir a divertirse que ir al colegio a aprender, por lo que no siempre hacia caso del grillo.\n",
      "un dia, pinocho se fue al teatro de titeres para escuchar una historia. cuando le vio, el dueã±o del teatro quiso quedarse con el:\n",
      " â¡oh, un titere que camina por si mismo, y habla! con el en la compaã±ia, voy a hacerme rico dijo el titiritero, pensando que pinocho le haria ganar mucho dinero.\n",
      "a pesar de las recomendaciones del pequeã±o grillo, que le decia que era mejor irse de alli, pinocho decidio quedarse en el teatro, pensando que asi podria ganar dinero para comprar un abrigo nuevo a gepetto, que habia vendido el suyo para comprarle los libros.\n",
      "y asi hizo, durante todo el dia estuvo actuando para el titiritero. pasados unos dias, cuando queria volver a casa, el dueã±o del teatro de marionetas le dijo que no podia irse, que tenia que quedarse con el.\n",
      "pinocho se echo a llorar tan desconsolado diciendo que queria volver a casa que el malvado titiritero lo encerro en una jaula para que no pudiera escapar.\n",
      "por suerte, su hada madrina que todo lo sabe, aparecio durante la noche y lo libero de su cautiverio abriendo la puerta de la jaula con su varita magica. antes de irse, pinocho tomo de encima de la mesa las monedas que habia ganado actuando.\n",
      "de vuelta a casa pinocho volvio a tener las prejas normales, cuando de repente, el grillo y pinocho, se cruzaron con dos astutos ladrones que convencieron al niã±o de que si enterraba las monedas en un campo cercano, llamado el \"campo de los milagros\", el dinero se multiplicaria y se haria rico.\n",
      "confiando en los dos hombres, y sin escuchar al grillo que le advertia del engaã±o, pinocho enterro las monedas y se fue. rapidamente, los dos ladrones se llevaron las monedas y pinocho tuvo que volver a casa sin monedas.\n",
      "durante los dias que pinocho habia estado fuera, gepetto se habia puesto muy triste y, preocupado, habia salido a buscarle por todos los rincones. asi, cuando pinocho y el grillo llegaron a casa, se encontraron solos. por suerte, el hada que habia convertido a pinocho en niã±o, les explico que el carpintero habia salido direccion al mar para buscarles.\n",
      "pinocho y grillo decidieron ir a buscarle, pero se cruzaron con un grupo de niã±os:\n",
      "  â¿donde vais?  pregunto pinocho.\n",
      "  al pais de los juguetes   respondio un niã±o . â¡alli podremos jugar sin parar! â¿quieres venir con nosotros?\n",
      "  â¡oh, no, no, no!  le advirtio el grillo . recuerda que tenemos que encontrar a gepetto, que esta triste y preocupado por ti.\n",
      "  â¡solo un rato!  dijo pinocho  despues seguimos buscandole.\n",
      "y pinocho se fue con los niã±os, seguido del grillo que intentava seguir convenciendole de continuar buscando al carpintero. pinocho jugo y brinco todo lo que quiso. enseguida se olvido de gepetto, solo pensaba en divertirse y seguir jugando. pero a medida que pasaba mas y mas horas en el pais de los juguetes, pinocho se iba convirtiendo en un burro. cuando se dio cuenta de ello se echo a llorar. al oirle, el hada se compadecio de el y le devolvio su aspecto, pero le advirtio:\n",
      "  a partir de ahora, cada vez que mientas te crecera la nariz.\n",
      "pinocho y el grillo salieron rapidamente en busca de gepetto.\n",
      "geppetto, que habia salido en busca de su hijo pinocho en un pequeã±o bote de vela, habia sido tragado por una enorme ballena.\n",
      "entonces pinocho y el grillito, desesperados, se hicieron a la mar para rescatar al pobre ancianito papa de pinocho.\n",
      "cuando pinocho estuvo frente a la ballena le pidio porfavor que le devolviese a su papa, pero la enorme ballena abrio muy grande la boca y se lo trago tambien a el.\n",
      "â¡por fin geppetto y pinocho estaban nuevamente juntos!, ahora debian pensar como conseguir salir de la barriga de la ballena.\n",
      "  â¡ya se, dijo pepito hagamos una fogata! el fuego hizo estornudar a la enorme ballena, y la balsa salio volando con sus tres tripulantes.\n",
      "una vez a salvo pinocho le conto todo lo sucedido a gepetto y le pidio perdon. a gepetto, a pesar de haber sufrido mucho los ultimos dias, solo le importaba volver a tener a su hijo con el. por lo que le propuso que olvidaran todo y volvieran a casa.\n",
      "pasado un tiempo, pinocho demostro que habia aprendido la leccion y se portaba bien: iba al colegio, escuchaba los consejos del grillo y ayudaba a su padre en todo lo que podia.\n",
      "como recompensa por su comportamiento, el hada decidio convertir a pinocho en un niã±o de carne y hueso. a partir de aquel dia, pinocho y gepetto fueron muy felices como padre e hijo.\n",
      " la cenicienta\n",
      "hubo una vez, hace mucho, mucho tiempo una joven muy bella, tan bella que no hay palabras para describirla. se llamaba cenicienta\n",
      "cenicienta era pobre, no tenia padres y vivia con su madrastra, una mujer viuda muy cascarrabias que siempre estaba enfadada y dando ordenes gritos a todo el mundo.\n",
      "con la madrastra tambien vivian su dos hijas, que eran muy feas e insoportables.cenicienta era la que hacia los trabajos mas duros de la casa, como por ejemplo\n",
      "limpiar la chimenea cada dia, por lo que sus vestidos siempre estaban sucios o manchados de ceniza, por eso las personas del lugar la llamaban cenicienta. cenicienta apenas tenia amigos, solo a dos ratoncitos muy simpaticos que vivian en un agujero de la casa.\n",
      "un buen dia, sucedio algo inesperado; el rey de aquel lugar hizo saber a todos los habitantes de la region que invitaba a todas las chicas jovenes a un gran baile que se celebraba en el palacio real.\n",
      "el motivo del baile era encontrar una esposa para el hijo del rey; el principe! para casarse con ella y convertirla en princesa.\n",
      "la noticia llego a los oidos de cenicienta y se puso muy contenta. por unos instantes soã±o con que seria ella, la futura mujer del principe. la princesa!\n",
      "pero, por desgracia, las cosas no serian tan faciles para nuestra amiga cenicienta\n",
      "la madrastra de cenicienta le dijo en un tono malvado y cruel:   tu cenicienta, no iras al baile del principe, porque te quedaras aqui en casa fregando el suelo, limpiando el carbon y ceniza de la chimenea y preparando la cena para cuando nosotras volvamos.\n",
      "cenicienta esa noche lloro en su habitacion, estaba muy triste porque ella queria ir al baile y conocer al principe.\n",
      "al cabo de unos dias llego la esperada fecha: el dia del baile en palacio\n",
      "cenicienta veia como sus hermanastras se arreglaban y se intentaban poner guapas y bonitas, pero era imposible, porque eran muy feas de tan malas que eranpero sus vestidoseran muy bonitos!\n",
      "al llegar la noche, su madrasta y hermanastras partieron hacia el palacio real, y cenicienta, sola en casa, una vez mas se puso a llorar de tristeza.\n",
      "entre llanto y llanto, dijo en voz alta:   â¿por que sere tan desgraciada? por favor, si hay algun ser magico que pueda ayudarme.. decia cenicienta con desesperacion.\n",
      "de pronto, sucedio algo increible; se le aparecio un hada madrina muy buena y muy poderosa.\n",
      "y con voz suave, tierna y muy agradable le dijo a cenicienta;   no llores mas, te ayudare.\n",
      "de verdad ? dijo cenicienta un poco incredulapero como vas a ayudarme ? no tengo ningun vestido bonito para ir al baile y mis zapatos estan todos rotos!\n",
      "la hada madrina saco su varita magica y con ella toco suavemente a cenicienta, y al momentooh!, que milagro! un maravilloso vestido aparecio en el cuerpo de cenicienta, asi como tambien unos preciosos zapatos.\n",
      "ahora ya puedes ir al baile de palacio cenicienta, peroten en cuenta una cosa muy importante: tu vestido a las 12 de la noche volvera a ser los arapos que llevas ahora.\n",
      "hay algo mas que debes saber, delante de la casa te espera un carruaje que te llevara al gran baile en palacio, pero a las 12 de la noche, se transformara en una calabaza!. bien, dijo cenicienta, ya soy feliz, solo por poder ir al baile.\n",
      "cuando cenicienta llego al palacio, causo mucha impresion a todos los asistentes, nadie nunca habia visto tanta belleza, cenicienta estaba preciosa!\n",
      "el principe, no tardo en darse cuenta de la presencia de esa joven tan bonita. se dirigio hacia ella y le pregunto si queria bailar.\n",
      "cenicienta, dijo si!, claro que si! y estuvieron bailando durante horas y horas\n",
      "las hermanastras de cenicienta no la reconocieron, debido a que ella siempre iba sucia y llena de ceniza, incluso se preguntaban quien seria aquella chica tan preciosa.\n",
      "pero de repenteoh!, dijo cenicienta, son casi las 12 de la noche, mi vestido esta a punto de convertirse en una ropa sucia, y el carruaje se transformara en una calabaza!\n",
      "  â¡oh, dios mio! â¡tengo que irme! le dijo al principe que estaba en sus brazos bailando.\n",
      "salio a toda prisa del salon de baile bajo la escalinata hacia la salida de palacio perdiendo en su huida un zapato, que el principe encontro y recogio.\n",
      "a partir de ese momento, el principe ya sabia quien iba a ser la futura princesa la joven que habia perdido el zapato!, pero..caramba!, exclamo el principe, pero si no se ni como se llama, y mucho menos donde vive!\n",
      "para encontrar a la bella joven, el principe ideo un plan. se casaria con aquella que pudiera calzarse el zapato.\n",
      "envio a sus sirvientes a recorrer todo el reino. todas las jovenes, chicas y mujeres se probaban el zapato, pero no habia ni una a que pudiera calzarse el zapato.\n",
      "al cabo de unas semanas, los sirvientes de palacio llegaron a casa de cenicienta.\n",
      "la madrastra llamo a sus feas hijas para que probasen el zapato, pero evidentemente no pudieron calzar el zapato.\n",
      "uno de los sirvientes del principe vio a cenicienta en un rincon de la casa, y exclamo:  eh!, tu tambien tienes que provarte el zapato!\n",
      "la madrastra y sus hijas dijeron:  por favor!, como quiere usted que cenicienta sea la chica que busca el principe?, ella es pobre, siempre esta sucia y no fue a la fiesta de palacio!\n",
      "pero cuando cenicienta se puso el zapato y le encajo a la perfecciontodos los presentes se quedaron de piedra!,  oooh!, es ella! la futura princesa!\n",
      "inmediatamente la llevaron a palacio y a los pocos dias se caso con el principe, por lo que fue una princesa!\n",
      "nunca mas volvio con su madrastra, vivio feliz en palacio hasta el ultimo de sus dias.\n",
      " robbin hodd\n",
      "erase una vez un valiente joven que habitaba en el bosque de sherwood, en nottingham, llamado robin hood.  junto con su mejor amigo, little john, pasaban los dias robando a los ricos para darselo a los pobres. robin hood era conocido por ser el mejor arquero del reino de inglaterra y por ser tambien una persona justa y bondadosa que ayudaba a aquellos que mas lo necesitaban. tambien era conocido por todos que el corazon del arquero era de lady mariam, por quien suspiraba mientras esperaba el momento oportuno para pedirle la mano.\n",
      "el valiente rey ricardo corazon de leon se encontraba luchando en las cruzadas y, en su ausencia, su hermano, el principe juan, era el que se sentaba en el trono. juan era famoso por su gran avaricia y su crueldad. trataba mal al pueblo y cobraba elevados impuestos, sobre todo a los mas pobres. la mayoria ya no tenian con que pagar pero el principe era implacable y seguia recorriendo sus tierras, junto con el sheriff de nottingham, para obligar a los campesinos a pagar los impuestos. la gente de los pueblos se moria de hambre y robin hood y sus hombres no podian permitirlo. \n",
      "todo aquel que se enteraba de que el principe juan habia salido a recaudar las riquezas de los pobres campesinos iba rapidamente a decirselo a robin. este preparaba una emboscada en alguno de los caminos del bosque de sherwood y le robaba a juan todo lo recaudado. durante unos dias se escondian y, cuando el peligro habia pasado, volvian y repartian entre los pobres los distintos tesoros. robin cuidaba de las gentes de nottingham y ellos lo apreciaban y protegian por ello.\n",
      "enojado por los continuos robos que estaba sufriendo, el principe quiso tenderles una trampa. con este proposito organizo un concurso de tiro con arco, cuyo premio seria entregado por lady mariam. robin hood  no pudo resistir la tentacion y se presento. para evitar ser descubierto y detenido lo hizo disfrazado, de esa forma participo sin ser identificado. haciendo honor a su fama y su habilidad gano el concurso. una vez declarado ganador desvelo su verdadera identidad para burlarse del principe. y, aunque este hizo lo imposible por intentar atraparlo, finalmente escapo audazmente ayudado por sus amigos.\n",
      "en venganza, el principe juan, dio orden de captura contra los amigos de robin. el sheriff de nottingham consiguio capturar a algunos, que serian ejecutados por alta traicion el dia siguiente por la maã±ana. el principe sabia que robin hood intentaria salvarles, asi que planeo una emboscada para deshacerse de el de una vez por todas. pero robin estaba al tanto de la trampa, asi que elaborando un astuto plan el y little john penetraron en la fortaleza sin ser vistos. cuando estaban a punto de ejecutar a los presos en la orca entro en accion, cortando las sogas con una flecha liberando a sus amigos, quienes se abrieron paso luchando contra los guardias, venciendolos  y liberando a otros prisioneros que se unieron a ellos en la lucha contra las injusticias del principe juan. en la huida nuestro heroe se vio rodeado por muchos guardias, pero logro escapar con mucha habilidad y reunirse mas tarde con sus compaã±eros.\n",
      "durante un tiempo los bosques estuvieron llenos de gente que vivia en ellos. no podian volver a sus casas porque el sheriff  habia puesto precio a sus cabezas. pero un dia, por fin, el rey ricardo regreso y fue informado de como se habia comportado su hermano, el principe juan. el honorable rey restablecio unos impuestos justos, dejo en libertad a todos aquellos que se encontraban presos y termino con la persecucion contra robin y sus amigos. el principe juan, el sheriff de nottingham y todos aquellos que los habian ayudado, fueron encarcelados por sus abusos continuados contra la humilde poblacion de inglaterra.\n",
      "cuando todo volvio a la calma, robin, pudo, por fin, tomar por esposa a lady mariam. se casaron en una hermosa fiesta en la que el rey ricardo acompaã±o a la joven al altar. todos juntos celebraron la felicidad de la nueva pareja y el final de la tirania del principe juan.\n",
      " los tres cerditos\n",
      "habia una vez tres hermanos cerditos que vivian en el bosque. como el malvado lobo siempre los estaba persiguiendo para comerselos dijo un dia el mayor:\n",
      "  tenemos que hacer una casa para protegernos de lobo. asi podremos escondernos dentro de ella cada vez que el lobo aparezca por aqui.\n",
      "a los otros dos les parecio muy buena idea, pero no se ponian de acuerdo respecto a que material utilizar. al final, y para no discutir, decidieron que cada uno la hiciera de lo que quisiese.\n",
      "el mas pequeã±o opto por utilizar paja, para no tardar mucho y poder irse a jugar despues.\n",
      "el mediano prefirio construirla de madera, que era mas resistente que la paja y tampoco le llevaria mucho tiempo hacerla. pero el mayor penso que aunque tardara mas que sus hermanos, lo mejor era hacer una casa resistente y fuerte con ladrillos.\n",
      "  ademas asi podre hacer una chimenea con la que calentarme en invierno, penso el cerdito.\n",
      "cuando los tres acabaron sus casas se metieron cada uno en la suya y entonces aparecio por ahi el malvado lobo. se dirigio a la de paja y llamo a la puerta:\n",
      "  anda cerdito se bueno y dejame entrar...\n",
      "  â¡no! â¡eso ni pensarlo!\n",
      "  â¡pues soplare y soplare y la casita derribare!\n",
      "y el lobo empezo a soplar y a estornudar, la debil casa acabo viniendose abajo. pero el cerdito echo a correr y se refugio en la casa de su hermano mediano, que estaba hecha de madera.\n",
      "  anda cerditos sed buenos y dejarme entrar...\n",
      "  â¡no! â¡eso ni pensarlo!, dijeron los dos\n",
      "  â¡pues soplare y soplare y la casita derribare!\n",
      "el lobo empezo a soplar y a estornudar y aunque esta vez tuvo que hacer mas esfuerzos para derribar la casa, al final la madera acabo cediendo y los cerditos salieron corriendo en direccion hacia la casa de su hermano mayor.\n",
      "el lobo estaba cada vez mas hambriento asi que soplo y soplo con todas sus fuerzas, pero esta vez no tenia nada que hacer porque la casa no se movia ni siquiera un poco. dentro los cerditos celebraban la resistencia de la casa de su hermano y cantaban alegres por haberse librado del lobo:\n",
      " â¿quien teme al lobo feroz? â¡no, no, no!\n",
      "fuera el lobo continuaba soplando en vano, cada vez mas enfadado. hasta que decidio parar para descansar y entonces reparo en que la casa tenia una chimenea.\n",
      "  â¡ja! â¡pensaban que de mi iban a librarse! â¡subire por la chimenea y me los comere a los tres!\n",
      "pero los cerditos le oyeron, y para darle su merecido llenaron la chimenea de leã±a y pusieron al fuego un gran caldero con agua.\n",
      "asi cuando el lobo cayo por la chimenea el agua estaba hirviendo y se pego tal quemazo que salio gritando de la casa y no volvio a comer cerditos en una larga temporada.\n",
      " la bella y la bestia\n",
      "habia una vez un mercader adinerado que tenia tres hijas. las tres eran muy hermosas, pero lo era especialmente la mas joven, a quien todos llamaban desde pequeã±a bella. ademas de bonita, era tambien bondadosa y por eso sus orgullosas hermanas la envidiaban y la consideraban estupida por pasar el dia tocando el piano y rodeada de libros.\n",
      "sucedio que repentinamente el mercader perdio todo cuanto tenia y no le quedo nada mas que una humilde casa en el campo. tuvo que trasladarse alli con sus hijas y les dijo que no les quedaba mas remedio que aprender a labrar la tierra. las dos hermanas mayores se negaron desde el primer momento mientras que bella se enfrento con determinacion a la situacion:\n",
      "  llorando no conseguire nada, trabajando si. puedo ser feliz aunque sea pobre.\n",
      "asi que bella era quien lo hacia todo. preparaba la comida, limpiaba la casa, cultivaba la tierra y hasta encontraba tiempo para leer. sus hermanas, lejos de estarle agradecidas, la insultaban y se burlaban de ella.\n",
      "llevaban un aã±o viviendo asi cuando el mercader recibio una carta en la que le informaban de que un barco que acababa de arribar traia mercancias suyas. al oir la noticias las hijas mayores solo pensaron en que podrian recuperar su vida anterior y se apresuraron a pedirle a su padre que les trajera caros vestidos. bella en cambio, solo pidio a su padre unas sencillas rosas ya que por alli no crecia ninguna.\n",
      "pero el mercader apenas pudo recuperar sus mercancias y volvio tan pobre como antes. cuando no le quedaba mucho para llegar hasta la casa, se desato una tormenta de aire y nieve terrible. estaba muerto de frio y hambre y los aullidos de los lobos sonaban cada vez mas cerca. entonces, vio una lejana luz que provenia de un castillo.\n",
      "al llegar al castillo entro dentro y no encontro a nadie. sin embargo, el fuego estaba encendido y la mesa rebosaba comida. tenia tanta hambre que no pudo evitar probarla.\n",
      "se sintio tan cansado que encontro un aposento y se acosto en la cama. al dia siguiente encontro ropas limpias en su habitacion y una taza de chocolate caliente esperandole. el hombre estaba seguro de que el castillo tenia que ser de un hada buena.\n",
      "a punto estaba de marcharse y al ver las rosas del jardin recordo la promesa que habia hecho a bella. se dispuso a cortarlas cuando sono un estruendo terrible y aparecio ante el una bestia enorme.\n",
      "  â¿asi es como pagais mi gratitud?\n",
      "  â¡lo siento! yo solo pretendiaâ¦ son para una de mis hijasâ¦\n",
      "  â¡basta! os perdonare la vida con la condicion de que una de vuestras hijas me ofrezca la suya a cambio. ahora â¡iros!\n",
      "el hombre llego a casa exhausto y apesadumbrado porque sabia que seria la ultima vez que volveria a ver a sus tres hijas.\n",
      "entrego las rosas a bella y les conto lo que habia sucedido. las hermanas de bella comenzaron a insultarla, a llamarla caprichosa y a decirle que tenia la culpa de todo.\n",
      "  ire yo, dijo con firmeza\n",
      "  â¿como dices bella?, pregunto el padre\n",
      "  he dicho que sere yo quien vuelva al castillo y entregue su vida a la bestia. por favor padre.\n",
      "cuando bella llego al castillo se asombro de su esplendor. mas aun cuando encontro escrito en una puerta âaposento de bellaâ y encontro un piano y una biblioteca. pero se sento en su cama y deseo con tristeza saber que estaria haciendo su padre en aquel momento. entonces levanto la vista y vio un espejo en el que se reflejaba su casa y a su padre llegando a ella.\n",
      "bella empezo a pensar que la bestia no era tal y que era en realidad un ser muy amable.\n",
      "esa noche bajo a cenar y aunque estuvo muy nerviosa al principio, fue dandose cuenta de lo humilde y bondadoso que era la bestia.\n",
      "  si hay algo que deseeis no teneis mas que pedirmelo, dijo la bestia.\n",
      "con el tiempo, bella comenzo a sentir afecto por la bestia. se daba cuenta de lo mucho que se esforzaba en complacerla y todos los dias descubria en el nuevas virtudes. pero pese a eso, cuando todos los dias la bestia le preguntaba si queria ser su esposa ella siempre contestaba con honestidad:\n",
      " la bella y la bestia lo siento. sois muy bueno conmigo pero no creo que pueda casarme con vos.\n",
      "la bestia pese a eso no se enfadaba sino que lanzaba un largo suspiro y desaparecia.\n",
      "un dia bella le pidio a la bestia que le dejara ir a ver a su padre, ya que habia caido enfermo. la bestia no puso ningun impedimento y solo le pidio que por favor volviera pronto si no queria encontrarselo muerto de tristeza.\n",
      "  no dejare que mueras bestia. te prometo que volvere en ocho dias, dijo bella.\n",
      "bella estuvo en casa de su padre durante diez dias. pensaba ya en volver cuando soã±o con la bestia yaciendo en el jardin del castillo medio muerta.\n",
      "regreso de inmediato al castillo y no lo vio por ninguna parte. recordo su sueã±o y lo encontro en el jardin. la pobre bestia no habia podido soportar estar lejos de ella.\n",
      "  no os preocupeis. muero tranquilo porque he podido veros una vez mas.\n",
      "  â¡no! â¡no os podeis morir! â¡sere vuestra esposa!\n",
      "entonces una luz maravillosa ilumino el castillo, sonaron las campanas y estallaron fuegos artificiales. bella se dio la vuelta hacia la bestia y, â¿donde estaba? en su lugar habia un apuesto principe que le sonreia dulcemente.\n",
      "  gracias bella. habeis roto el hechizo. un hada me condeno a vivir con esta forma hasta que encontrase a una joven capaz de amarme y casarse conmigo y vos lo habeis hecho.\n",
      "el principe se caso con bella y ambos vivieron juntos y felices durante muchos muchos aã±os.\n",
      " peter pan\n",
      "hace tiempo, alla por 1880, vivia en la ciudad de londres la familia darling. estaba formada por el seã±or y la seã±ora darling y sus hijos: wendy, michael y john. sin olvidarnos de nana, por supuesto, el perro niã±era.\n",
      "vivian felices y tranquilos hasta que peter pan llego a sus vidas. todo comenzo la noche en que nana tenia el dia libre y la seã±ora darling se quedo a cargo de sus hijos. cuando todos, incluida ella, estuvieron dormidos el muchacho entro por la ventana. pero entonces ella se desperto y se asusto tanto al verle que lanzo un fuerte grito. entonces aparecio nana, que cerro la ventana para evitar que saliera y acabo atrapando su sombra. y asi fue como la sombra de peter pan acabo en un cajon de la casa de los darling.\n",
      "una noche el seã±or y la seã±ora darling salieron a cenar a casa de los vecinos del numero 27. los niã±os se quedaron en casa al cuidado de nana y no tardaron en quedarse todos dormidos.\n",
      "pero cuando la casa estaba en silencio, entro una diminuta hada revoloteando a gran velocidad y tras ella, peter pan, dispuesto a recuperar su sombra. la encontro en el cajon en el que la habia guardado nana pero se entristecio mucho cuando comprobo que la sombra no le seguia. probo a pegarsela con jabon pero no dio resultado y desesperado se sento en el suelo a llorar.\n",
      "  â¿quien esta llorando?   pregunto wendy, a quien despertaron los sollozos.\n",
      "  soy yo   contesto peter\n",
      "  â¿como te llamas?   pregunto la niã±a, aunque ella estaba casi segura de saber quien era\n",
      "  peter pan\n",
      "  â¿y que te pasa peter?\n",
      "  que no consigo que mi sombra se me quede pegada\n",
      "  tranquilo. creo que podre cosertela\n",
      "wendy ayudo a peter y mientras los dos niã±os comenzaron a hacerse amigos.\n",
      "  yo vivo en el pais de nunca jamas. es maravilloso, alli eres siempre un niã±o y no tienes que obedecer a nadie. conmigo viven los niã±os perdidos, ya sabes, los niã±os que caen de los carritos cuando la niã±era mira a otro lado. ademas hay piratas, hadas, indios y toda clase de seres.\n",
      "peter decia que era muy feliz alli aunque reconocio que a el y a los niã±os perdidos les gustaria que hubiese alguien que les contara cuentos como hacia ella con sus hermanos. peter le propuso ir con el al pais de nunca jamas y a wendy le parecio de inmediato una idea maravillosa.\n",
      "  pero, â¿y mis hermanos? â¿pueden venir ellos tambien?\n",
      "  si tu quieres, â¡claro!\n",
      "  â¡estupendo!\n",
      "wendy desperto a michael y john y peter para iniciar su viaje. pero antes de partir peter les explico que debian aprender a volar. les echo un poco de polvo de hada por encima y enseguida los tres niã±os comenzaron a elevarse por el aire. a todos les parecio muy divertido y comenzaron a dar vueltas y mas vueltas por la casa. armaron tal revuelo que acabaron despertando a nana.\n",
      "peter la oyo venir asi que pudieron volver a sus camas rapidamente como si no hubiese pasado nada. asi, cuando la niã±era entro en la habitacion creyo que los tres dormian placidamente.\n",
      "pero nana estaba intranquila y estaba casi segura de que algo raro estaba ocurriendo en el cuarto de los niã±os, de modo que corrio a avisar a los seã±ores darling. pero cuando volvieron, los niã±os ya no estaban. los tres habian partido rumbo a nunca jamas nerviosos e ilusionados por vivir aquella fantastica aventura.\n",
      "volaron durante dias atravesando oceanos pero al final llegaron al pais de nunca jamas.\n",
      "al primero que vieron desde el aire fue al temible capitan garfio, el peor enemigo de peter pan. en una lucha hacia tiempo peter habia logrado arrebatarle la mano derecha y por eso el pirata llevaba en su lugar ahora un garfio. pero lo manejaba perfectamente y eso, unido a sus ganas de venganza, lo hacian muy peligroso. aunque habia algo a lo que el capitan garfio tenia miedo: el cocodrilo. una vez estuvo a punto de comerselo y por eso ahora no queria otra cosa que no fuese el. menos mal que el capitan le arrojo un reloj y por eso ahora hacia tic tac cada vez que se acercaba.\n",
      "llegaron hasta el lugar donde estaban los niã±os perdidos. pero campanilla, que estaba muy celosa de wendy porque estaba todo el tiempo junto a peter, se adelanto para tramar algo.\n",
      "  peter dice que ataqueis a wendy   le dijo a los niã±os perdidos.\n",
      "  â¡de acuerdo!   contestaron todos al unisono corriendo a por sus arcos y flechas\n",
      "asi que los niã±os comenzaron a disparar sus arcos y flechas hacia wendy y sus hermanos. pero afortunadamente no les paso nada.\n",
      "en cuanto llego peter detras de todos les echo una gran bronca.\n",
      "  â¿pero que haceis? â¡encima que os traigo a una madre para que os cuente cuentos la recibis asi!\n",
      "los niã±os perdidos, que iban vestidos con las pieles de los osos que cazaban, se disculparon y peter les presento a wendy y a los demas.\n",
      "  estos son tootles, slightly, nibs, curly y los gemelos\n",
      "  hola   contesto la muchacha   estos son mis hermanos michael y john y yo soy wendy.\n",
      "wendy y sus hermanos decidieron quedarse alli y junto con los niã±os perdidos y peter formaron una gran familia que vivia feliz en su guarida subterranea.\n",
      "un dia estaban los niã±os jugando en la laguna de las sirenas, concretamente en la roca de los desamparados, cuando sucedio algo extraã±o. de repente el sol desaparecio por completo, se hizo de noche y entre las sombras aparecio un bote con dos de los piratas de garfio, smee y starkey, que llevaban como prisionera a la princesa india tigridia. peter, wendy y los demas se escondieron y vieron como arrojaban a tigridia sobre la roca de los desamparados. entonces a peter se le ocurrio una idea.\n",
      "  â¡soltadla!   dijo a los piratas imitando la voz del capitan garfio\n",
      "  â¿capitan?   dijeron los dos piratas mirando a todos los lados\n",
      "  â¡ya me habeis oido! â¡hacedlo!\n",
      "asi que los piratas cortaron las cuerdas que apresaban a la princesa. entonces aparecio por la laguna el capitan garfio a bordo de su barco. iba para contarles que sabia que los niã±os perdidos habian encontrado una madre y de ninguna manera podian permitirlo.\n",
      "  los raptaremos, los obligaremos a lanzarse por la borda y wendy se convertira en nuestra madre.\n",
      "  â¡si! â¡es una idea estupenda capitan!, contestaron smee y starkey\n",
      "wendy se quedo palida al oir aquello y peter, que no aguanto mas callado, de nuevo imito la voz de garfio. pero esta vez el pirata fue mas listo que en otras ocasiones y supo que se trataba de peter pan. lo encontro y lucho contra el hasta que logro herirlo con su garfio, mientras los niã±os escapaban en el bote. wendy se salvo gracias a la ayuda de las sirenas y a la cometa que michael habia perdido unos dias antes y que aparecio por alli, mientras que peter logro sobrevivir gracias a la ayuda de la pajara de nunca jamas.\n",
      "aquella aventura hizo que peter se hiciera muy amigo de los indios pieles rojas, pues le estaban agradecidos por haber salvado a la princesa tigridia y prometieron defenderlo con sus arcos y flechas del ataque de los piratas.\n",
      "una noche estaba wendy contando a los niã±os su cuento de antes de ir a dormir cuando hablo de las madres, de lo buenas y atentas que son con sus hijos. peter no estuvo de acuerdo con las ideas de wendy y discutio con ella y al mismo tiempo los hermanos de la muchacha empezaron a sentir nostalgia por lo que entre todos decidieron que habia llegado el momento de volver a casa.\n",
      "  nos iremos esta misma noche   contesto tajante wendy\n",
      "los niã±os perdidos se sintieron muy tristes al oir esto y decidieron que se irian con ella. no asi peter, que de ninguna manera queria abandonar el pais de nunca jamas. al menos se preocupo porque campanilla y los pieles rojas acompaã±aran a los niã±os por el bosque en su camino de vuelta a inglaterra.\n",
      "peter panpero en su camino de vuelta surgieron nuevas complicaciones. los piratas estaban al corriente de que iban a pasar por alli y los esperaban encaramados a los arboles del bosque. los niã±os, y tampoco peter pan, se podian esperar algo asi, asi que los cogieron desprevenidos.\n",
      "mientras tanto garfio acudio a la guarida secreta de peter, donde el muchacho pasaba el tiempo en soledad haciendo ver que no le importaba haberse quedado solo. el pirata y se escondio dentro de un tronco y espero a que peter se durmiera para echar en un vaso que tenia el muchacho junto a su cama un poco del veneno secreto y mortal que siempre llevaba consigo. esta vez conseguiria acabar con el.\n",
      "pero en mitad de la noche campanilla llego para contarle a peter lo ocurrido y advertirle de que sabia que el capitan garfio le habia echado veneno en su vaso. asi que peter salio veloz con sus armas dispuesto a rescatar a los niã±os.\n",
      "peter llego hasta el barco de los piratas, el jolly roger, un barco siniestro en el que los niã±os estaban a punto de ser obligados a saltar por la pasarela al mar.\n",
      "los piratas estaban atando a wendy al palo mayor en ese momento cuando de repente sono algo que nadie esperaba... tic  tac, tic tac, tic tac...\n",
      "  â¡es ese maldito cocodrilo! â¡rapido smee escondeme! â¡no dejes que me coja!  grito garfio preso del panico\n",
      "pero alli no habia ningun cocodrilo, era peter, que habilmente se habia hecho pasar por el. en cuanto garfio fue a su camarote a esconderse peter aparecio en la cubierta del barco de un salto y empezo a acabar con los piratas uno por uno. pero desde sus aposentos garfio dejo de oir el tic tac y creyo que el cocodrilo habia huido y podia salir de nuevo.\n",
      "al salir garfio se encontro con varios piratas muertos. nadie sabia que habia ocurrido exactamente asi que todos empezaron a pensar que el barco estaba maldito pues ya se sabe que los piratas son algo supersticiosos. estaban a punto de lanzar a wendy por la borda convencidos de que era ella quien atraia a la mala suerte, cuando peter salio de su escondrijo para evitarlo.\n",
      "  â¡joven descarado, preparate para morir! â dijo garfio\n",
      "  â¡de eso nada maldito capitan garfio! â¡no es mi hora sino la tuya!   contesto el valiente peter pan\n",
      "se enzarzaron en una violenta lucha de espadas y al final garfio acabo gravemente herido en las costillas, tanto, que no vio salida y decidio lanzarse por la borda sin saber que el cocodrilo lo estaba\n",
      " el patito feo\n",
      "todos esperaban en la granja el gran acontecimiento. el nacimiento de los polluelos de mama pata. llevaba dias empollandolos y podian llegar en cualquier momento.\n",
      "el dia mas caluroso del verano mama pata escucho de repenteâ¦â¡cuac, cuac! y vio al levantarse como uno por uno empezaban a romper el cascaron. bueno, todos menos uno.\n",
      "  â¡eso es un huevo de pavo!, le dijo una pata vieja a mama pata.\n",
      "  no importa, le dare un poco mas de calor para que salga.\n",
      "pero cuando por fin salio resulto que ser un pato totalmente diferente al resto. era grande y feo, y no parecia un pavo. el resto de animales del corral no tardaron en fijarse en su aspecto y comenzaron a reirse de el.\n",
      "  â¡feo, feo, eres muy feo!, le cantaban\n",
      "su madre lo defendia pero pasado el tiempo ya no supo que decir. los patos le daban picotazos, los pavos le perseguian y las gallinas se burlaban de el. al final su propia madre acabo convencida de que era un pato feo y tonto.\n",
      "  â¡vete, no quiero que estes aqui!\n",
      "el pobre patito se sintio muy triste al oir esas palabras y escapo corriendo de alli ante el rechazo de todos.\n",
      "acabo en una cienaga donde conocio a dos gansos silvestres que a pesar de su fealdad, quisieron ser sus amigos, pero un dia aparecieron alli unos cazadores y acabaron repentinamente con ellos. de hecho, a punto estuvo el patito de correr la misma suerte de no ser porque los perros lo vieron y decidieron no morderle.\n",
      "  â¡soy tan feo que ni siquiera los perros me muerden!  penso el pobre patito.\n",
      "continuo su viaje y acabo en la casa de una mujer anciana que vivia con un gato y una gallina. pero como no fue capaz de poner huevos tambien tuvo que abandonar aquel lugar. el pobre sentia que no valia para nada.\n",
      "un atardecer de otoã±o estaba mirando al cielo cuando contemplo una bandada de pajaros grandes que le dejo con la boca abierta. el no lo sabia, pero no eran pajaros, sino cisnes.\n",
      "  â¡que grandes son! â¡y que blancos! sus plumas parecen nieve .\n",
      "deseo con todas sus fuerzas ser uno de ellos, pero abrio los ojos y se dio cuenta de que seguia siendo un animalucho feo.\n",
      "tras el otoã±o, llego el frio invierno y el patito paso muchas calamidades. un dia de mucho frio se metio en el estanque y se quedo helado. gracias a que paso por alli un campesino, rompio el frio hielo y se lo llevo a su casa el patito siguio vivo. estando alli vio que se le acercaban unos niã±os y creyo que iban a hacerle daã±o por ser un pato tan feo, asi que se asusto y causo un revuelo terrible hasta que logro escaparse de alli.\n",
      "eel patito feol resto del invierno fue duro para el pobre patito. solo, muerto de frio y a menudo muerto de hambre tambien. pero a pesar de todo logro sobrevivir y por fin llego la primavera.\n",
      "una tarde en la que el sol empezaba a calentar decidio acudir al parque para contemplar las flores, que comenzaban a llenarlo todo. alli vio en el estanque dos de aquellos pajaros grandes y blancos y majestuosos que habia visto una vez hace tiempo. volvio a quedarse hechizado mirandolos, pero esta vez tuvo el valor de acercarse a ellos.\n",
      "volo hasta donde estaban y entonces, algo llamo su atencion en su reflejo. â¿donde estaba la imagen del pato grande y feo que era? â¡en su lugar habia un cisne! entonces eso queria decir queâ¦ â¡se habia convertido en cisne! o mejor dicho, siempre lo habia sido.\n",
      "desde aquel dia el patito tuvo toda la felicidad que hasta entonces la vida le habia negado y aunque escucho muchos elogios alabando su belleza, el nunca acabo de acostumbrarse.\n",
      " el gato con botas\n",
      "habia una vez un molinero pobre que cuando murio solo pudo dejar a sus hijos por herencia el molino, un asno y un gato. en el reparto el molino fue para el mayor, el asno para el segundo y el gato para el mas pequeã±o. este ultimo se lamento de su suerte en cuanto supo cual era su parte.\n",
      "  â¿y ahora que hare? mis hermanos trabajaran juntos y haran fortuna, pero yo solo tengo un pobre gato.\n",
      "el gato, que no andaba muy lejos, le contesto:\n",
      "  no os preocupeis mi seã±or, estoy seguro de que os sere mas valioso de lo que pensais.\n",
      "  â¿ah si? â¿como?, dijo el amo incredulo\n",
      "  dadme un par de botas y un saco y os lo demostrare.\n",
      "el amo no acababa de creer del todo en sus palabras, pero como sabia que era un gato astuto le dio lo que pedia.\n",
      "el gato fue al monte, lleno el saco de salvado y de trampas y se hizo el muerto junto a el. inmediatamente cayo un conejo en el saco y el gato puso rumbo hacia el palacio del rey.\n",
      "  buenos dias majestad, os traigo en nombre de mi amo el marques de carabas   pues este fue el nombre que primero se le ocurrio   este conejo.\n",
      "  muchas gracias gato, dadle las gracias tambien al seã±or marques de mi parte.\n",
      "al dia siguiente el gato cazo dos perdices y de nuevo fue a ofrecerselas al rey, quien le dio una propina en agradecimiento.\n",
      "los dias fueron pasando y el gato continuo durante meses llevando lo que cazaba al rey de parte del marques de carabas.\n",
      "un dia se entero de que el monarca iba a salir al rio junto con su hija la princesa y le dijo a su amo:\n",
      "  haced lo que os digo amo. acudid al rio y baã±aos en el lugar que os diga. yo me encargare del resto.\n",
      "el amo le hizo caso y cuando paso junto al rio la carroza del rey, el gato comenzo a gritar diciendo que el marques se ahogaba. al verlo, el rey ordeno a sus guardias que lo salvaran y el gato aprovecho para contarle al rey que unos forajidos habian robado la ropa del marques mientras se baã±aba. el rey, en agradecimiento por los regalos que habia recibido de su parte mando rapidamente que le llevaran su traje mas hermoso. con el puesto, el marques resultaba especialmente hermoso y la princesa no tardo en darse cuenta de ello. de modo que el rey lo invito a subir a su carroza para dar un paseo.\n",
      "el gato se coloco por delante de ellos y en cuanto vio a un par de campesinos segando corrio hacia ellos.\n",
      "  buenas gentes que segais, si no decis al rey que el prado que estais segando pertenece al seã±or marques de carabas, os haran picadillo como carne de pastel.\n",
      "los campesinos hicieron caso y cuando el rey paso junto a ellos y les pregunto de quien era aquel prado, contestaron que del marques de carabas.\n",
      "siguieron camino adelante y se cruzaron con otro par de campesinos a los que se acerco el gato.\n",
      "  buenas gentes que segais, si no decis al rey que todos estos trigales pertenecen al seã±or marques de carabas, os haran picadillo como carne de pastel.\n",
      "y en cuanto el rey pregunto a los segadores, respondieron sin dudar que aquellos campos tambien eran del marques.\n",
      "cel gato con botasontinuaron su paseo y se encontraron con un majestuoso castillo. el gato sabia que su dueã±o era un ogro asi que fue a hablar con el.\n",
      "  he oido que teneis el don de convertiros en cualquier animal que deseeis. â¿es eso cierto?\n",
      "  pues claro. vereis como me convierto en leon\n",
      "y el ogro lo hizo. el pobre gato se asusto mucho, pero siguio adelante con su habil plan.\n",
      "  ya veo que estan en lo cierto. pero seguro que no sois capaces de convertiros en un animal muy pequeã±o como un raton.\n",
      "  â¿ah no? â¡mirad esto!\n",
      "el ogro cumplio su palabra y se convirtio en un raton, pero entonces el gato fue mas rapido, lo cazo de un zarpazo y se lo comio.\n",
      "asi, cuando el rey y el marques llegaron hasta el castillo no habia ni rastro del ogro y el gato pudo decir que se encontraban en el estupendo castillo del marques de carabas.\n",
      "el rey quedo fascinado ante tanto esplendor y acabo pensando que se trataba del candidato perfecto para casarse con su hija.\n",
      "el marques y la princesa se casaron felizmente y el gato solo volvio a cazar ratones para entretenerse.\n",
      " blancanieves\n",
      "un dia de invierno la reina miraba como caian los copos de nieve mientras cosia. le cautivaron de tal forma que se despisto y se pincho en un dedo dejando caer tres gotas de la sangre mas roja sobre la nieve. en ese momento penso:\n",
      "  como desearia tener una hija asi, blanca como la nieve, sonrosada como la sangre y de cabellos negros como el ebano.\n",
      "al cabo de un tiempo su deseo se cumplio y dio a luz a una niã±a bellisima, blanca como la nieve, sonrosada como la sangre y con los cabellos como el ebano. de nombre le pusieron blancanieves, aunque su nacimiento supuso la muerte de su madre.\n",
      "pasados los aã±os el rey viudo decidio casarse con otra mujer. una mujer tan bella como envidiosa y orgullosa. tenia esta un espejo magico al que cada dia preguntaba:\n",
      "  espejito espejito, contestadme a una cosa â¿no soy yo la mas hermosa?\n",
      "y el espejo siempre contestaba:\n",
      "  si, mi reina. vos sois la mas hermosa.\n",
      "pero el dia en que blancanieves cumplio siete aã±os el espejo cambio su respuesta:\n",
      "  no, mi reina. la mas hermosa es ahora blancanieves.\n",
      "al oir esto la reina monto en colera. la envidia la comia por dentro y tal era el odio que sentia por ella que acabo por ordenar a un cazador que la llevara al bosque, la matara y volviese con su corazon para saber que habia cumplido con sus ordenes.\n",
      "pero una vez en el bosque el cazador miro a la joven y dulce blancanieves y no fue capaz de hacerlo. en su lugar, mato a un pequeã±o jabali que pasaba por alli para poder entregar su corazon a la reina.\n",
      "blancanieves se quedo entonces sola en el bosque, asustada y sin saber donde ir. comenzo a correr hasta que cayo la noche. entonces vio luz en una casita y entro en ella.\n",
      "era una casita particular. todo era muy pequeã±o alli. en la mesa habia colocados siete platitos, siete tenedores, siete cucharas, siete cuchillos y siete vasitos. blancanieves estaba tan hambrienta que probo un bocado de cada plato y se sento como pudo en una de las sillitas.\n",
      "estaba tan agotada que le entro sueã±o, entonces encontro una habitacion con siete camitas y se acurruco en una de ellas.\n",
      "bien entrada la noche regresaron los enanitos de la mina, donde trabajaban excavando piedras preciosas. al llegar se dieron cuenta rapidamente de que alguien habia estado alli.\n",
      "  â¡alguien ha comido de mi plato!, dijo el primero\n",
      "  â¡alguien ha usado mi tenedor!, dijo el segundo\n",
      "  â¡alguien ha bebido de mi vaso!, dijo el tercero\n",
      "  â¡alguien ha cortado con mi cuchillo!, dijo el cuarto\n",
      "  â¡alguien se ha limpiado con mi servilleta!, dijo el quinto\n",
      "  â¡alguien ha comido de mi pan!, dijo el sexto\n",
      "  â¡alguien se ha sentado en mi silla!, dijo el septimo\n",
      "cuando entraron en la habitacion desvelaron el misterio sobre lo ocurrido y se quedaron con la boca abierta al ver a una muchacha tan bella. tanto les gusto que decidieron dejar que durmiera.\n",
      "al dia siguiente blancanieves les conto a los enanitos la historia de como habia llegado hasta alli. los enanitos sintieron mucha lastima por ella y le ofrecieron quedarse en su casa. pero eso si, le advirtieron de que tuviera mucho cuidado y no abriese la puerta a nadie cuando ellos no estuvieran.\n",
      "la madrastra mientras tanto, convencida de que blancanieves estaba muerta, se puso ante su espejo y volvio a preguntarle:\n",
      "  espejito espejito, contestadme a una cosa â¿no soy yo la mas hermosa?\n",
      "  mi reina, vos sois una estrella pero siento deciros que blancanieves, sigue siendo la mas bella.\n",
      "la reina se puso furiosa y utilizo sus poderes para saber donde se escondia la muchacha. cuando supo que se encontraba en casa de los enanitos, preparo una manzana envenenada, se vistio de campesina y se encamino hacia montaã±a.\n",
      "cuando llego llamo a la puerta. blancanieves se asomo por la ventana y contesto:\n",
      "  no puedo abrir a nadie, me lo han prohibido los enanitos.\n",
      "  no temas hija mia, solo vengo a traerte manzanas. tengo muchas y no se que hacer con ellas. te dejare aqui una, por si te apetece mas tarde.\n",
      "blancanieves se fio de ella, mordio la manzana yâ¦ cayo al suelo de repente.\n",
      "la malvada reina que la vio, se marcho riendose por haberse salido con la suya. solo deseaba llegar a palacio y preguntar a su espejo magico quien era la mas bella ahora.\n",
      "blancanieves\n",
      "  espejito espejito, contestadme a una cosa â¿no soy yo la mas hermosa?\n",
      "  si, mi reina. de nuevo vos sois la mas hermosa.\n",
      "cuando los enanitos llegaron a casa y se la encontraron muerta en el suelo a blancanieves trataron de ver si aun podian hacer algo, pero todos sus esfuerzos fueron en vano. blancanieves estaba muerta.\n",
      "de modo que puesto que no podian hacer otra cosa, mandaron fabricar una caja de cristal, la colocaron en ella y la llevaron hasta la cumpre de la montaã±a donde estuvieron velandola por mucho tiempo. junto a ellos se unieron muchos animales del bosque que lloraban la perdida de la muchacha. pero un dia aparecio por alli un principe que al verla, se enamoro de inmediato de ella, y le pregunto a los enanitos si podia llevarsela con el.\n",
      "a los enanitos no les convencia la idea, pero el principe prometio cuidarla y venerarla, asi que accedieron.\n",
      "cuando los hombres del principe transportaban a blancanieves tropezaron con una piedra y del golpe, salio disparado el bocado de manzana envenenada de la garganta de blancanieves. en ese momento, blancanieves abrio los ojos de nuevo.\n",
      "  â¿donde estoy? â¿que ha pasado?, pregunto desorientada blancanieves\n",
      "  tranquila, estais sana y salva por fin y me habeis hecho con eso el hombre mas afortunado del mundo.\n",
      "blancanieves y el principe se convirtieron en marido y mujer y vivieron felices en su castillo.\n",
      " la bella durmiente\n",
      "erase una vez un rey y una reina que aunque vivian felices en su castillo ansiaban dia tras dia tener un hijo. un dia, estaba la reina baã±andose en el rio cuando una rana que oyo sus plegarias le dijo.\n",
      "  mi reina, muy pronto vereis cumplido vuestro deseo. en menos de un aã±o dareis a luz a una niã±a.\n",
      "al cabo de un aã±o se cumplio el pronostico y la reina dio a luz a una bella princesita. ella y su marido, el rey, estaban tan contentos que quisieron celebrar una gran fiesta en honor a su primogenita. a ella acudio todo el reino, incluidas las hadas, a quien el rey quiso invitar expresamente para que otorgaran nobles virtudes a su hija. pero sucedio que las hadas del reino eran trece, y el rey tenia solo doce platos de oro, por lo que tuvo que dejar de invitar a una de ellas. pero el soberano no le dio importancia a este hecho.\n",
      "al terminar el banquete cada hada regalo un don a la princesita. la primera le otorgo virtud; la segunda, belleza; la tercera, riqueza.. pero cuando ya solo quedaba la ultima hada por otorgar su virtud, aparecio muy enfadada el hada que no habia sido invitada y dijo:\n",
      "  cuando la princesa cumpla quince aã±os se pinchara con el huso de una rueca y morira.\n",
      "todos los invitados se quedaron con la boca abierta, asustados, sin saber que decir o que hacer. todavia quedaba un hada, pero no tenia poder suficiente para anular el encantamiento, asi que hizo lo que pudo para aplacar la condena:\n",
      "  no morira, sino que se quedara dormida durante cien aã±os.\n",
      "tras el incidente, el rey mando quemar todos los husos del reino creyendo que asi evitaria que se cumpliera el encantamiento.\n",
      "la princesa crecio y en ella florecieron todos sus dones. era hermosa, humilde, inteligenteâ¦ una princesa de la que todo el que la veia quedaba prendado.\n",
      "llego el dia marcado: el decimo quinto cumpleaã±os de la princesa, y coincidio que el rey y la reina estaban fuera de palacio, por lo que la princesa aprovecho para dar una vuelta por el castillo. llego a la torre y se encontro con una vieja que hilaba lino.\n",
      "  â¿que es eso que da vueltas?   dijo la muchacha seã±alando al huso.\n",
      "pero acerco su dedo un poco mas y apenas lo rozo el encantamiento surtio efecto y la princesa cayo profundamente dormida.\n",
      "el sueã±o se fue extendiendo por la corte y todo el mundo que vivia dentro de las paredes de palacio comenzo a quedarse dormido inexplicablemente. el rey y la reina, las sirvientas, el cocinero, los caballos, los perrosâ¦ hasta el fuego de la cocina se quedo dormido. pero mientras en el interior el sueã±o se apoderaba de todo, en el exterior un seto de rosales silvestres comenzo a crecer y acabo por rodear el castillo hasta llegar a cubrirlo por completo. por eso la princesa empezo a ser conocida como rosa silvestre.\n",
      "la bella durmientecon el paso de los aã±os fueron muchos los intrepidos caballeros que creyeron que podrian cruzar el rosal y acceder al castillo, pero se equivocaban porque era imposible atravesarlo.\n",
      "un dia llego el hijo de un rey, y se dispuso a intentarlo una vez mas. pero como el encantamiento estaba a punto de romperse porque ya casi habian transcurrido los cien aã±os, esta vez el rosal se abrio ante si, dejandole acceder a su interior. recorrio el palacio hasta llegar a la princesa y se quedo hechizado al verla. se acerco a ella y apenas la beso la princesa abrio los ojos tras su largo letargo. con ella fueron despertando tambien poco a poco todas las personas de palacio y tambien los animales y el reino recupero su esplendor y alegria.\n",
      "en aquel ambiente de alegria tuvo lugar la boda entre el principe y la princesa y estos fueron felices para siempre.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0beyHLj_Idb"
   },
   "source": [
    "### Convertir el texto a números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1eGB8IXBLDM"
   },
   "source": [
    "Obtener los caracteres del texto de manera ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AcsqVYkR_Ps1"
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFpeLbav_Y5e",
    "outputId": "09a05dbe-f469-4eda-93b6-8482221d821c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', ',', '.', '0', '1', '2', '7', '8', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x80', '\\x93', '\\x9c', '\\x9d', '¡', '¦', '±', '¿', 'â', 'ã']\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZX5XU8KBc9C"
   },
   "source": [
    "Crear un diccionario asignando un índice a los caracteres obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qhE2Kvr4_nyF"
   },
   "outputs": [],
   "source": [
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdSioVDCAQmh",
    "outputId": "e08cdc36-a420-4794-93d4-c4008b041f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, ',': 4, '.': 5, '0': 6, '1': 7, '2': 8, '7': 9, '8': 10, ':': 11, ';': 12, '?': 13, 'a': 14, 'b': 15, 'c': 16, 'd': 17, 'e': 18, 'f': 19, 'g': 20, 'h': 21, 'i': 22, 'j': 23, 'k': 24, 'l': 25, 'm': 26, 'n': 27, 'o': 28, 'p': 29, 'q': 30, 'r': 31, 's': 32, 't': 33, 'u': 34, 'v': 35, 'w': 36, 'x': 37, 'y': 38, 'z': 39, '\\x80': 40, '\\x93': 41, '\\x9c': 42, '\\x9d': 43, '¡': 44, '¦': 45, '±': 46, '¿': 47, 'â': 48, 'ã': 49}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Y7t6Oj-MAWW5"
   },
   "outputs": [],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BM3GTzj9AteC",
    "outputId": "4a28ebf4-42de-41ec-a47f-20cc92f6c435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de caracteres 55517. Tamaño del diccionario en caracteres 50.\n"
     ]
    }
   ],
   "source": [
    "print('Número de caracteres %d. Tamaño del diccionario en caracteres %d.' % (n_chars, n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-VQmcnUBnns"
   },
   "source": [
    "### Dividir el texto en secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iXoZrDRB2tt"
   },
   "source": [
    "Dividimos el texto en secuencias, convertimos los caracteres a números enteros usando nuestra tabla de búsqueda que preparamos anteriormente, es decir que preparamos el conjunto de datos de los pares de entrada a salida codificándolos como enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dlttXmEnBxDX"
   },
   "outputs": [],
   "source": [
    "seq_lenght = 100\n",
    "dataX = []\n",
    "dataY = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-EUjiT7_CZk4"
   },
   "outputs": [],
   "source": [
    "for i in range(0, n_chars-seq_lenght, 1):\n",
    "    seq_in = raw_text[i:i + seq_lenght]\n",
    "    seq_out = raw_text[i + seq_lenght]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ma9YvLE4DJ22"
   },
   "outputs": [],
   "source": [
    "n_patterns = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAVew8YFDNVI",
    "outputId": "8cf92f51-7c3e-4437-8c61-2aeea72322f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se generaron 55417 secuencias de 100 caracteres desde el texto\n"
     ]
    }
   ],
   "source": [
    "print('Se generaron {} secuencias de 100 caracteres desde el texto'.format(n_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCDqMI8RDdat",
    "outputId": "e67857ba-5202-490b-fc36-ac863a19c367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 16, 14, 29, 18, 31, 34, 16, 22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14], [1, 16, 14, 29, 18, 31, 34, 16, 22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15], [16, 14, 29, 18, 31, 34, 16, 22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14], [14, 29, 18, 31, 34, 16, 22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1], [29, 18, 31, 34, 16, 22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18], [18, 31, 34, 16, 22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27], [31, 34, 16, 22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1], [34, 16, 22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33], [16, 22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28], [22, 33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17], [33, 14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28], [14, 1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1], [1, 31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25], [31, 28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28], [28, 23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1], [23, 14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30], [14, 0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34], [0, 21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18], [21, 14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1], [14, 15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29], [15, 22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28], [22, 14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17], [14, 1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22], [1, 34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14], [34, 27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1], [27, 14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38], [14, 1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1], [1, 35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16], [35, 18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28], [18, 39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26], [39, 1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28], [1, 34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1], [34, 27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18], [27, 14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31], [14, 1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14], [1, 17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1], [17, 34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33], [34, 25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14], [25, 16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27], [16, 18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1], [18, 1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15], [1, 27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34], [27, 22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18], [22, 49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27], [49, 46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14], [46, 14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1], [14, 1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18], [1, 30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25], [30, 34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1], [34, 18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17], [18, 1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22], [1, 30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14], [30, 34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1], [34, 18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17], [18, 31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18], [31, 22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1], [22, 14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32], [14, 1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34], [1, 26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1], [26, 34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16], [34, 16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34], [16, 21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26], [21, 28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29], [28, 1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25], [1, 14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18], [14, 1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14], [1, 32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49], [32, 34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46], [34, 1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28], [1, 26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32], [26, 14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1], [14, 17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32], [17, 31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34], [31, 18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1], [18, 1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14], [1, 38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15], [38, 1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34], [1, 14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18], [14, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25], [1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14], [32, 34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1], [34, 1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25], [1, 14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18], [14, 15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1], [15, 34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31], [34, 18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18], [18, 25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20], [25, 14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14], [14, 5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25], [5, 1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28], [1, 25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1], [25, 18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34], [18, 32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34, 27], [32, 1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34, 27, 14], [1, 14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34, 27, 14, 1], [14, 38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34, 27, 14, 1, 16], [38, 34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34, 27, 14, 1, 16, 14], [34, 17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34, 27, 14, 1, 16, 14, 29], [17, 14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34, 27, 14, 1, 16, 14, 29, 18], [14, 15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34, 27, 14, 1, 16, 14, 29, 18, 31]]\n"
     ]
    }
   ],
   "source": [
    "print(dataX[0 : 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQ00FGIjDnkF",
    "outputId": "37300205-d731-4c35-b495-6f841788b753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 14, 1, 18, 27, 1, 33, 28, 17, 28, 1, 25, 28, 1, 30, 34, 18, 1, 29, 28, 17, 22, 14, 1, 38, 1, 16, 28, 26, 28, 1, 18, 31, 14, 1, 33, 14, 27, 1, 15, 34, 18, 27, 14, 1, 18, 25, 1, 17, 22, 14, 1, 17, 18, 1, 32, 34, 1, 16, 34, 26, 29, 25, 18, 14, 49, 46, 28, 32, 1, 32, 34, 1, 14, 15, 34, 18, 25, 14, 1, 25, 18, 1, 31, 18, 20, 14, 25, 28, 1, 34, 27, 14, 1, 16, 14, 29, 18, 31, 34]\n"
     ]
    }
   ],
   "source": [
    "print(dataY[0: 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZLC3JxADuQB"
   },
   "source": [
    "## Preparar nuestos datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNqdRB5tD2r1"
   },
   "source": [
    "1. Transformar la lista de secuencias de entrada a la forma (muestras - pasos de tiempo - características) esperada por una red LSTM.\n",
    "2. Luego debemos cambiar la escala de los números enteros al rango de 0 a 1 para que los patrones sean más fáciles de aprender mediante la red LSTM que utiliza la función de activación sigmoidea de forma predeterminada.\n",
    "3. Necesitamos convertir los patrones de salida (caracteres individuales convertidos en enteros) en una codificación one-hot. Esto es para que podamos configurar la red para predecir la probabilidad de cada uno de los caracteres del diccionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjdLIAsLEvfc"
   },
   "source": [
    "Transformar la lista X de secuencias de entrada a la forma [muestras, pasos de tiempo, características]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2F4E8JAADzle"
   },
   "outputs": [],
   "source": [
    "X = np.reshape(dataX, (n_patterns, seq_lenght, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMDycHOsGHR5"
   },
   "source": [
    "Normalizar (cambiar la escala de los números de 0 a 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5bTNrMABGNv0"
   },
   "outputs": [],
   "source": [
    "X = X / float(n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAZ461nyGRuC"
   },
   "source": [
    "Convertir los patrones de salida (caracteres individuales convertidos en enteros) en una codificación one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fSvMH0inGb1z"
   },
   "outputs": [],
   "source": [
    "Y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9hncNX1GhOK",
    "outputId": "0441994d-6054-4572-88f0-6835b106d948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.02]\n",
      "  [0.02]\n",
      "  [0.32]\n",
      "  ...\n",
      "  [0.68]\n",
      "  [0.34]\n",
      "  [0.28]]\n",
      "\n",
      " [[0.02]\n",
      "  [0.32]\n",
      "  [0.28]\n",
      "  ...\n",
      "  [0.34]\n",
      "  [0.28]\n",
      "  [0.3 ]]\n",
      "\n",
      " [[0.32]\n",
      "  [0.28]\n",
      "  [0.58]\n",
      "  ...\n",
      "  [0.28]\n",
      "  [0.3 ]\n",
      "  [0.28]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.54]\n",
      "  [0.66]\n",
      "  [0.36]\n",
      "  ...\n",
      "  [0.52]\n",
      "  [0.58]\n",
      "  [0.62]]\n",
      "\n",
      " [[0.66]\n",
      "  [0.36]\n",
      "  [0.02]\n",
      "  ...\n",
      "  [0.58]\n",
      "  [0.62]\n",
      "  [0.36]]\n",
      "\n",
      " [[0.36]\n",
      "  [0.02]\n",
      "  [0.34]\n",
      "  ...\n",
      "  [0.62]\n",
      "  [0.36]\n",
      "  [0.1 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxJ4YwLfGmDL",
    "outputId": "d77937af-8f79-4a68-fdc0-4d3474307dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JcdlMziGtZ-"
   },
   "source": [
    "## Construcción del modelo RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPNa5-DZGy4M"
   },
   "source": [
    "***Definimos nuestro modelo LSTM***: Aquí definimos una única capa LSTM oculta con 256 unidades de memoria. La red usa deserción con una probabilidad de 20%. La capa de salida es una capa densa que usa la función de activación `softmax` para generar una predicción de probabilidad para cada uno de los caracteres del diccionario entre 0 y 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hX8uFT4HtDV"
   },
   "source": [
    "Definir el modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IhSLmPEiH1Xl"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KcFwIZbXH5Aa"
   },
   "outputs": [],
   "source": [
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=False))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNwTZ1JEJigD"
   },
   "source": [
    "Utilizamos el algoritmo de optimización de ADAM para la velocidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "wbLfE4D4JtRx"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-CsTDXGJ-LM"
   },
   "source": [
    "## Creación de Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqpnL6NAKB6o"
   },
   "source": [
    "La red neuronal es lenta de entrenar teniendo activa la GPU, por lo que creamos *checkpoints* para registrar todos los pesos de la red para archivar cada vez que se observe una mejora en la pérdida al final del epoch. Usaremos el mejor conjunto de pesos (menor pérdida) para instanciar nuestro modelo generativo en la siguiente sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Lpc5UW4rKpgb"
   },
   "outputs": [],
   "source": [
    "filepath = './Checkpoints/weights-improvement-epoch_{epoch:02d}-loss_{loss:.4f}-accuracy_{accuracy:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvlKnBbRMiyG"
   },
   "source": [
    "[Documentación de ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "do1-7qkWLUTu"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "IMj5E8uvMfP0"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OykIXyyOou4"
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXWPSIQ2OrL4"
   },
   "source": [
    "- epoch = Un límite arbitrario, generalmente definido como \"una pasada sobre todo el conjunto de datos\", que se utiliza para separar el entrenamiento en distintas fases, que es útil para el registro y la evaluación periódica.\n",
    "\n",
    "- batch_size = Define la cantidad de muestras que se propagarán a través de la red. (50 x 1109)=55450, recordar que: *Se generaron 55417 secuencias de 100 caracteres desde texto*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjHG1UGNO--_",
    "outputId": "6fac72a6-3204-4b32-cebe-5c6116fd5e6b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9628 - accuracy: 0.1674\n",
      "Epoch 00001: loss improved from inf to 2.96290, saving model to ./Checkpoints\\weights-improvement-epoch_01-loss_2.9629-accuracy_0.1674.hdf5\n",
      "555/555 [==============================] - 46s 69ms/step - loss: 2.9629 - accuracy: 0.1674 - val_loss: 2.9020 - val_accuracy: 0.1784\n",
      "Epoch 2/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.8996 - accuracy: 0.1773\n",
      "Epoch 00002: loss improved from 2.96290 to 2.89951, saving model to ./Checkpoints\\weights-improvement-epoch_02-loss_2.8995-accuracy_0.1774.hdf5\n",
      "555/555 [==============================] - 39s 71ms/step - loss: 2.8995 - accuracy: 0.1774 - val_loss: 2.8143 - val_accuracy: 0.1888\n",
      "Epoch 3/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.7581 - accuracy: 0.1919\n",
      "Epoch 00003: loss improved from 2.89951 to 2.75821, saving model to ./Checkpoints\\weights-improvement-epoch_03-loss_2.7582-accuracy_0.1919.hdf5\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 2.7582 - accuracy: 0.1919 - val_loss: 2.6453 - val_accuracy: 0.2141\n",
      "Epoch 4/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.6450 - accuracy: 0.2107\n",
      "Epoch 00004: loss improved from 2.75821 to 2.64491, saving model to ./Checkpoints\\weights-improvement-epoch_04-loss_2.6449-accuracy_0.2108.hdf5\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 2.6449 - accuracy: 0.2108 - val_loss: 2.5605 - val_accuracy: 0.2347\n",
      "Epoch 5/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.5827 - accuracy: 0.2270\n",
      "Epoch 00005: loss improved from 2.64491 to 2.58258, saving model to ./Checkpoints\\weights-improvement-epoch_05-loss_2.5826-accuracy_0.2271.hdf5\n",
      "555/555 [==============================] - 37s 66ms/step - loss: 2.5826 - accuracy: 0.2271 - val_loss: 2.5110 - val_accuracy: 0.2427\n",
      "Epoch 6/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.5421 - accuracy: 0.2355\n",
      "Epoch 00006: loss improved from 2.58258 to 2.54224, saving model to ./Checkpoints\\weights-improvement-epoch_06-loss_2.5422-accuracy_0.2355.hdf5\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 2.5422 - accuracy: 0.2355 - val_loss: 2.4855 - val_accuracy: 0.2502\n",
      "Epoch 7/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.5083 - accuracy: 0.2465\n",
      "Epoch 00007: loss improved from 2.54224 to 2.50827, saving model to ./Checkpoints\\weights-improvement-epoch_07-loss_2.5083-accuracy_0.2465.hdf5\n",
      "555/555 [==============================] - 36s 64ms/step - loss: 2.5083 - accuracy: 0.2465 - val_loss: 2.4274 - val_accuracy: 0.2747\n",
      "Epoch 8/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.4758 - accuracy: 0.2574\n",
      "Epoch 00008: loss improved from 2.50827 to 2.47579, saving model to ./Checkpoints\\weights-improvement-epoch_08-loss_2.4758-accuracy_0.2574.hdf5\n",
      "555/555 [==============================] - 40s 72ms/step - loss: 2.4758 - accuracy: 0.2574 - val_loss: 2.3860 - val_accuracy: 0.2807\n",
      "Epoch 9/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.4445 - accuracy: 0.2666 ETA: 0s - loss: 2.4447 - \n",
      "Epoch 00009: loss improved from 2.47579 to 2.44436, saving model to ./Checkpoints\\weights-improvement-epoch_09-loss_2.4444-accuracy_0.2666.hdf5\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 2.4444 - accuracy: 0.2666 - val_loss: 2.3541 - val_accuracy: 0.2947\n",
      "Epoch 10/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.4158 - accuracy: 0.2752\n",
      "Epoch 00010: loss improved from 2.44436 to 2.41568, saving model to ./Checkpoints\\weights-improvement-epoch_10-loss_2.4157-accuracy_0.2753.hdf5\n",
      "555/555 [==============================] - 37s 67ms/step - loss: 2.4157 - accuracy: 0.2753 - val_loss: 2.3236 - val_accuracy: 0.3045\n",
      "Epoch 11/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.3907 - accuracy: 0.2798\n",
      "Epoch 00011: loss improved from 2.41568 to 2.39065, saving model to ./Checkpoints\\weights-improvement-epoch_11-loss_2.3907-accuracy_0.2798.hdf5\n",
      "555/555 [==============================] - 37s 66ms/step - loss: 2.3907 - accuracy: 0.2798 - val_loss: 2.2908 - val_accuracy: 0.3160\n",
      "Epoch 12/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.3584 - accuracy: 0.2914 ETA:  - ETA: 1s - loss:\n",
      "Epoch 00012: loss improved from 2.39065 to 2.35847, saving model to ./Checkpoints\\weights-improvement-epoch_12-loss_2.3585-accuracy_0.2914.hdf5\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 2.3585 - accuracy: 0.2914 - val_loss: 2.2592 - val_accuracy: 0.3231\n",
      "Epoch 13/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.3312 - accuracy: 0.3013\n",
      "Epoch 00013: loss improved from 2.35847 to 2.33124, saving model to ./Checkpoints\\weights-improvement-epoch_13-loss_2.3312-accuracy_0.3013.hdf5\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 2.3312 - accuracy: 0.3013 - val_loss: 2.2125 - val_accuracy: 0.3348\n",
      "Epoch 14/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.3061 - accuracy: 0.3081\n",
      "Epoch 00014: loss improved from 2.33124 to 2.30604, saving model to ./Checkpoints\\weights-improvement-epoch_14-loss_2.3060-accuracy_0.3080.hdf5\n",
      "555/555 [==============================] - 42s 76ms/step - loss: 2.3060 - accuracy: 0.3080 - val_loss: 2.1866 - val_accuracy: 0.3430\n",
      "Epoch 15/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.2791 - accuracy: 0.3137\n",
      "Epoch 00015: loss improved from 2.30604 to 2.27921, saving model to ./Checkpoints\\weights-improvement-epoch_15-loss_2.2792-accuracy_0.3137.hdf5\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 2.2792 - accuracy: 0.3137 - val_loss: 2.1546 - val_accuracy: 0.3513\n",
      "Epoch 16/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.2447 - accuracy: 0.3219\n",
      "Epoch 00016: loss improved from 2.27921 to 2.24473, saving model to ./Checkpoints\\weights-improvement-epoch_16-loss_2.2447-accuracy_0.3219.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 2.2447 - accuracy: 0.3219 - val_loss: 2.1358 - val_accuracy: 0.3521\n",
      "Epoch 17/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.2221 - accuracy: 0.3282\n",
      "Epoch 00017: loss improved from 2.24473 to 2.22207, saving model to ./Checkpoints\\weights-improvement-epoch_17-loss_2.2221-accuracy_0.3282.hdf5\n",
      "555/555 [==============================] - 34s 62ms/step - loss: 2.2221 - accuracy: 0.3282 - val_loss: 2.0947 - val_accuracy: 0.3647\n",
      "Epoch 18/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.1965 - accuracy: 0.3359 ETA: 3s - loss: - ETA: \n",
      "Epoch 00018: loss improved from 2.22207 to 2.19653, saving model to ./Checkpoints\\weights-improvement-epoch_18-loss_2.1965-accuracy_0.3359.hdf5\n",
      "555/555 [==============================] - 43s 77ms/step - loss: 2.1965 - accuracy: 0.3359 - val_loss: 2.0763 - val_accuracy: 0.3703\n",
      "Epoch 19/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.1783 - accuracy: 0.3417 ETA: 0s - loss: 2.1780 - accuracy: 0.34\n",
      "Epoch 00019: loss improved from 2.19653 to 2.17817, saving model to ./Checkpoints\\weights-improvement-epoch_19-loss_2.1782-accuracy_0.3417.hdf5\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 2.1782 - accuracy: 0.3417 - val_loss: 2.0423 - val_accuracy: 0.3805\n",
      "Epoch 20/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.1516 - accuracy: 0.3488\n",
      "Epoch 00020: loss improved from 2.17817 to 2.15174, saving model to ./Checkpoints\\weights-improvement-epoch_20-loss_2.1517-accuracy_0.3488.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 2.1517 - accuracy: 0.3488 - val_loss: 2.0185 - val_accuracy: 0.3890\n",
      "Epoch 21/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.1332 - accuracy: 0.3555\n",
      "Epoch 00021: loss improved from 2.15174 to 2.13321, saving model to ./Checkpoints\\weights-improvement-epoch_21-loss_2.1332-accuracy_0.3555.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 2.1332 - accuracy: 0.3555 - val_loss: 1.9979 - val_accuracy: 0.3946\n",
      "Epoch 22/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.1160 - accuracy: 0.3593 - ETA: 21s - loss - ETA: 19s - loss: 2.1130 - accura - ET\n",
      "Epoch 00022: loss improved from 2.13321 to 2.11597, saving model to ./Checkpoints\\weights-improvement-epoch_22-loss_2.1160-accuracy_0.3593.hdf5\n",
      "555/555 [==============================] - 34s 62ms/step - loss: 2.1160 - accuracy: 0.3593 - val_loss: 1.9706 - val_accuracy: 0.4017\n",
      "Epoch 23/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0945 - accuracy: 0.3666\n",
      "Epoch 00023: loss improved from 2.11597 to 2.09444, saving model to ./Checkpoints\\weights-improvement-epoch_23-loss_2.0944-accuracy_0.3666.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 2.0944 - accuracy: 0.3666 - val_loss: 1.9580 - val_accuracy: 0.4033\n",
      "Epoch 24/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0795 - accuracy: 0.3685\n",
      "Epoch 00024: loss improved from 2.09444 to 2.07944, saving model to ./Checkpoints\\weights-improvement-epoch_24-loss_2.0794-accuracy_0.3686.hdf5\n",
      "555/555 [==============================] - 35s 63ms/step - loss: 2.0794 - accuracy: 0.3686 - val_loss: 1.9248 - val_accuracy: 0.4149\n",
      "Epoch 25/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0602 - accuracy: 0.3742 ETA: \n",
      "Epoch 00025: loss improved from 2.07944 to 2.06026, saving model to ./Checkpoints\\weights-improvement-epoch_25-loss_2.0603-accuracy_0.3741.hdf5\n",
      "555/555 [==============================] - 35s 62ms/step - loss: 2.0603 - accuracy: 0.3741 - val_loss: 1.9072 - val_accuracy: 0.4198\n",
      "Epoch 26/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0444 - accuracy: 0.3793\n",
      "Epoch 00026: loss improved from 2.06026 to 2.04437, saving model to ./Checkpoints\\weights-improvement-epoch_26-loss_2.0444-accuracy_0.3792.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 2.0444 - accuracy: 0.3792 - val_loss: 1.8837 - val_accuracy: 0.4241\n",
      "Epoch 27/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0247 - accuracy: 0.3833\n",
      "Epoch 00027: loss improved from 2.04437 to 2.02456, saving model to ./Checkpoints\\weights-improvement-epoch_27-loss_2.0246-accuracy_0.3833.hdf5\n",
      "555/555 [==============================] - 34s 62ms/step - loss: 2.0246 - accuracy: 0.3833 - val_loss: 1.8678 - val_accuracy: 0.4341\n",
      "Epoch 28/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0129 - accuracy: 0.3888\n",
      "Epoch 00028: loss improved from 2.02456 to 2.01299, saving model to ./Checkpoints\\weights-improvement-epoch_28-loss_2.0130-accuracy_0.3888.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 2.0130 - accuracy: 0.3888 - val_loss: 1.8498 - val_accuracy: 0.4371\n",
      "Epoch 29/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9970 - accuracy: 0.3940 ETA: 5s - loss: 1 - ETA: 4s - los - ETA: 2s - loss: 1.995\n",
      "Epoch 00029: loss improved from 2.01299 to 1.99701, saving model to ./Checkpoints\\weights-improvement-epoch_29-loss_1.9970-accuracy_0.3940.hdf5\n",
      "555/555 [==============================] - 34s 62ms/step - loss: 1.9970 - accuracy: 0.3940 - val_loss: 1.8339 - val_accuracy: 0.4420\n",
      "Epoch 30/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9845 - accuracy: 0.3954 - ET - ETA: 19s  - ETA: 5s - loss: 1.9808 - accura - ETA: 4s - loss: 1\n",
      "Epoch 00030: loss improved from 1.99701 to 1.98445, saving model to ./Checkpoints\\weights-improvement-epoch_30-loss_1.9845-accuracy_0.3954.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 1.9845 - accuracy: 0.3954 - val_loss: 1.8214 - val_accuracy: 0.4446\n",
      "Epoch 31/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9699 - accuracy: 0.4010 ETA: 1s - loss: 1.9702 - accuracy:  - ETA: 1s - loss: 1.9714 - accuracy:  - ETA: 1s - los\n",
      "Epoch 00031: loss improved from 1.98445 to 1.96988, saving model to ./Checkpoints\\weights-improvement-epoch_31-loss_1.9699-accuracy_0.4010.hdf5\n",
      "555/555 [==============================] - 34s 62ms/step - loss: 1.9699 - accuracy: 0.4010 - val_loss: 1.7983 - val_accuracy: 0.4478\n",
      "Epoch 32/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9571 - accuracy: 0.4035\n",
      "Epoch 00032: loss improved from 1.96988 to 1.95715, saving model to ./Checkpoints\\weights-improvement-epoch_32-loss_1.9572-accuracy_0.4036.hdf5\n",
      "555/555 [==============================] - 35s 64ms/step - loss: 1.9572 - accuracy: 0.4036 - val_loss: 1.7912 - val_accuracy: 0.4507\n",
      "Epoch 33/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9441 - accuracy: 0.4060\n",
      "Epoch 00033: loss improved from 1.95715 to 1.94413, saving model to ./Checkpoints\\weights-improvement-epoch_33-loss_1.9441-accuracy_0.4060.hdf5\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 1.9441 - accuracy: 0.4060 - val_loss: 1.7697 - val_accuracy: 0.4586\n",
      "Epoch 34/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9324 - accuracy: 0.4118\n",
      "Epoch 00034: loss improved from 1.94413 to 1.93266, saving model to ./Checkpoints\\weights-improvement-epoch_34-loss_1.9327-accuracy_0.4117.hdf5\n",
      "555/555 [==============================] - 46s 84ms/step - loss: 1.9327 - accuracy: 0.4117 - val_loss: 1.7586 - val_accuracy: 0.4645\n",
      "Epoch 35/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9186 - accuracy: 0.4137\n",
      "Epoch 00035: loss improved from 1.93266 to 1.91874, saving model to ./Checkpoints\\weights-improvement-epoch_35-loss_1.9187-accuracy_0.4137.hdf5\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 1.9187 - accuracy: 0.4137 - val_loss: 1.7407 - val_accuracy: 0.4685\n",
      "Epoch 36/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9115 - accuracy: 0.4163\n",
      "Epoch 00036: loss improved from 1.91874 to 1.91149, saving model to ./Checkpoints\\weights-improvement-epoch_36-loss_1.9115-accuracy_0.4163.hdf5\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 1.9115 - accuracy: 0.4163 - val_loss: 1.7299 - val_accuracy: 0.4687\n",
      "Epoch 37/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.8928 - accuracy: 0.4197\n",
      "Epoch 00037: loss improved from 1.91149 to 1.89285, saving model to ./Checkpoints\\weights-improvement-epoch_37-loss_1.8929-accuracy_0.4197.hdf5\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 1.8929 - accuracy: 0.4197 - val_loss: 1.7125 - val_accuracy: 0.4725\n",
      "Epoch 38/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.8823 - accuracy: 0.4214\n",
      "Epoch 00038: loss improved from 1.89285 to 1.88225, saving model to ./Checkpoints\\weights-improvement-epoch_38-loss_1.8823-accuracy_0.4214.hdf5\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 1.8823 - accuracy: 0.4214 - val_loss: 1.6914 - val_accuracy: 0.4798\n",
      "Epoch 39/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.8714 - accuracy: 0.4285\n",
      "Epoch 00039: loss improved from 1.88225 to 1.87142, saving model to ./Checkpoints\\weights-improvement-epoch_39-loss_1.8714-accuracy_0.4285.hdf5\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 1.8714 - accuracy: 0.4285 - val_loss: 1.6854 - val_accuracy: 0.4829\n",
      "Epoch 40/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.8654 - accuracy: 0.4297\n",
      "Epoch 00040: loss improved from 1.87142 to 1.86539, saving model to ./Checkpoints\\weights-improvement-epoch_40-loss_1.8654-accuracy_0.4297.hdf5\n",
      "555/555 [==============================] - 50s 90ms/step - loss: 1.8654 - accuracy: 0.4297 - val_loss: 1.6725 - val_accuracy: 0.4873\n",
      "Epoch 41/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.8497 - accuracy: 0.4347\n",
      "Epoch 00041: loss improved from 1.86539 to 1.84977, saving model to ./Checkpoints\\weights-improvement-epoch_41-loss_1.8498-accuracy_0.4347.hdf5\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 1.8498 - accuracy: 0.4347 - val_loss: 1.6580 - val_accuracy: 0.4905\n",
      "Epoch 42/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.8461 - accuracy: 0.4344\n",
      "Epoch 00042: loss improved from 1.84977 to 1.84609, saving model to ./Checkpoints\\weights-improvement-epoch_42-loss_1.8461-accuracy_0.4344.hdf5\n",
      "555/555 [==============================] - 46s 82ms/step - loss: 1.8461 - accuracy: 0.4344 - val_loss: 1.6639 - val_accuracy: 0.4889\n",
      "Epoch 43/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/555 [==============================] - ETA: 0s - loss: 1.8321 - accuracy: 0.4378\n",
      "Epoch 00043: loss improved from 1.84609 to 1.83208, saving model to ./Checkpoints\\weights-improvement-epoch_43-loss_1.8321-accuracy_0.4378.hdf5\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 1.8321 - accuracy: 0.4378 - val_loss: 1.6332 - val_accuracy: 0.4955\n",
      "Epoch 44/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.8164 - accuracy: 0.4390\n",
      "Epoch 00044: loss improved from 1.83208 to 1.81642, saving model to ./Checkpoints\\weights-improvement-epoch_44-loss_1.8164-accuracy_0.4390.hdf5\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 1.8164 - accuracy: 0.4390 - val_loss: 1.6317 - val_accuracy: 0.4983\n",
      "Epoch 45/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.8102 - accuracy: 0.4409\n",
      "Epoch 00045: loss improved from 1.81642 to 1.81022, saving model to ./Checkpoints\\weights-improvement-epoch_45-loss_1.8102-accuracy_0.4409.hdf5\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 1.8102 - accuracy: 0.4409 - val_loss: 1.6162 - val_accuracy: 0.5050\n",
      "Epoch 46/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.8005 - accuracy: 0.4486\n",
      "Epoch 00046: loss improved from 1.81022 to 1.80068, saving model to ./Checkpoints\\weights-improvement-epoch_46-loss_1.8007-accuracy_0.4486.hdf5\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 1.8007 - accuracy: 0.4486 - val_loss: 1.5983 - val_accuracy: 0.5043\n",
      "Epoch 47/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7916 - accuracy: 0.4473\n",
      "Epoch 00047: loss improved from 1.80068 to 1.79169, saving model to ./Checkpoints\\weights-improvement-epoch_47-loss_1.7917-accuracy_0.4473.hdf5\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 1.7917 - accuracy: 0.4473 - val_loss: 1.5887 - val_accuracy: 0.5086\n",
      "Epoch 48/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7866 - accuracy: 0.4497\n",
      "Epoch 00048: loss improved from 1.79169 to 1.78656, saving model to ./Checkpoints\\weights-improvement-epoch_48-loss_1.7866-accuracy_0.4497.hdf5\n",
      "555/555 [==============================] - 42s 76ms/step - loss: 1.7866 - accuracy: 0.4497 - val_loss: 1.5857 - val_accuracy: 0.5096\n",
      "Epoch 49/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7753 - accuracy: 0.4524\n",
      "Epoch 00049: loss improved from 1.78656 to 1.77505, saving model to ./Checkpoints\\weights-improvement-epoch_49-loss_1.7751-accuracy_0.4524.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.7751 - accuracy: 0.4524 - val_loss: 1.5705 - val_accuracy: 0.5126\n",
      "Epoch 50/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.7667 - accuracy: 0.4562\n",
      "Epoch 00050: loss improved from 1.77505 to 1.76668, saving model to ./Checkpoints\\weights-improvement-epoch_50-loss_1.7667-accuracy_0.4562.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 1.7667 - accuracy: 0.4562 - val_loss: 1.5611 - val_accuracy: 0.5172\n",
      "Epoch 51/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.7566 - accuracy: 0.4564\n",
      "Epoch 00051: loss improved from 1.76668 to 1.75656, saving model to ./Checkpoints\\weights-improvement-epoch_51-loss_1.7566-accuracy_0.4564.hdf5\n",
      "555/555 [==============================] - 30s 55ms/step - loss: 1.7566 - accuracy: 0.4564 - val_loss: 1.5459 - val_accuracy: 0.5197\n",
      "Epoch 52/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.7551 - accuracy: 0.4579\n",
      "Epoch 00052: loss improved from 1.75656 to 1.75505, saving model to ./Checkpoints\\weights-improvement-epoch_52-loss_1.7551-accuracy_0.4579.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.7551 - accuracy: 0.4579 - val_loss: 1.5376 - val_accuracy: 0.5229\n",
      "Epoch 53/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.7416 - accuracy: 0.4595\n",
      "Epoch 00053: loss improved from 1.75505 to 1.74160, saving model to ./Checkpoints\\weights-improvement-epoch_53-loss_1.7416-accuracy_0.4595.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.7416 - accuracy: 0.4595 - val_loss: 1.5303 - val_accuracy: 0.5239\n",
      "Epoch 54/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7340 - accuracy: 0.4632\n",
      "Epoch 00054: loss improved from 1.74160 to 1.73403, saving model to ./Checkpoints\\weights-improvement-epoch_54-loss_1.7340-accuracy_0.4632.hdf5\n",
      "555/555 [==============================] - 38s 68ms/step - loss: 1.7340 - accuracy: 0.4632 - val_loss: 1.5303 - val_accuracy: 0.5233\n",
      "Epoch 55/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7265 - accuracy: 0.4636\n",
      "Epoch 00055: loss improved from 1.73403 to 1.72647, saving model to ./Checkpoints\\weights-improvement-epoch_55-loss_1.7265-accuracy_0.4636.hdf5\n",
      "555/555 [==============================] - 37s 67ms/step - loss: 1.7265 - accuracy: 0.4636 - val_loss: 1.5162 - val_accuracy: 0.5285\n",
      "Epoch 56/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.7215 - accuracy: 0.4685\n",
      "Epoch 00056: loss improved from 1.72647 to 1.72146, saving model to ./Checkpoints\\weights-improvement-epoch_56-loss_1.7215-accuracy_0.4685.hdf5\n",
      "555/555 [==============================] - 37s 66ms/step - loss: 1.7215 - accuracy: 0.4685 - val_loss: 1.5006 - val_accuracy: 0.5318\n",
      "Epoch 57/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7123 - accuracy: 0.4694\n",
      "Epoch 00057: loss improved from 1.72146 to 1.71230, saving model to ./Checkpoints\\weights-improvement-epoch_57-loss_1.7123-accuracy_0.4694.hdf5\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 1.7123 - accuracy: 0.4694 - val_loss: 1.4890 - val_accuracy: 0.5351\n",
      "Epoch 58/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7051 - accuracy: 0.4701\n",
      "Epoch 00058: loss improved from 1.71230 to 1.70512, saving model to ./Checkpoints\\weights-improvement-epoch_58-loss_1.7051-accuracy_0.4701.hdf5\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 1.7051 - accuracy: 0.4701 - val_loss: 1.4920 - val_accuracy: 0.5365\n",
      "Epoch 59/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6959 - accuracy: 0.4724\n",
      "Epoch 00059: loss improved from 1.70512 to 1.69588, saving model to ./Checkpoints\\weights-improvement-epoch_59-loss_1.6959-accuracy_0.4724.hdf5\n",
      "555/555 [==============================] - 38s 68ms/step - loss: 1.6959 - accuracy: 0.4724 - val_loss: 1.4693 - val_accuracy: 0.5411\n",
      "Epoch 60/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6836 - accuracy: 0.4762\n",
      "Epoch 00060: loss improved from 1.69588 to 1.68352, saving model to ./Checkpoints\\weights-improvement-epoch_60-loss_1.6835-accuracy_0.4762.hdf5\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 1.6835 - accuracy: 0.4762 - val_loss: 1.4584 - val_accuracy: 0.5443\n",
      "Epoch 61/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6821 - accuracy: 0.4767\n",
      "Epoch 00061: loss improved from 1.68352 to 1.68214, saving model to ./Checkpoints\\weights-improvement-epoch_61-loss_1.6821-accuracy_0.4767.hdf5\n",
      "555/555 [==============================] - 41s 73ms/step - loss: 1.6821 - accuracy: 0.4767 - val_loss: 1.4630 - val_accuracy: 0.5437\n",
      "Epoch 62/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6759 - accuracy: 0.4772\n",
      "Epoch 00062: loss improved from 1.68214 to 1.67584, saving model to ./Checkpoints\\weights-improvement-epoch_62-loss_1.6758-accuracy_0.4772.hdf5\n",
      "555/555 [==============================] - 35s 63ms/step - loss: 1.6758 - accuracy: 0.4772 - val_loss: 1.4447 - val_accuracy: 0.5507\n",
      "Epoch 63/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6672 - accuracy: 0.4798\n",
      "Epoch 00063: loss improved from 1.67584 to 1.66724, saving model to ./Checkpoints\\weights-improvement-epoch_63-loss_1.6672-accuracy_0.4798.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.6672 - accuracy: 0.4798 - val_loss: 1.4374 - val_accuracy: 0.5506\n",
      "Epoch 64/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6605 - accuracy: 0.4817\n",
      "Epoch 00064: loss improved from 1.66724 to 1.66069, saving model to ./Checkpoints\\weights-improvement-epoch_64-loss_1.6607-accuracy_0.4817.hdf5\n",
      "555/555 [==============================] - 39s 71ms/step - loss: 1.6607 - accuracy: 0.4817 - val_loss: 1.4340 - val_accuracy: 0.5494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6477 - accuracy: 0.4831\n",
      "Epoch 00065: loss improved from 1.66069 to 1.64772, saving model to ./Checkpoints\\weights-improvement-epoch_65-loss_1.6477-accuracy_0.4831.hdf5\n",
      "555/555 [==============================] - 37s 67ms/step - loss: 1.6477 - accuracy: 0.4831 - val_loss: 1.4192 - val_accuracy: 0.5545\n",
      "Epoch 66/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6442 - accuracy: 0.4860\n",
      "Epoch 00066: loss improved from 1.64772 to 1.64413, saving model to ./Checkpoints\\weights-improvement-epoch_66-loss_1.6441-accuracy_0.4860.hdf5\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 1.6441 - accuracy: 0.4860 - val_loss: 1.4123 - val_accuracy: 0.5588\n",
      "Epoch 67/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6383 - accuracy: 0.4904\n",
      "Epoch 00067: loss improved from 1.64413 to 1.63839, saving model to ./Checkpoints\\weights-improvement-epoch_67-loss_1.6384-accuracy_0.4904.hdf5\n",
      "555/555 [==============================] - 37s 66ms/step - loss: 1.6384 - accuracy: 0.4904 - val_loss: 1.4092 - val_accuracy: 0.5580\n",
      "Epoch 68/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6338 - accuracy: 0.4908\n",
      "Epoch 00068: loss improved from 1.63839 to 1.63384, saving model to ./Checkpoints\\weights-improvement-epoch_68-loss_1.6338-accuracy_0.4909.hdf5\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 1.6338 - accuracy: 0.4909 - val_loss: 1.4052 - val_accuracy: 0.5594\n",
      "Epoch 69/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6275 - accuracy: 0.4894\n",
      "Epoch 00069: loss improved from 1.63384 to 1.62781, saving model to ./Checkpoints\\weights-improvement-epoch_69-loss_1.6278-accuracy_0.4893.hdf5\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 1.6278 - accuracy: 0.4893 - val_loss: 1.3863 - val_accuracy: 0.5680\n",
      "Epoch 70/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6235 - accuracy: 0.4931\n",
      "Epoch 00070: loss improved from 1.62781 to 1.62330, saving model to ./Checkpoints\\weights-improvement-epoch_70-loss_1.6233-accuracy_0.4932.hdf5\n",
      "555/555 [==============================] - 36s 64ms/step - loss: 1.6233 - accuracy: 0.4932 - val_loss: 1.3760 - val_accuracy: 0.5691\n",
      "Epoch 71/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6162 - accuracy: 0.4945\n",
      "Epoch 00071: loss improved from 1.62330 to 1.61617, saving model to ./Checkpoints\\weights-improvement-epoch_71-loss_1.6162-accuracy_0.4944.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 1.6162 - accuracy: 0.4944 - val_loss: 1.3721 - val_accuracy: 0.5705\n",
      "Epoch 72/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6073 - accuracy: 0.4936\n",
      "Epoch 00072: loss improved from 1.61617 to 1.60730, saving model to ./Checkpoints\\weights-improvement-epoch_72-loss_1.6073-accuracy_0.4936.hdf5\n",
      "555/555 [==============================] - 32s 57ms/step - loss: 1.6073 - accuracy: 0.4936 - val_loss: 1.3669 - val_accuracy: 0.5691\n",
      "Epoch 73/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6041 - accuracy: 0.4953\n",
      "Epoch 00073: loss improved from 1.60730 to 1.60410, saving model to ./Checkpoints\\weights-improvement-epoch_73-loss_1.6041-accuracy_0.4953.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.6041 - accuracy: 0.4953 - val_loss: 1.3646 - val_accuracy: 0.5720\n",
      "Epoch 74/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5928 - accuracy: 0.5006\n",
      "Epoch 00074: loss improved from 1.60410 to 1.59279, saving model to ./Checkpoints\\weights-improvement-epoch_74-loss_1.5928-accuracy_0.5006.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.5928 - accuracy: 0.5006 - val_loss: 1.3540 - val_accuracy: 0.5741\n",
      "Epoch 75/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5908 - accuracy: 0.5000\n",
      "Epoch 00075: loss improved from 1.59279 to 1.59082, saving model to ./Checkpoints\\weights-improvement-epoch_75-loss_1.5908-accuracy_0.5000.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.5908 - accuracy: 0.5000 - val_loss: 1.3412 - val_accuracy: 0.5775\n",
      "Epoch 76/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5875 - accuracy: 0.5025\n",
      "Epoch 00076: loss improved from 1.59082 to 1.58747, saving model to ./Checkpoints\\weights-improvement-epoch_76-loss_1.5875-accuracy_0.5025.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.5875 - accuracy: 0.5025 - val_loss: 1.3288 - val_accuracy: 0.5772\n",
      "Epoch 77/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5780 - accuracy: 0.5037\n",
      "Epoch 00077: loss improved from 1.58747 to 1.57797, saving model to ./Checkpoints\\weights-improvement-epoch_77-loss_1.5780-accuracy_0.5037.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.5780 - accuracy: 0.5037 - val_loss: 1.3273 - val_accuracy: 0.5791\n",
      "Epoch 78/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5752 - accuracy: 0.5046\n",
      "Epoch 00078: loss improved from 1.57797 to 1.57517, saving model to ./Checkpoints\\weights-improvement-epoch_78-loss_1.5752-accuracy_0.5046.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.5752 - accuracy: 0.5046 - val_loss: 1.3250 - val_accuracy: 0.5828\n",
      "Epoch 79/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5644 - accuracy: 0.5058\n",
      "Epoch 00079: loss improved from 1.57517 to 1.56440, saving model to ./Checkpoints\\weights-improvement-epoch_79-loss_1.5644-accuracy_0.5058.hdf5\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.5644 - accuracy: 0.5058 - val_loss: 1.3088 - val_accuracy: 0.5887\n",
      "Epoch 80/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.5650 - accuracy: 0.5050\n",
      "Epoch 00080: loss did not improve from 1.56440\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 1.5648 - accuracy: 0.5050 - val_loss: 1.3137 - val_accuracy: 0.5845\n",
      "Epoch 81/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.5531 - accuracy: 0.5095\n",
      "Epoch 00081: loss improved from 1.56440 to 1.55310, saving model to ./Checkpoints\\weights-improvement-epoch_81-loss_1.5531-accuracy_0.5095.hdf5\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 1.5531 - accuracy: 0.5095 - val_loss: 1.3002 - val_accuracy: 0.5891\n",
      "Epoch 82/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.5480 - accuracy: 0.5118\n",
      "Epoch 00082: loss improved from 1.55310 to 1.54798, saving model to ./Checkpoints\\weights-improvement-epoch_82-loss_1.5480-accuracy_0.5118.hdf5\n",
      "555/555 [==============================] - 36s 66ms/step - loss: 1.5480 - accuracy: 0.5118 - val_loss: 1.2992 - val_accuracy: 0.5896\n",
      "Epoch 83/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5530 - accuracy: 0.5102\n",
      "Epoch 00083: loss did not improve from 1.54798\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 1.5530 - accuracy: 0.5102 - val_loss: 1.2822 - val_accuracy: 0.5954\n",
      "Epoch 84/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5362 - accuracy: 0.5135\n",
      "Epoch 00084: loss improved from 1.54798 to 1.53620, saving model to ./Checkpoints\\weights-improvement-epoch_84-loss_1.5362-accuracy_0.5135.hdf5\n",
      "555/555 [==============================] - 38s 68ms/step - loss: 1.5362 - accuracy: 0.5135 - val_loss: 1.2772 - val_accuracy: 0.5966\n",
      "Epoch 85/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5349 - accuracy: 0.5167\n",
      "Epoch 00085: loss improved from 1.53620 to 1.53493, saving model to ./Checkpoints\\weights-improvement-epoch_85-loss_1.5349-accuracy_0.5167.hdf5\n",
      "555/555 [==============================] - 37s 66ms/step - loss: 1.5349 - accuracy: 0.5167 - val_loss: 1.2827 - val_accuracy: 0.5950\n",
      "Epoch 86/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5319 - accuracy: 0.5135\n",
      "Epoch 00086: loss improved from 1.53493 to 1.53191, saving model to ./Checkpoints\\weights-improvement-epoch_86-loss_1.5319-accuracy_0.5135.hdf5\n",
      "555/555 [==============================] - 39s 71ms/step - loss: 1.5319 - accuracy: 0.5135 - val_loss: 1.2662 - val_accuracy: 0.6010\n",
      "Epoch 87/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.5223 - accuracy: 0.5164\n",
      "Epoch 00087: loss improved from 1.53191 to 1.52223, saving model to ./Checkpoints\\weights-improvement-epoch_87-loss_1.5222-accuracy_0.5164.hdf5\n",
      "555/555 [==============================] - 34s 62ms/step - loss: 1.5222 - accuracy: 0.5164 - val_loss: 1.2717 - val_accuracy: 0.5967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.5217 - accuracy: 0.5203\n",
      "Epoch 00088: loss improved from 1.52223 to 1.52167, saving model to ./Checkpoints\\weights-improvement-epoch_88-loss_1.5217-accuracy_0.5203.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.5217 - accuracy: 0.5203 - val_loss: 1.2550 - val_accuracy: 0.6014\n",
      "Epoch 89/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.5187 - accuracy: 0.5175\n",
      "Epoch 00089: loss improved from 1.52167 to 1.51874, saving model to ./Checkpoints\\weights-improvement-epoch_89-loss_1.5187-accuracy_0.5175.hdf5\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 1.5187 - accuracy: 0.5175 - val_loss: 1.2510 - val_accuracy: 0.6037\n",
      "Epoch 90/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5087 - accuracy: 0.5202\n",
      "Epoch 00090: loss improved from 1.51874 to 1.50870, saving model to ./Checkpoints\\weights-improvement-epoch_90-loss_1.5087-accuracy_0.5202.hdf5\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 1.5087 - accuracy: 0.5202 - val_loss: 1.2408 - val_accuracy: 0.6091\n",
      "Epoch 91/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5008 - accuracy: 0.5231\n",
      "Epoch 00091: loss improved from 1.50870 to 1.50083, saving model to ./Checkpoints\\weights-improvement-epoch_91-loss_1.5008-accuracy_0.5231.hdf5\n",
      "555/555 [==============================] - 39s 69ms/step - loss: 1.5008 - accuracy: 0.5231 - val_loss: 1.2491 - val_accuracy: 0.6030\n",
      "Epoch 92/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.5015 - accuracy: 0.5232\n",
      "Epoch 00092: loss did not improve from 1.50083\n",
      "555/555 [==============================] - 37s 67ms/step - loss: 1.5015 - accuracy: 0.5232 - val_loss: 1.2287 - val_accuracy: 0.6091\n",
      "Epoch 93/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5033 - accuracy: 0.5209\n",
      "Epoch 00093: loss did not improve from 1.50083\n",
      "555/555 [==============================] - 37s 67ms/step - loss: 1.5033 - accuracy: 0.5209 - val_loss: 1.2249 - val_accuracy: 0.6096\n",
      "Epoch 94/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4897 - accuracy: 0.5284\n",
      "Epoch 00094: loss improved from 1.50083 to 1.48975, saving model to ./Checkpoints\\weights-improvement-epoch_94-loss_1.4897-accuracy_0.5284.hdf5\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 1.4897 - accuracy: 0.5284 - val_loss: 1.2275 - val_accuracy: 0.6111\n",
      "Epoch 95/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.4810 - accuracy: 0.5281\n",
      "Epoch 00095: loss improved from 1.48975 to 1.48091, saving model to ./Checkpoints\\weights-improvement-epoch_95-loss_1.4809-accuracy_0.5281.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.4809 - accuracy: 0.5281 - val_loss: 1.2144 - val_accuracy: 0.6168\n",
      "Epoch 96/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4768 - accuracy: 0.5307\n",
      "Epoch 00096: loss improved from 1.48091 to 1.47677, saving model to ./Checkpoints\\weights-improvement-epoch_96-loss_1.4768-accuracy_0.5307.hdf5\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 1.4768 - accuracy: 0.5307 - val_loss: 1.2064 - val_accuracy: 0.6177\n",
      "Epoch 97/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4760 - accuracy: 0.5294\n",
      "Epoch 00097: loss improved from 1.47677 to 1.47601, saving model to ./Checkpoints\\weights-improvement-epoch_97-loss_1.4760-accuracy_0.5294.hdf5\n",
      "555/555 [==============================] - 30s 55ms/step - loss: 1.4760 - accuracy: 0.5294 - val_loss: 1.2057 - val_accuracy: 0.6177\n",
      "Epoch 98/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4722 - accuracy: 0.5311\n",
      "Epoch 00098: loss improved from 1.47601 to 1.47219, saving model to ./Checkpoints\\weights-improvement-epoch_98-loss_1.4722-accuracy_0.5311.hdf5\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.4722 - accuracy: 0.5311 - val_loss: 1.1876 - val_accuracy: 0.6235\n",
      "Epoch 99/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4762 - accuracy: 0.5300\n",
      "Epoch 00099: loss did not improve from 1.47219\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 1.4762 - accuracy: 0.5300 - val_loss: 1.1937 - val_accuracy: 0.6219\n",
      "Epoch 100/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4633 - accuracy: 0.5325\n",
      "Epoch 00100: loss improved from 1.47219 to 1.46330, saving model to ./Checkpoints\\weights-improvement-epoch_100-loss_1.4633-accuracy_0.5325.hdf5\n",
      "555/555 [==============================] - 36s 64ms/step - loss: 1.4633 - accuracy: 0.5325 - val_loss: 1.1772 - val_accuracy: 0.6273\n",
      "Epoch 101/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.4554 - accuracy: 0.5339\n",
      "Epoch 00101: loss improved from 1.46330 to 1.45561, saving model to ./Checkpoints\\weights-improvement-epoch_101-loss_1.4556-accuracy_0.5339.hdf5\n",
      "555/555 [==============================] - 34s 62ms/step - loss: 1.4556 - accuracy: 0.5339 - val_loss: 1.1930 - val_accuracy: 0.6212\n",
      "Epoch 102/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4551 - accuracy: 0.5392\n",
      "Epoch 00102: loss improved from 1.45561 to 1.45513, saving model to ./Checkpoints\\weights-improvement-epoch_102-loss_1.4551-accuracy_0.5392.hdf5\n",
      "555/555 [==============================] - 32s 57ms/step - loss: 1.4551 - accuracy: 0.5392 - val_loss: 1.1834 - val_accuracy: 0.6241\n",
      "Epoch 103/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4481 - accuracy: 0.5386\n",
      "Epoch 00103: loss improved from 1.45513 to 1.44808, saving model to ./Checkpoints\\weights-improvement-epoch_103-loss_1.4481-accuracy_0.5386.hdf5\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 1.4481 - accuracy: 0.5386 - val_loss: 1.1748 - val_accuracy: 0.6295\n",
      "Epoch 104/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.4482 - accuracy: 0.5377\n",
      "Epoch 00104: loss improved from 1.44808 to 1.44804, saving model to ./Checkpoints\\weights-improvement-epoch_104-loss_1.4480-accuracy_0.5378.hdf5\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 1.4480 - accuracy: 0.5378 - val_loss: 1.1626 - val_accuracy: 0.6304\n",
      "Epoch 105/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.4346 - accuracy: 0.5440\n",
      "Epoch 00105: loss improved from 1.44804 to 1.43449, saving model to ./Checkpoints\\weights-improvement-epoch_105-loss_1.4345-accuracy_0.5440.hdf5\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 1.4345 - accuracy: 0.5440 - val_loss: 1.1536 - val_accuracy: 0.6327\n",
      "Epoch 106/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.4416 - accuracy: 0.5379\n",
      "Epoch 00106: loss did not improve from 1.43449\n",
      "555/555 [==============================] - 37s 67ms/step - loss: 1.4417 - accuracy: 0.5379 - val_loss: 1.1580 - val_accuracy: 0.6330\n",
      "Epoch 107/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4382 - accuracy: 0.5412\n",
      "Epoch 00107: loss did not improve from 1.43449\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 1.4382 - accuracy: 0.5412 - val_loss: 1.1516 - val_accuracy: 0.6357\n",
      "Epoch 108/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4297 - accuracy: 0.5436\n",
      "Epoch 00108: loss improved from 1.43449 to 1.42968, saving model to ./Checkpoints\\weights-improvement-epoch_108-loss_1.4297-accuracy_0.5436.hdf5\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 1.4297 - accuracy: 0.5436 - val_loss: 1.1473 - val_accuracy: 0.6352\n",
      "Epoch 109/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4278 - accuracy: 0.5469\n",
      "Epoch 00109: loss improved from 1.42968 to 1.42781, saving model to ./Checkpoints\\weights-improvement-epoch_109-loss_1.4278-accuracy_0.5469.hdf5\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.4278 - accuracy: 0.5469 - val_loss: 1.1368 - val_accuracy: 0.6388\n",
      "Epoch 110/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4215 - accuracy: 0.5454\n",
      "Epoch 00110: loss improved from 1.42781 to 1.42146, saving model to ./Checkpoints\\weights-improvement-epoch_110-loss_1.4215-accuracy_0.5454.hdf5\n",
      "555/555 [==============================] - 31s 57ms/step - loss: 1.4215 - accuracy: 0.5454 - val_loss: 1.1315 - val_accuracy: 0.6398\n",
      "Epoch 111/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/555 [==============================] - ETA: 0s - loss: 1.4180 - accuracy: 0.5454\n",
      "Epoch 00111: loss improved from 1.42146 to 1.41801, saving model to ./Checkpoints\\weights-improvement-epoch_111-loss_1.4180-accuracy_0.5454.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.4180 - accuracy: 0.5454 - val_loss: 1.1261 - val_accuracy: 0.6428\n",
      "Epoch 112/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4169 - accuracy: 0.5461\n",
      "Epoch 00112: loss improved from 1.41801 to 1.41688, saving model to ./Checkpoints\\weights-improvement-epoch_112-loss_1.4169-accuracy_0.5461.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.4169 - accuracy: 0.5461 - val_loss: 1.1259 - val_accuracy: 0.6424\n",
      "Epoch 113/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4071 - accuracy: 0.5502\n",
      "Epoch 00113: loss improved from 1.41688 to 1.40709, saving model to ./Checkpoints\\weights-improvement-epoch_113-loss_1.4071-accuracy_0.5502.hdf5\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 1.4071 - accuracy: 0.5502 - val_loss: 1.1227 - val_accuracy: 0.6436\n",
      "Epoch 114/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4118 - accuracy: 0.5480\n",
      "Epoch 00114: loss did not improve from 1.40709\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.4118 - accuracy: 0.5480 - val_loss: 1.1245 - val_accuracy: 0.6416\n",
      "Epoch 115/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3973 - accuracy: 0.5528\n",
      "Epoch 00115: loss improved from 1.40709 to 1.39730, saving model to ./Checkpoints\\weights-improvement-epoch_115-loss_1.3973-accuracy_0.5528.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3973 - accuracy: 0.5528 - val_loss: 1.1116 - val_accuracy: 0.6486\n",
      "Epoch 116/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3911 - accuracy: 0.5548\n",
      "Epoch 00116: loss improved from 1.39730 to 1.39107, saving model to ./Checkpoints\\weights-improvement-epoch_116-loss_1.3911-accuracy_0.5548.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3911 - accuracy: 0.5548 - val_loss: 1.1010 - val_accuracy: 0.6507\n",
      "Epoch 117/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3972 - accuracy: 0.5520 ETA: 1s - l\n",
      "Epoch 00117: loss did not improve from 1.39107\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 1.3972 - accuracy: 0.5520 - val_loss: 1.1023 - val_accuracy: 0.6501\n",
      "Epoch 118/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3903 - accuracy: 0.5541\n",
      "Epoch 00118: loss improved from 1.39107 to 1.39033, saving model to ./Checkpoints\\weights-improvement-epoch_118-loss_1.3903-accuracy_0.5541.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3903 - accuracy: 0.5541 - val_loss: 1.1052 - val_accuracy: 0.6473\n",
      "Epoch 119/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3829 - accuracy: 0.5571\n",
      "Epoch 00119: loss improved from 1.39033 to 1.38292, saving model to ./Checkpoints\\weights-improvement-epoch_119-loss_1.3829-accuracy_0.5571.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3829 - accuracy: 0.5571 - val_loss: 1.1161 - val_accuracy: 0.6444\n",
      "Epoch 120/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3854 - accuracy: 0.5538\n",
      "Epoch 00120: loss did not improve from 1.38292\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3854 - accuracy: 0.5538 - val_loss: 1.0932 - val_accuracy: 0.6529\n",
      "Epoch 121/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3754 - accuracy: 0.5576\n",
      "Epoch 00121: loss improved from 1.38292 to 1.37538, saving model to ./Checkpoints\\weights-improvement-epoch_121-loss_1.3754-accuracy_0.5576.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.3754 - accuracy: 0.5576 - val_loss: 1.0768 - val_accuracy: 0.6581\n",
      "Epoch 122/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.3738 - accuracy: 0.5579\n",
      "Epoch 00122: loss improved from 1.37538 to 1.37384, saving model to ./Checkpoints\\weights-improvement-epoch_122-loss_1.3738-accuracy_0.5579.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.3738 - accuracy: 0.5579 - val_loss: 1.0767 - val_accuracy: 0.6579\n",
      "Epoch 123/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3687 - accuracy: 0.5597\n",
      "Epoch 00123: loss improved from 1.37384 to 1.36874, saving model to ./Checkpoints\\weights-improvement-epoch_123-loss_1.3687-accuracy_0.5597.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3687 - accuracy: 0.5597 - val_loss: 1.0839 - val_accuracy: 0.6577\n",
      "Epoch 124/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3667 - accuracy: 0.5631 ETA: 0s - loss: 1\n",
      "Epoch 00124: loss improved from 1.36874 to 1.36675, saving model to ./Checkpoints\\weights-improvement-epoch_124-loss_1.3667-accuracy_0.5631.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3667 - accuracy: 0.5631 - val_loss: 1.0674 - val_accuracy: 0.6594\n",
      "Epoch 125/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3601 - accuracy: 0.5632\n",
      "Epoch 00125: loss improved from 1.36675 to 1.36012, saving model to ./Checkpoints\\weights-improvement-epoch_125-loss_1.3601-accuracy_0.5632.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3601 - accuracy: 0.5632 - val_loss: 1.0655 - val_accuracy: 0.6626\n",
      "Epoch 126/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3566 - accuracy: 0.5660\n",
      "Epoch 00126: loss improved from 1.36012 to 1.35662, saving model to ./Checkpoints\\weights-improvement-epoch_126-loss_1.3566-accuracy_0.5660.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3566 - accuracy: 0.5660 - val_loss: 1.0471 - val_accuracy: 0.6694\n",
      "Epoch 127/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3573 - accuracy: 0.5642\n",
      "Epoch 00127: loss did not improve from 1.35662\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3573 - accuracy: 0.5642 - val_loss: 1.0470 - val_accuracy: 0.6665\n",
      "Epoch 128/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3517 - accuracy: 0.5639\n",
      "Epoch 00128: loss improved from 1.35662 to 1.35174, saving model to ./Checkpoints\\weights-improvement-epoch_128-loss_1.3517-accuracy_0.5639.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3517 - accuracy: 0.5639 - val_loss: 1.0406 - val_accuracy: 0.6714\n",
      "Epoch 129/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3494 - accuracy: 0.5653\n",
      "Epoch 00129: loss improved from 1.35174 to 1.34944, saving model to ./Checkpoints\\weights-improvement-epoch_129-loss_1.3494-accuracy_0.5653.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3494 - accuracy: 0.5653 - val_loss: 1.0557 - val_accuracy: 0.6656\n",
      "Epoch 130/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3410 - accuracy: 0.5662\n",
      "Epoch 00130: loss improved from 1.34944 to 1.34104, saving model to ./Checkpoints\\weights-improvement-epoch_130-loss_1.3410-accuracy_0.5662.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3410 - accuracy: 0.5662 - val_loss: 1.0424 - val_accuracy: 0.6669\n",
      "Epoch 131/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3397 - accuracy: 0.5690\n",
      "Epoch 00131: loss improved from 1.34104 to 1.33968, saving model to ./Checkpoints\\weights-improvement-epoch_131-loss_1.3397-accuracy_0.5690.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3397 - accuracy: 0.5690 - val_loss: 1.0373 - val_accuracy: 0.6746\n",
      "Epoch 132/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3432 - accuracy: 0.5689\n",
      "Epoch 00132: loss did not improve from 1.33968\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3432 - accuracy: 0.5689 - val_loss: 1.0250 - val_accuracy: 0.6746\n",
      "Epoch 133/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3357 - accuracy: 0.5696\n",
      "Epoch 00133: loss improved from 1.33968 to 1.33571, saving model to ./Checkpoints\\weights-improvement-epoch_133-loss_1.3357-accuracy_0.5696.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3357 - accuracy: 0.5696 - val_loss: 1.0265 - val_accuracy: 0.6783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3266 - accuracy: 0.5718\n",
      "Epoch 00134: loss improved from 1.33571 to 1.32656, saving model to ./Checkpoints\\weights-improvement-epoch_134-loss_1.3266-accuracy_0.5718.hdf5\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 1.3266 - accuracy: 0.5718 - val_loss: 1.0168 - val_accuracy: 0.6789\n",
      "Epoch 135/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3271 - accuracy: 0.5729\n",
      "Epoch 00135: loss did not improve from 1.32656\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.3271 - accuracy: 0.5729 - val_loss: 1.0156 - val_accuracy: 0.6775\n",
      "Epoch 136/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3268 - accuracy: 0.5715\n",
      "Epoch 00136: loss did not improve from 1.32656\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 1.3268 - accuracy: 0.5715 - val_loss: 1.0171 - val_accuracy: 0.6776\n",
      "Epoch 137/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3216 - accuracy: 0.5743\n",
      "Epoch 00137: loss improved from 1.32656 to 1.32160, saving model to ./Checkpoints\\weights-improvement-epoch_137-loss_1.3216-accuracy_0.5743.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3216 - accuracy: 0.5743 - val_loss: 1.0138 - val_accuracy: 0.6784\n",
      "Epoch 138/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3254 - accuracy: 0.5742\n",
      "Epoch 00138: loss did not improve from 1.32160\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3254 - accuracy: 0.5742 - val_loss: 1.0178 - val_accuracy: 0.6813\n",
      "Epoch 139/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.3144 - accuracy: 0.5769 ETA\n",
      "Epoch 00139: loss improved from 1.32160 to 1.31436, saving model to ./Checkpoints\\weights-improvement-epoch_139-loss_1.3144-accuracy_0.5769.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.3144 - accuracy: 0.5769 - val_loss: 1.0062 - val_accuracy: 0.6836\n",
      "Epoch 140/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3126 - accuracy: 0.5743\n",
      "Epoch 00140: loss improved from 1.31436 to 1.31257, saving model to ./Checkpoints\\weights-improvement-epoch_140-loss_1.3126-accuracy_0.5743.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.3126 - accuracy: 0.5743 - val_loss: 1.0026 - val_accuracy: 0.6817\n",
      "Epoch 141/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3110 - accuracy: 0.5759\n",
      "Epoch 00141: loss improved from 1.31257 to 1.31100, saving model to ./Checkpoints\\weights-improvement-epoch_141-loss_1.3110-accuracy_0.5759.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.3110 - accuracy: 0.5759 - val_loss: 0.9939 - val_accuracy: 0.6861\n",
      "Epoch 142/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3080 - accuracy: 0.5767\n",
      "Epoch 00142: loss improved from 1.31100 to 1.30804, saving model to ./Checkpoints\\weights-improvement-epoch_142-loss_1.3080-accuracy_0.5767.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.3080 - accuracy: 0.5767 - val_loss: 0.9886 - val_accuracy: 0.6868\n",
      "Epoch 143/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3050 - accuracy: 0.5791\n",
      "Epoch 00143: loss improved from 1.30804 to 1.30502, saving model to ./Checkpoints\\weights-improvement-epoch_143-loss_1.3050-accuracy_0.5791.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.3050 - accuracy: 0.5791 - val_loss: 0.9863 - val_accuracy: 0.6883\n",
      "Epoch 144/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3014 - accuracy: 0.5792\n",
      "Epoch 00144: loss improved from 1.30502 to 1.30139, saving model to ./Checkpoints\\weights-improvement-epoch_144-loss_1.3014-accuracy_0.5792.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.3014 - accuracy: 0.5792 - val_loss: 0.9778 - val_accuracy: 0.6919\n",
      "Epoch 145/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2982 - accuracy: 0.5800\n",
      "Epoch 00145: loss improved from 1.30139 to 1.29819, saving model to ./Checkpoints\\weights-improvement-epoch_145-loss_1.2982-accuracy_0.5800.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.2982 - accuracy: 0.5800 - val_loss: 0.9860 - val_accuracy: 0.6909\n",
      "Epoch 146/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2899 - accuracy: 0.5827\n",
      "Epoch 00146: loss improved from 1.29819 to 1.28991, saving model to ./Checkpoints\\weights-improvement-epoch_146-loss_1.2899-accuracy_0.5827.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.2899 - accuracy: 0.5827 - val_loss: 0.9907 - val_accuracy: 0.6857\n",
      "Epoch 147/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2917 - accuracy: 0.5805\n",
      "Epoch 00147: loss did not improve from 1.28991\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.2917 - accuracy: 0.5805 - val_loss: 0.9680 - val_accuracy: 0.6924\n",
      "Epoch 148/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2873 - accuracy: 0.5841\n",
      "Epoch 00148: loss improved from 1.28991 to 1.28733, saving model to ./Checkpoints\\weights-improvement-epoch_148-loss_1.2873-accuracy_0.5841.hdf5\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 1.2873 - accuracy: 0.5841 - val_loss: 0.9727 - val_accuracy: 0.6923\n",
      "Epoch 149/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2819 - accuracy: 0.5861\n",
      "Epoch 00149: loss improved from 1.28733 to 1.28193, saving model to ./Checkpoints\\weights-improvement-epoch_149-loss_1.2819-accuracy_0.5861.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.2819 - accuracy: 0.5861 - val_loss: 0.9531 - val_accuracy: 0.6991\n",
      "Epoch 150/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2771 - accuracy: 0.5865\n",
      "Epoch 00150: loss improved from 1.28193 to 1.27711, saving model to ./Checkpoints\\weights-improvement-epoch_150-loss_1.2771-accuracy_0.5865.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.2771 - accuracy: 0.5865 - val_loss: 0.9565 - val_accuracy: 0.6974\n",
      "Epoch 151/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2823 - accuracy: 0.5865 ETA: 1s\n",
      "Epoch 00151: loss did not improve from 1.27711\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.2823 - accuracy: 0.5865 - val_loss: 0.9693 - val_accuracy: 0.6952\n",
      "Epoch 152/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.2779 - accuracy: 0.5873\n",
      "Epoch 00152: loss did not improve from 1.27711\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.2781 - accuracy: 0.5873 - val_loss: 0.9474 - val_accuracy: 0.6981\n",
      "Epoch 153/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2683 - accuracy: 0.5895\n",
      "Epoch 00153: loss improved from 1.27711 to 1.26827, saving model to ./Checkpoints\\weights-improvement-epoch_153-loss_1.2683-accuracy_0.5895.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.2683 - accuracy: 0.5895 - val_loss: 0.9523 - val_accuracy: 0.6985\n",
      "Epoch 154/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2671 - accuracy: 0.5905\n",
      "Epoch 00154: loss improved from 1.26827 to 1.26710, saving model to ./Checkpoints\\weights-improvement-epoch_154-loss_1.2671-accuracy_0.5905.hdf5\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.2671 - accuracy: 0.5905 - val_loss: 0.9510 - val_accuracy: 0.6995\n",
      "Epoch 155/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2596 - accuracy: 0.5925\n",
      "Epoch 00155: loss improved from 1.26710 to 1.25960, saving model to ./Checkpoints\\weights-improvement-epoch_155-loss_1.2596-accuracy_0.5925.hdf5\n",
      "555/555 [==============================] - 32s 57ms/step - loss: 1.2596 - accuracy: 0.5925 - val_loss: 0.9351 - val_accuracy: 0.7042\n",
      "Epoch 156/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.2661 - accuracy: 0.5895\n",
      "Epoch 00156: loss did not improve from 1.25960\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 1.2661 - accuracy: 0.5895 - val_loss: 0.9398 - val_accuracy: 0.7029\n",
      "Epoch 157/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2603 - accuracy: 0.5933\n",
      "Epoch 00157: loss did not improve from 1.25960\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 1.2603 - accuracy: 0.5933 - val_loss: 0.9273 - val_accuracy: 0.7074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2625 - accuracy: 0.5899\n",
      "Epoch 00158: loss did not improve from 1.25960\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 1.2625 - accuracy: 0.5899 - val_loss: 0.9426 - val_accuracy: 0.7047\n",
      "Epoch 159/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2540 - accuracy: 0.5939\n",
      "Epoch 00159: loss improved from 1.25960 to 1.25400, saving model to ./Checkpoints\\weights-improvement-epoch_159-loss_1.2540-accuracy_0.5939.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 1.2540 - accuracy: 0.5939 - val_loss: 0.9235 - val_accuracy: 0.7109\n",
      "Epoch 160/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2616 - accuracy: 0.5906\n",
      "Epoch 00160: loss did not improve from 1.25400\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.2616 - accuracy: 0.5906 - val_loss: 0.9228 - val_accuracy: 0.7087\n",
      "Epoch 161/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2454 - accuracy: 0.5954\n",
      "Epoch 00161: loss improved from 1.25400 to 1.24541, saving model to ./Checkpoints\\weights-improvement-epoch_161-loss_1.2454-accuracy_0.5954.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.2454 - accuracy: 0.5954 - val_loss: 0.9239 - val_accuracy: 0.7100\n",
      "Epoch 162/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2438 - accuracy: 0.5965\n",
      "Epoch 00162: loss improved from 1.24541 to 1.24380, saving model to ./Checkpoints\\weights-improvement-epoch_162-loss_1.2438-accuracy_0.5965.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.2438 - accuracy: 0.5965 - val_loss: 0.9265 - val_accuracy: 0.7081\n",
      "Epoch 163/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2437 - accuracy: 0.5976\n",
      "Epoch 00163: loss improved from 1.24380 to 1.24369, saving model to ./Checkpoints\\weights-improvement-epoch_163-loss_1.2437-accuracy_0.5976.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.2437 - accuracy: 0.5976 - val_loss: 0.9147 - val_accuracy: 0.7102\n",
      "Epoch 164/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2406 - accuracy: 0.5963\n",
      "Epoch 00164: loss improved from 1.24369 to 1.24057, saving model to ./Checkpoints\\weights-improvement-epoch_164-loss_1.2406-accuracy_0.5963.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.2406 - accuracy: 0.5963 - val_loss: 0.9027 - val_accuracy: 0.7164\n",
      "Epoch 165/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2301 - accuracy: 0.5992\n",
      "Epoch 00165: loss improved from 1.24057 to 1.23013, saving model to ./Checkpoints\\weights-improvement-epoch_165-loss_1.2301-accuracy_0.5992.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 1.2301 - accuracy: 0.5992 - val_loss: 0.8919 - val_accuracy: 0.7170\n",
      "Epoch 166/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2321 - accuracy: 0.5982 ETA: 0s - loss: 1.2321 - \n",
      "Epoch 00166: loss did not improve from 1.23013\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 1.2321 - accuracy: 0.5982 - val_loss: 0.9011 - val_accuracy: 0.7131\n",
      "Epoch 167/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2315 - accuracy: 0.5999\n",
      "Epoch 00167: loss did not improve from 1.23013\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.2315 - accuracy: 0.5999 - val_loss: 0.8998 - val_accuracy: 0.7172\n",
      "Epoch 168/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2229 - accuracy: 0.6025\n",
      "Epoch 00168: loss improved from 1.23013 to 1.22295, saving model to ./Checkpoints\\weights-improvement-epoch_168-loss_1.2229-accuracy_0.6025.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.2229 - accuracy: 0.6025 - val_loss: 0.8871 - val_accuracy: 0.7191\n",
      "Epoch 169/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.2246 - accuracy: 0.6012\n",
      "Epoch 00169: loss did not improve from 1.22295\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.2249 - accuracy: 0.6011 - val_loss: 0.8906 - val_accuracy: 0.7216\n",
      "Epoch 170/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2212 - accuracy: 0.6039 ETA: 0s - loss: 1.2215 - accuracy: 0.\n",
      "Epoch 00170: loss improved from 1.22295 to 1.22115, saving model to ./Checkpoints\\weights-improvement-epoch_170-loss_1.2212-accuracy_0.6039.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.2212 - accuracy: 0.6039 - val_loss: 0.8942 - val_accuracy: 0.7181\n",
      "Epoch 171/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2237 - accuracy: 0.6020\n",
      "Epoch 00171: loss did not improve from 1.22115\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.2237 - accuracy: 0.6020 - val_loss: 0.8835 - val_accuracy: 0.7222\n",
      "Epoch 172/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2151 - accuracy: 0.6045\n",
      "Epoch 00172: loss improved from 1.22115 to 1.21508, saving model to ./Checkpoints\\weights-improvement-epoch_172-loss_1.2151-accuracy_0.6045.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.2151 - accuracy: 0.6045 - val_loss: 0.8894 - val_accuracy: 0.7207\n",
      "Epoch 173/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2069 - accuracy: 0.6060\n",
      "Epoch 00173: loss improved from 1.21508 to 1.20692, saving model to ./Checkpoints\\weights-improvement-epoch_173-loss_1.2069-accuracy_0.6060.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.2069 - accuracy: 0.6060 - val_loss: 0.8652 - val_accuracy: 0.7276\n",
      "Epoch 174/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2114 - accuracy: 0.6060\n",
      "Epoch 00174: loss did not improve from 1.20692\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.2114 - accuracy: 0.6060 - val_loss: 0.8742 - val_accuracy: 0.7225\n",
      "Epoch 175/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2120 - accuracy: 0.6048 ETA: 0s - loss: 1.2121 - \n",
      "Epoch 00175: loss did not improve from 1.20692\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.2120 - accuracy: 0.6048 - val_loss: 0.8760 - val_accuracy: 0.7255\n",
      "Epoch 176/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.2056 - accuracy: 0.6080\n",
      "Epoch 00176: loss improved from 1.20692 to 1.20561, saving model to ./Checkpoints\\weights-improvement-epoch_176-loss_1.2056-accuracy_0.6080.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.2056 - accuracy: 0.6080 - val_loss: 0.8618 - val_accuracy: 0.7310\n",
      "Epoch 177/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.2016 - accuracy: 0.6094\n",
      "Epoch 00177: loss improved from 1.20561 to 1.20173, saving model to ./Checkpoints\\weights-improvement-epoch_177-loss_1.2017-accuracy_0.6093.hdf5\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 1.2017 - accuracy: 0.6093 - val_loss: 0.8555 - val_accuracy: 0.7318\n",
      "Epoch 178/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1987 - accuracy: 0.6108\n",
      "Epoch 00178: loss improved from 1.20173 to 1.19867, saving model to ./Checkpoints\\weights-improvement-epoch_178-loss_1.1987-accuracy_0.6108.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.1987 - accuracy: 0.6108 - val_loss: 0.8544 - val_accuracy: 0.7298\n",
      "Epoch 179/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1956 - accuracy: 0.6109\n",
      "Epoch 00179: loss improved from 1.19867 to 1.19558, saving model to ./Checkpoints\\weights-improvement-epoch_179-loss_1.1956-accuracy_0.6109.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1956 - accuracy: 0.6109 - val_loss: 0.8586 - val_accuracy: 0.7291\n",
      "Epoch 180/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1913 - accuracy: 0.6112\n",
      "Epoch 00180: loss improved from 1.19558 to 1.19132, saving model to ./Checkpoints\\weights-improvement-epoch_180-loss_1.1913-accuracy_0.6112.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1913 - accuracy: 0.6112 - val_loss: 0.8487 - val_accuracy: 0.7340\n",
      "Epoch 181/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1928 - accuracy: 0.6133\n",
      "Epoch 00181: loss did not improve from 1.19132\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1928 - accuracy: 0.6133 - val_loss: 0.8554 - val_accuracy: 0.7306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1945 - accuracy: 0.6108\n",
      "Epoch 00182: loss did not improve from 1.19132\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1945 - accuracy: 0.6108 - val_loss: 0.8516 - val_accuracy: 0.7333\n",
      "Epoch 183/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1880 - accuracy: 0.6121\n",
      "Epoch 00183: loss improved from 1.19132 to 1.18797, saving model to ./Checkpoints\\weights-improvement-epoch_183-loss_1.1880-accuracy_0.6121.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1880 - accuracy: 0.6121 - val_loss: 0.8554 - val_accuracy: 0.7313\n",
      "Epoch 184/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1811 - accuracy: 0.6149\n",
      "Epoch 00184: loss improved from 1.18797 to 1.18112, saving model to ./Checkpoints\\weights-improvement-epoch_184-loss_1.1811-accuracy_0.6149.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1811 - accuracy: 0.6149 - val_loss: 0.8492 - val_accuracy: 0.7320\n",
      "Epoch 185/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1837 - accuracy: 0.6143\n",
      "Epoch 00185: loss did not improve from 1.18112\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1837 - accuracy: 0.6143 - val_loss: 0.8485 - val_accuracy: 0.7356\n",
      "Epoch 186/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1741 - accuracy: 0.6168\n",
      "Epoch 00186: loss improved from 1.18112 to 1.17436, saving model to ./Checkpoints\\weights-improvement-epoch_186-loss_1.1744-accuracy_0.6167.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1744 - accuracy: 0.6167 - val_loss: 0.8356 - val_accuracy: 0.7379\n",
      "Epoch 187/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1777 - accuracy: 0.6143\n",
      "Epoch 00187: loss did not improve from 1.17436\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1777 - accuracy: 0.6143 - val_loss: 0.8349 - val_accuracy: 0.7378\n",
      "Epoch 188/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1717 - accuracy: 0.6176\n",
      "Epoch 00188: loss improved from 1.17436 to 1.17170, saving model to ./Checkpoints\\weights-improvement-epoch_188-loss_1.1717-accuracy_0.6176.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1717 - accuracy: 0.6176 - val_loss: 0.8234 - val_accuracy: 0.7412\n",
      "Epoch 189/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1751 - accuracy: 0.6160\n",
      "Epoch 00189: loss did not improve from 1.17170\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1751 - accuracy: 0.6160 - val_loss: 0.8303 - val_accuracy: 0.7387\n",
      "Epoch 190/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1620 - accuracy: 0.6203\n",
      "Epoch 00190: loss improved from 1.17170 to 1.16204, saving model to ./Checkpoints\\weights-improvement-epoch_190-loss_1.1620-accuracy_0.6203.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1620 - accuracy: 0.6203 - val_loss: 0.8224 - val_accuracy: 0.7417\n",
      "Epoch 191/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1681 - accuracy: 0.6190\n",
      "Epoch 00191: loss did not improve from 1.16204\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1681 - accuracy: 0.6190 - val_loss: 0.8188 - val_accuracy: 0.7415\n",
      "Epoch 192/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1598 - accuracy: 0.6200\n",
      "Epoch 00192: loss improved from 1.16204 to 1.15974, saving model to ./Checkpoints\\weights-improvement-epoch_192-loss_1.1597-accuracy_0.6200.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1597 - accuracy: 0.6200 - val_loss: 0.8143 - val_accuracy: 0.7451\n",
      "Epoch 193/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1640 - accuracy: 0.6175\n",
      "Epoch 00193: loss did not improve from 1.15974\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1640 - accuracy: 0.6175 - val_loss: 0.8118 - val_accuracy: 0.7448\n",
      "Epoch 194/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1657 - accuracy: 0.6191\n",
      "Epoch 00194: loss did not improve from 1.15974\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1657 - accuracy: 0.6191 - val_loss: 0.8052 - val_accuracy: 0.7471\n",
      "Epoch 195/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1549 - accuracy: 0.6211 ETA: 0s - loss: 1.1545 - ac\n",
      "Epoch 00195: loss improved from 1.15974 to 1.15493, saving model to ./Checkpoints\\weights-improvement-epoch_195-loss_1.1549-accuracy_0.6211.hdf5\n",
      "555/555 [==============================] - 29s 51ms/step - loss: 1.1549 - accuracy: 0.6211 - val_loss: 0.8005 - val_accuracy: 0.7492\n",
      "Epoch 196/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1511 - accuracy: 0.6244\n",
      "Epoch 00196: loss improved from 1.15493 to 1.15106, saving model to ./Checkpoints\\weights-improvement-epoch_196-loss_1.1511-accuracy_0.6244.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1511 - accuracy: 0.6244 - val_loss: 0.7950 - val_accuracy: 0.7517\n",
      "Epoch 197/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1492 - accuracy: 0.6241\n",
      "Epoch 00197: loss improved from 1.15106 to 1.14916, saving model to ./Checkpoints\\weights-improvement-epoch_197-loss_1.1492-accuracy_0.6241.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1492 - accuracy: 0.6241 - val_loss: 0.7983 - val_accuracy: 0.7522\n",
      "Epoch 198/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1505 - accuracy: 0.6243\n",
      "Epoch 00198: loss did not improve from 1.14916\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1505 - accuracy: 0.6243 - val_loss: 0.8003 - val_accuracy: 0.7486\n",
      "Epoch 199/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1448 - accuracy: 0.6275\n",
      "Epoch 00199: loss improved from 1.14916 to 1.14476, saving model to ./Checkpoints\\weights-improvement-epoch_199-loss_1.1448-accuracy_0.6275.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1448 - accuracy: 0.6275 - val_loss: 0.7955 - val_accuracy: 0.7536\n",
      "Epoch 200/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1399 - accuracy: 0.6264\n",
      "Epoch 00200: loss improved from 1.14476 to 1.13984, saving model to ./Checkpoints\\weights-improvement-epoch_200-loss_1.1398-accuracy_0.6264.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1398 - accuracy: 0.6264 - val_loss: 0.7844 - val_accuracy: 0.7524\n",
      "Epoch 201/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1442 - accuracy: 0.6268\n",
      "Epoch 00201: loss did not improve from 1.13984\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1442 - accuracy: 0.6268 - val_loss: 0.7836 - val_accuracy: 0.7576\n",
      "Epoch 202/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1420 - accuracy: 0.6230\n",
      "Epoch 00202: loss did not improve from 1.13984\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1420 - accuracy: 0.6230 - val_loss: 0.7793 - val_accuracy: 0.7562\n",
      "Epoch 203/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1349 - accuracy: 0.6274\n",
      "Epoch 00203: loss improved from 1.13984 to 1.13492, saving model to ./Checkpoints\\weights-improvement-epoch_203-loss_1.1349-accuracy_0.6274.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1349 - accuracy: 0.6274 - val_loss: 0.7785 - val_accuracy: 0.7602\n",
      "Epoch 204/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1329 - accuracy: 0.6305\n",
      "Epoch 00204: loss improved from 1.13492 to 1.13282, saving model to ./Checkpoints\\weights-improvement-epoch_204-loss_1.1328-accuracy_0.6306.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1328 - accuracy: 0.6306 - val_loss: 0.7788 - val_accuracy: 0.7584\n",
      "Epoch 205/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1382 - accuracy: 0.6265\n",
      "Epoch 00205: loss did not improve from 1.13282\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1382 - accuracy: 0.6265 - val_loss: 0.7859 - val_accuracy: 0.7534\n",
      "Epoch 206/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1255 - accuracy: 0.6314\n",
      "Epoch 00206: loss improved from 1.13282 to 1.12571, saving model to ./Checkpoints\\weights-improvement-epoch_206-loss_1.1257-accuracy_0.6314.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 1.1257 - accuracy: 0.6314 - val_loss: 0.7799 - val_accuracy: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1282 - accuracy: 0.6302\n",
      "Epoch 00207: loss did not improve from 1.12571\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.1281 - accuracy: 0.6302 - val_loss: 0.7848 - val_accuracy: 0.7527\n",
      "Epoch 208/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1218 - accuracy: 0.6316\n",
      "Epoch 00208: loss improved from 1.12571 to 1.12197, saving model to ./Checkpoints\\weights-improvement-epoch_208-loss_1.1220-accuracy_0.6316.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1220 - accuracy: 0.6316 - val_loss: 0.7697 - val_accuracy: 0.7623\n",
      "Epoch 209/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1212 - accuracy: 0.6333\n",
      "Epoch 00209: loss improved from 1.12197 to 1.12117, saving model to ./Checkpoints\\weights-improvement-epoch_209-loss_1.1212-accuracy_0.6333.hdf5\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.1212 - accuracy: 0.6333 - val_loss: 0.7777 - val_accuracy: 0.7562\n",
      "Epoch 210/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1176 - accuracy: 0.6331\n",
      "Epoch 00210: loss improved from 1.12117 to 1.11764, saving model to ./Checkpoints\\weights-improvement-epoch_210-loss_1.1176-accuracy_0.6331.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.1176 - accuracy: 0.6331 - val_loss: 0.7685 - val_accuracy: 0.7591\n",
      "Epoch 211/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1117 - accuracy: 0.6348\n",
      "Epoch 00211: loss improved from 1.11764 to 1.11169, saving model to ./Checkpoints\\weights-improvement-epoch_211-loss_1.1117-accuracy_0.6348.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1117 - accuracy: 0.6348 - val_loss: 0.7595 - val_accuracy: 0.7601\n",
      "Epoch 212/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1150 - accuracy: 0.6346\n",
      "Epoch 00212: loss did not improve from 1.11169\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1151 - accuracy: 0.6346 - val_loss: 0.7588 - val_accuracy: 0.7608\n",
      "Epoch 213/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1120 - accuracy: 0.6350\n",
      "Epoch 00213: loss did not improve from 1.11169\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1120 - accuracy: 0.6350 - val_loss: 0.7436 - val_accuracy: 0.7676\n",
      "Epoch 214/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.1079 - accuracy: 0.6352\n",
      "Epoch 00214: loss improved from 1.11169 to 1.10796, saving model to ./Checkpoints\\weights-improvement-epoch_214-loss_1.1080-accuracy_0.6352.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.1080 - accuracy: 0.6352 - val_loss: 0.7532 - val_accuracy: 0.7645\n",
      "Epoch 215/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1023 - accuracy: 0.6387\n",
      "Epoch 00215: loss improved from 1.10796 to 1.10235, saving model to ./Checkpoints\\weights-improvement-epoch_215-loss_1.1023-accuracy_0.6387.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1023 - accuracy: 0.6387 - val_loss: 0.7436 - val_accuracy: 0.7696\n",
      "Epoch 216/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1067 - accuracy: 0.6376\n",
      "Epoch 00216: loss did not improve from 1.10235\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1067 - accuracy: 0.6376 - val_loss: 0.7614 - val_accuracy: 0.7647\n",
      "Epoch 217/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1036 - accuracy: 0.6370\n",
      "Epoch 00217: loss did not improve from 1.10235\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1036 - accuracy: 0.6370 - val_loss: 0.7387 - val_accuracy: 0.7705\n",
      "Epoch 218/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0997 - accuracy: 0.6387\n",
      "Epoch 00218: loss improved from 1.10235 to 1.09974, saving model to ./Checkpoints\\weights-improvement-epoch_218-loss_1.0997-accuracy_0.6387.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0997 - accuracy: 0.6387 - val_loss: 0.7467 - val_accuracy: 0.7681\n",
      "Epoch 219/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.1026 - accuracy: 0.6386\n",
      "Epoch 00219: loss did not improve from 1.09974\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.1026 - accuracy: 0.6386 - val_loss: 0.7379 - val_accuracy: 0.7708\n",
      "Epoch 220/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0981 - accuracy: 0.6385\n",
      "Epoch 00220: loss improved from 1.09974 to 1.09808, saving model to ./Checkpoints\\weights-improvement-epoch_220-loss_1.0981-accuracy_0.6385.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.0981 - accuracy: 0.6385 - val_loss: 0.7393 - val_accuracy: 0.7708\n",
      "Epoch 221/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0951 - accuracy: 0.6414\n",
      "Epoch 00221: loss improved from 1.09808 to 1.09502, saving model to ./Checkpoints\\weights-improvement-epoch_221-loss_1.0950-accuracy_0.6414.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 1.0950 - accuracy: 0.6414 - val_loss: 0.7292 - val_accuracy: 0.7734\n",
      "Epoch 222/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0910 - accuracy: 0.6404\n",
      "Epoch 00222: loss improved from 1.09502 to 1.09105, saving model to ./Checkpoints\\weights-improvement-epoch_222-loss_1.0910-accuracy_0.6404.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 1.0910 - accuracy: 0.6404 - val_loss: 0.7366 - val_accuracy: 0.7714\n",
      "Epoch 223/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0939 - accuracy: 0.6424\n",
      "Epoch 00223: loss did not improve from 1.09105\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.0939 - accuracy: 0.6424 - val_loss: 0.7318 - val_accuracy: 0.7723\n",
      "Epoch 224/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0893 - accuracy: 0.6418 ETA: 0s - loss: 1.0879 - ac\n",
      "Epoch 00224: loss improved from 1.09105 to 1.08933, saving model to ./Checkpoints\\weights-improvement-epoch_224-loss_1.0893-accuracy_0.6418.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.0893 - accuracy: 0.6418 - val_loss: 0.7232 - val_accuracy: 0.7769\n",
      "Epoch 225/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0884 - accuracy: 0.6428\n",
      "Epoch 00225: loss improved from 1.08933 to 1.08840, saving model to ./Checkpoints\\weights-improvement-epoch_225-loss_1.0884-accuracy_0.6428.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0884 - accuracy: 0.6428 - val_loss: 0.7230 - val_accuracy: 0.7753\n",
      "Epoch 226/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0911 - accuracy: 0.6424\n",
      "Epoch 00226: loss did not improve from 1.08840\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0911 - accuracy: 0.6424 - val_loss: 0.7196 - val_accuracy: 0.7766\n",
      "Epoch 227/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0841 - accuracy: 0.6437\n",
      "Epoch 00227: loss improved from 1.08840 to 1.08423, saving model to ./Checkpoints\\weights-improvement-epoch_227-loss_1.0842-accuracy_0.6437.hdf5\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 1.0842 - accuracy: 0.6437 - val_loss: 0.7186 - val_accuracy: 0.7757\n",
      "Epoch 228/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0801 - accuracy: 0.6468\n",
      "Epoch 00228: loss improved from 1.08423 to 1.08010, saving model to ./Checkpoints\\weights-improvement-epoch_228-loss_1.0801-accuracy_0.6468.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0801 - accuracy: 0.6468 - val_loss: 0.7142 - val_accuracy: 0.7785\n",
      "Epoch 229/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0830 - accuracy: 0.6447\n",
      "Epoch 00229: loss did not improve from 1.08010\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0830 - accuracy: 0.6447 - val_loss: 0.7142 - val_accuracy: 0.7802\n",
      "Epoch 230/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0821 - accuracy: 0.6436\n",
      "Epoch 00230: loss did not improve from 1.08010\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.0821 - accuracy: 0.6436 - val_loss: 0.7054 - val_accuracy: 0.7800\n",
      "Epoch 231/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0776 - accuracy: 0.6448\n",
      "Epoch 00231: loss improved from 1.08010 to 1.07759, saving model to ./Checkpoints\\weights-improvement-epoch_231-loss_1.0776-accuracy_0.6448.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 1.0776 - accuracy: 0.6448 - val_loss: 0.7096 - val_accuracy: 0.7792\n",
      "Epoch 232/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0736 - accuracy: 0.6474\n",
      "Epoch 00232: loss improved from 1.07759 to 1.07361, saving model to ./Checkpoints\\weights-improvement-epoch_232-loss_1.0736-accuracy_0.6474.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 1.0736 - accuracy: 0.6474 - val_loss: 0.7051 - val_accuracy: 0.7834\n",
      "Epoch 233/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0713 - accuracy: 0.6475\n",
      "Epoch 00233: loss improved from 1.07361 to 1.07132, saving model to ./Checkpoints\\weights-improvement-epoch_233-loss_1.0713-accuracy_0.6475.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.0713 - accuracy: 0.6475 - val_loss: 0.7000 - val_accuracy: 0.7810\n",
      "Epoch 234/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0733 - accuracy: 0.6468 ETA\n",
      "Epoch 00234: loss did not improve from 1.07132\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0733 - accuracy: 0.6468 - val_loss: 0.7078 - val_accuracy: 0.7802\n",
      "Epoch 235/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0632 - accuracy: 0.6505\n",
      "Epoch 00235: loss improved from 1.07132 to 1.06338, saving model to ./Checkpoints\\weights-improvement-epoch_235-loss_1.0634-accuracy_0.6504.hdf5\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.0634 - accuracy: 0.6504 - val_loss: 0.7008 - val_accuracy: 0.7820\n",
      "Epoch 236/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0655 - accuracy: 0.6486\n",
      "Epoch 00236: loss did not improve from 1.06338\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.0654 - accuracy: 0.6486 - val_loss: 0.7097 - val_accuracy: 0.7808\n",
      "Epoch 237/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0625 - accuracy: 0.6500\n",
      "Epoch 00237: loss improved from 1.06338 to 1.06245, saving model to ./Checkpoints\\weights-improvement-epoch_237-loss_1.0625-accuracy_0.6500.hdf5\n",
      "555/555 [==============================] - 35s 63ms/step - loss: 1.0625 - accuracy: 0.6500 - val_loss: 0.6856 - val_accuracy: 0.7867\n",
      "Epoch 238/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0588 - accuracy: 0.6526 ETA: 0s - loss: 1.0581 - accuracy: \n",
      "Epoch 00238: loss improved from 1.06245 to 1.05899, saving model to ./Checkpoints\\weights-improvement-epoch_238-loss_1.0590-accuracy_0.6525.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.0590 - accuracy: 0.6525 - val_loss: 0.6930 - val_accuracy: 0.7855\n",
      "Epoch 239/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0610 - accuracy: 0.6506\n",
      "Epoch 00239: loss did not improve from 1.05899\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0610 - accuracy: 0.6506 - val_loss: 0.6834 - val_accuracy: 0.7909\n",
      "Epoch 240/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0617 - accuracy: 0.6493\n",
      "Epoch 00240: loss did not improve from 1.05899\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0617 - accuracy: 0.6493 - val_loss: 0.6871 - val_accuracy: 0.7897\n",
      "Epoch 241/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0548 - accuracy: 0.6515\n",
      "Epoch 00241: loss improved from 1.05899 to 1.05476, saving model to ./Checkpoints\\weights-improvement-epoch_241-loss_1.0548-accuracy_0.6515.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0548 - accuracy: 0.6515 - val_loss: 0.6857 - val_accuracy: 0.7864\n",
      "Epoch 242/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0469 - accuracy: 0.6531\n",
      "Epoch 00242: loss improved from 1.05476 to 1.04689, saving model to ./Checkpoints\\weights-improvement-epoch_242-loss_1.0469-accuracy_0.6531.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0469 - accuracy: 0.6531 - val_loss: 0.6834 - val_accuracy: 0.7875\n",
      "Epoch 243/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0468 - accuracy: 0.6535\n",
      "Epoch 00243: loss improved from 1.04689 to 1.04677, saving model to ./Checkpoints\\weights-improvement-epoch_243-loss_1.0468-accuracy_0.6535.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0468 - accuracy: 0.6535 - val_loss: 0.6744 - val_accuracy: 0.7905\n",
      "Epoch 244/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0449 - accuracy: 0.6555\n",
      "Epoch 00244: loss improved from 1.04677 to 1.04492, saving model to ./Checkpoints\\weights-improvement-epoch_244-loss_1.0449-accuracy_0.6554.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.0449 - accuracy: 0.6554 - val_loss: 0.6705 - val_accuracy: 0.7924\n",
      "Epoch 245/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0493 - accuracy: 0.6547\n",
      "Epoch 00245: loss did not improve from 1.04492\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0493 - accuracy: 0.6547 - val_loss: 0.6756 - val_accuracy: 0.7908\n",
      "Epoch 246/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0429 - accuracy: 0.6577\n",
      "Epoch 00246: loss improved from 1.04492 to 1.04280, saving model to ./Checkpoints\\weights-improvement-epoch_246-loss_1.0428-accuracy_0.6577.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0428 - accuracy: 0.6577 - val_loss: 0.6803 - val_accuracy: 0.7903\n",
      "Epoch 247/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0423 - accuracy: 0.6564\n",
      "Epoch 00247: loss improved from 1.04280 to 1.04221, saving model to ./Checkpoints\\weights-improvement-epoch_247-loss_1.0422-accuracy_0.6564.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0422 - accuracy: 0.6564 - val_loss: 0.6736 - val_accuracy: 0.7907\n",
      "Epoch 248/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0443 - accuracy: 0.6541\n",
      "Epoch 00248: loss did not improve from 1.04221\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0443 - accuracy: 0.6541 - val_loss: 0.6751 - val_accuracy: 0.7897\n",
      "Epoch 249/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0433 - accuracy: 0.6555 E\n",
      "Epoch 00249: loss did not improve from 1.04221\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0433 - accuracy: 0.6555 - val_loss: 0.6650 - val_accuracy: 0.7946\n",
      "Epoch 250/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0390 - accuracy: 0.6574\n",
      "Epoch 00250: loss improved from 1.04221 to 1.03899, saving model to ./Checkpoints\\weights-improvement-epoch_250-loss_1.0390-accuracy_0.6574.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0390 - accuracy: 0.6574 - val_loss: 0.6707 - val_accuracy: 0.7920\n",
      "Epoch 251/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0387 - accuracy: 0.6546\n",
      "Epoch 00251: loss improved from 1.03899 to 1.03871, saving model to ./Checkpoints\\weights-improvement-epoch_251-loss_1.0387-accuracy_0.6546.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0387 - accuracy: 0.6546 - val_loss: 0.6726 - val_accuracy: 0.7937\n",
      "Epoch 252/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0327 - accuracy: 0.6599\n",
      "Epoch 00252: loss improved from 1.03871 to 1.03271, saving model to ./Checkpoints\\weights-improvement-epoch_252-loss_1.0327-accuracy_0.6599.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.0327 - accuracy: 0.6599 - val_loss: 0.6610 - val_accuracy: 0.7971\n",
      "Epoch 253/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0271 - accuracy: 0.6598\n",
      "Epoch 00253: loss improved from 1.03271 to 1.02711, saving model to ./Checkpoints\\weights-improvement-epoch_253-loss_1.0271-accuracy_0.6598.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0271 - accuracy: 0.6598 - val_loss: 0.6508 - val_accuracy: 0.8002\n",
      "Epoch 254/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0252 - accuracy: 0.6622\n",
      "Epoch 00254: loss improved from 1.02711 to 1.02519, saving model to ./Checkpoints\\weights-improvement-epoch_254-loss_1.0252-accuracy_0.6622.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0252 - accuracy: 0.6622 - val_loss: 0.6447 - val_accuracy: 0.8025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0225 - accuracy: 0.6625\n",
      "Epoch 00255: loss improved from 1.02519 to 1.02247, saving model to ./Checkpoints\\weights-improvement-epoch_255-loss_1.0225-accuracy_0.6625.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0225 - accuracy: 0.6625 - val_loss: 0.6545 - val_accuracy: 0.7998\n",
      "Epoch 256/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0262 - accuracy: 0.6608\n",
      "Epoch 00256: loss did not improve from 1.02247\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0262 - accuracy: 0.6608 - val_loss: 0.6462 - val_accuracy: 0.8027\n",
      "Epoch 257/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0292 - accuracy: 0.6606\n",
      "Epoch 00257: loss did not improve from 1.02247\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0292 - accuracy: 0.6606 - val_loss: 0.6512 - val_accuracy: 0.7996\n",
      "Epoch 258/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0173 - accuracy: 0.6615\n",
      "Epoch 00258: loss improved from 1.02247 to 1.01725, saving model to ./Checkpoints\\weights-improvement-epoch_258-loss_1.0173-accuracy_0.6615.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0173 - accuracy: 0.6615 - val_loss: 0.6425 - val_accuracy: 0.8055\n",
      "Epoch 259/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0209 - accuracy: 0.6620\n",
      "Epoch 00259: loss did not improve from 1.01725\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0209 - accuracy: 0.6620 - val_loss: 0.6474 - val_accuracy: 0.7993\n",
      "Epoch 260/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0108 - accuracy: 0.6665\n",
      "Epoch 00260: loss improved from 1.01725 to 1.01077, saving model to ./Checkpoints\\weights-improvement-epoch_260-loss_1.0108-accuracy_0.6665.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0108 - accuracy: 0.6665 - val_loss: 0.6325 - val_accuracy: 0.8052\n",
      "Epoch 261/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0099 - accuracy: 0.6671\n",
      "Epoch 00261: loss improved from 1.01077 to 1.00990, saving model to ./Checkpoints\\weights-improvement-epoch_261-loss_1.0099-accuracy_0.6671.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0099 - accuracy: 0.6671 - val_loss: 0.6308 - val_accuracy: 0.8047\n",
      "Epoch 262/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0127 - accuracy: 0.6666\n",
      "Epoch 00262: loss did not improve from 1.00990\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.0126 - accuracy: 0.6666 - val_loss: 0.6472 - val_accuracy: 0.8024\n",
      "Epoch 263/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0143 - accuracy: 0.6640\n",
      "Epoch 00263: loss did not improve from 1.00990\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.0143 - accuracy: 0.6640 - val_loss: 0.6440 - val_accuracy: 0.8008\n",
      "Epoch 264/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0204 - accuracy: 0.6630\n",
      "Epoch 00264: loss did not improve from 1.00990\n",
      "555/555 [==============================] - 30s 55ms/step - loss: 1.0204 - accuracy: 0.6630 - val_loss: 0.6300 - val_accuracy: 0.8087\n",
      "Epoch 265/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0067 - accuracy: 0.6669\n",
      "Epoch 00265: loss improved from 1.00990 to 1.00656, saving model to ./Checkpoints\\weights-improvement-epoch_265-loss_1.0066-accuracy_0.6670.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.0066 - accuracy: 0.6670 - val_loss: 0.6255 - val_accuracy: 0.8059\n",
      "Epoch 266/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0008 - accuracy: 0.6692\n",
      "Epoch 00266: loss improved from 1.00656 to 1.00085, saving model to ./Checkpoints\\weights-improvement-epoch_266-loss_1.0008-accuracy_0.6692.hdf5\n",
      "555/555 [==============================] - 32s 57ms/step - loss: 1.0008 - accuracy: 0.6692 - val_loss: 0.6316 - val_accuracy: 0.8051\n",
      "Epoch 267/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.0082 - accuracy: 0.6664\n",
      "Epoch 00267: loss did not improve from 1.00085\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.0080 - accuracy: 0.6665 - val_loss: 0.6245 - val_accuracy: 0.8097\n",
      "Epoch 268/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0014 - accuracy: 0.6698\n",
      "Epoch 00268: loss did not improve from 1.00085\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.0014 - accuracy: 0.6698 - val_loss: 0.6275 - val_accuracy: 0.8073\n",
      "Epoch 269/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0015 - accuracy: 0.6718\n",
      "Epoch 00269: loss did not improve from 1.00085\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.0015 - accuracy: 0.6718 - val_loss: 0.6295 - val_accuracy: 0.8067\n",
      "Epoch 270/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9996 - accuracy: 0.6691\n",
      "Epoch 00270: loss improved from 1.00085 to 0.99962, saving model to ./Checkpoints\\weights-improvement-epoch_270-loss_0.9996-accuracy_0.6691.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.9996 - accuracy: 0.6691 - val_loss: 0.6222 - val_accuracy: 0.8088\n",
      "Epoch 271/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9968 - accuracy: 0.6725\n",
      "Epoch 00271: loss improved from 0.99962 to 0.99683, saving model to ./Checkpoints\\weights-improvement-epoch_271-loss_0.9968-accuracy_0.6725.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.9968 - accuracy: 0.6725 - val_loss: 0.6186 - val_accuracy: 0.8099\n",
      "Epoch 272/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.0072 - accuracy: 0.6665\n",
      "Epoch 00272: loss did not improve from 0.99683\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.0072 - accuracy: 0.6665 - val_loss: 0.6205 - val_accuracy: 0.8110\n",
      "Epoch 273/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9963 - accuracy: 0.6697\n",
      "Epoch 00273: loss improved from 0.99683 to 0.99634, saving model to ./Checkpoints\\weights-improvement-epoch_273-loss_0.9963-accuracy_0.6697.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.9963 - accuracy: 0.6697 - val_loss: 0.6163 - val_accuracy: 0.8121\n",
      "Epoch 274/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9836 - accuracy: 0.6737\n",
      "Epoch 00274: loss improved from 0.99634 to 0.98365, saving model to ./Checkpoints\\weights-improvement-epoch_274-loss_0.9836-accuracy_0.6737.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.9836 - accuracy: 0.6737 - val_loss: 0.6093 - val_accuracy: 0.8127\n",
      "Epoch 275/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9892 - accuracy: 0.6739\n",
      "Epoch 00275: loss did not improve from 0.98365\n",
      "555/555 [==============================] - 29s 51ms/step - loss: 0.9892 - accuracy: 0.6739 - val_loss: 0.6012 - val_accuracy: 0.8170\n",
      "Epoch 276/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9906 - accuracy: 0.6705 ETA: 1s - l\n",
      "Epoch 00276: loss did not improve from 0.98365\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.9906 - accuracy: 0.6705 - val_loss: 0.6137 - val_accuracy: 0.8131\n",
      "Epoch 277/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9958 - accuracy: 0.6708\n",
      "Epoch 00277: loss did not improve from 0.98365\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.9958 - accuracy: 0.6708 - val_loss: 0.6052 - val_accuracy: 0.8146\n",
      "Epoch 278/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9885 - accuracy: 0.6740\n",
      "Epoch 00278: loss did not improve from 0.98365\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.9885 - accuracy: 0.6740 - val_loss: 0.6053 - val_accuracy: 0.8154\n",
      "Epoch 279/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9846 - accuracy: 0.6737\n",
      "Epoch 00279: loss did not improve from 0.98365\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.9846 - accuracy: 0.6737 - val_loss: 0.6118 - val_accuracy: 0.8119\n",
      "Epoch 280/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9804 - accuracy: 0.6754\n",
      "Epoch 00280: loss improved from 0.98365 to 0.98042, saving model to ./Checkpoints\\weights-improvement-epoch_280-loss_0.9804-accuracy_0.6754.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.9804 - accuracy: 0.6754 - val_loss: 0.5950 - val_accuracy: 0.8163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9767 - accuracy: 0.6776\n",
      "Epoch 00281: loss improved from 0.98042 to 0.97670, saving model to ./Checkpoints\\weights-improvement-epoch_281-loss_0.9767-accuracy_0.6776.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.9767 - accuracy: 0.6776 - val_loss: 0.6064 - val_accuracy: 0.8152\n",
      "Epoch 282/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9793 - accuracy: 0.6757\n",
      "Epoch 00282: loss did not improve from 0.97670\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.9793 - accuracy: 0.6757 - val_loss: 0.5881 - val_accuracy: 0.8208\n",
      "Epoch 283/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9760 - accuracy: 0.6745\n",
      "Epoch 00283: loss improved from 0.97670 to 0.97611, saving model to ./Checkpoints\\weights-improvement-epoch_283-loss_0.9761-accuracy_0.6745.hdf5\n",
      "555/555 [==============================] - 32s 57ms/step - loss: 0.9761 - accuracy: 0.6745 - val_loss: 0.5947 - val_accuracy: 0.8192\n",
      "Epoch 284/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9750 - accuracy: 0.6766\n",
      "Epoch 00284: loss improved from 0.97611 to 0.97504, saving model to ./Checkpoints\\weights-improvement-epoch_284-loss_0.9750-accuracy_0.6766.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 0.9750 - accuracy: 0.6766 - val_loss: 0.5905 - val_accuracy: 0.8200\n",
      "Epoch 285/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9725 - accuracy: 0.6790\n",
      "Epoch 00285: loss improved from 0.97504 to 0.97253, saving model to ./Checkpoints\\weights-improvement-epoch_285-loss_0.9725-accuracy_0.6790.hdf5\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 0.9725 - accuracy: 0.6790 - val_loss: 0.5924 - val_accuracy: 0.8200\n",
      "Epoch 286/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9805 - accuracy: 0.6751\n",
      "Epoch 00286: loss did not improve from 0.97253\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.9805 - accuracy: 0.6751 - val_loss: 0.5953 - val_accuracy: 0.8178\n",
      "Epoch 287/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9702 - accuracy: 0.6778\n",
      "Epoch 00287: loss improved from 0.97253 to 0.97025, saving model to ./Checkpoints\\weights-improvement-epoch_287-loss_0.9702-accuracy_0.6778.hdf5\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 0.9702 - accuracy: 0.6778 - val_loss: 0.5901 - val_accuracy: 0.8203\n",
      "Epoch 288/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9650 - accuracy: 0.6835\n",
      "Epoch 00288: loss improved from 0.97025 to 0.96506, saving model to ./Checkpoints\\weights-improvement-epoch_288-loss_0.9651-accuracy_0.6835.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 0.9651 - accuracy: 0.6835 - val_loss: 0.5850 - val_accuracy: 0.8235\n",
      "Epoch 289/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9707 - accuracy: 0.6777 ETA\n",
      "Epoch 00289: loss did not improve from 0.96506\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 0.9707 - accuracy: 0.6777 - val_loss: 0.5835 - val_accuracy: 0.8219\n",
      "Epoch 290/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9651 - accuracy: 0.6809\n",
      "Epoch 00290: loss did not improve from 0.96506\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 0.9652 - accuracy: 0.6809 - val_loss: 0.5804 - val_accuracy: 0.8241\n",
      "Epoch 291/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9625 - accuracy: 0.6818\n",
      "Epoch 00291: loss improved from 0.96506 to 0.96248, saving model to ./Checkpoints\\weights-improvement-epoch_291-loss_0.9625-accuracy_0.6818.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.9625 - accuracy: 0.6818 - val_loss: 0.5749 - val_accuracy: 0.8256\n",
      "Epoch 292/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9654 - accuracy: 0.6802\n",
      "Epoch 00292: loss did not improve from 0.96248\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.9655 - accuracy: 0.6801 - val_loss: 0.5782 - val_accuracy: 0.8243\n",
      "Epoch 293/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9622 - accuracy: 0.6823\n",
      "Epoch 00293: loss improved from 0.96248 to 0.96216, saving model to ./Checkpoints\\weights-improvement-epoch_293-loss_0.9622-accuracy_0.6823.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 0.9622 - accuracy: 0.6823 - val_loss: 0.5859 - val_accuracy: 0.8181\n",
      "Epoch 294/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9632 - accuracy: 0.6804\n",
      "Epoch 00294: loss did not improve from 0.96216\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.9632 - accuracy: 0.6804 - val_loss: 0.5864 - val_accuracy: 0.8236\n",
      "Epoch 295/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9547 - accuracy: 0.6840\n",
      "Epoch 00295: loss improved from 0.96216 to 0.95467, saving model to ./Checkpoints\\weights-improvement-epoch_295-loss_0.9547-accuracy_0.6840.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 0.9547 - accuracy: 0.6840 - val_loss: 0.5643 - val_accuracy: 0.8293\n",
      "Epoch 296/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9537 - accuracy: 0.6815\n",
      "Epoch 00296: loss improved from 0.95467 to 0.95367, saving model to ./Checkpoints\\weights-improvement-epoch_296-loss_0.9537-accuracy_0.6815.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.9537 - accuracy: 0.6815 - val_loss: 0.5678 - val_accuracy: 0.8273\n",
      "Epoch 297/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9511 - accuracy: 0.6830\n",
      "Epoch 00297: loss improved from 0.95367 to 0.95103, saving model to ./Checkpoints\\weights-improvement-epoch_297-loss_0.9510-accuracy_0.6830.hdf5\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 0.9510 - accuracy: 0.6830 - val_loss: 0.5694 - val_accuracy: 0.8268\n",
      "Epoch 298/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9537 - accuracy: 0.6831 ETA: 0s - loss: 0.9523 - accu\n",
      "Epoch 00298: loss did not improve from 0.95103\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 0.9537 - accuracy: 0.6831 - val_loss: 0.5663 - val_accuracy: 0.8286\n",
      "Epoch 299/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9512 - accuracy: 0.6843\n",
      "Epoch 00299: loss did not improve from 0.95103\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.9512 - accuracy: 0.6843 - val_loss: 0.5694 - val_accuracy: 0.8242\n",
      "Epoch 300/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9539 - accuracy: 0.6853\n",
      "Epoch 00300: loss did not improve from 0.95103\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 0.9539 - accuracy: 0.6853 - val_loss: 0.5674 - val_accuracy: 0.8285\n",
      "Epoch 301/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9448 - accuracy: 0.6854\n",
      "Epoch 00301: loss improved from 0.95103 to 0.94478, saving model to ./Checkpoints\\weights-improvement-epoch_301-loss_0.9448-accuracy_0.6854.hdf5\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 0.9448 - accuracy: 0.6854 - val_loss: 0.5638 - val_accuracy: 0.8292\n",
      "Epoch 302/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9407 - accuracy: 0.6852\n",
      "Epoch 00302: loss improved from 0.94478 to 0.94067, saving model to ./Checkpoints\\weights-improvement-epoch_302-loss_0.9407-accuracy_0.6852.hdf5\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 0.9407 - accuracy: 0.6852 - val_loss: 0.5656 - val_accuracy: 0.8283\n",
      "Epoch 303/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9497 - accuracy: 0.6840\n",
      "Epoch 00303: loss did not improve from 0.94067\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.9497 - accuracy: 0.6840 - val_loss: 0.5522 - val_accuracy: 0.8345\n",
      "Epoch 304/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9416 - accuracy: 0.6880\n",
      "Epoch 00304: loss did not improve from 0.94067\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 0.9416 - accuracy: 0.6880 - val_loss: 0.5544 - val_accuracy: 0.8318\n",
      "Epoch 305/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9409 - accuracy: 0.6869\n",
      "Epoch 00305: loss did not improve from 0.94067\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 0.9409 - accuracy: 0.6869 - val_loss: 0.5573 - val_accuracy: 0.8313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9371 - accuracy: 0.6894\n",
      "Epoch 00306: loss improved from 0.94067 to 0.93708, saving model to ./Checkpoints\\weights-improvement-epoch_306-loss_0.9371-accuracy_0.6894.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.9371 - accuracy: 0.6894 - val_loss: 0.5511 - val_accuracy: 0.8341\n",
      "Epoch 307/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9438 - accuracy: 0.6886\n",
      "Epoch 00307: loss did not improve from 0.93708\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 0.9438 - accuracy: 0.6886 - val_loss: 0.5503 - val_accuracy: 0.8329\n",
      "Epoch 308/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9357 - accuracy: 0.6882\n",
      "Epoch 00308: loss improved from 0.93708 to 0.93572, saving model to ./Checkpoints\\weights-improvement-epoch_308-loss_0.9357-accuracy_0.6882.hdf5\n",
      "555/555 [==============================] - 35s 63ms/step - loss: 0.9357 - accuracy: 0.6882 - val_loss: 0.5360 - val_accuracy: 0.8384\n",
      "Epoch 309/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9339 - accuracy: 0.6927 ETA: 1s -\n",
      "Epoch 00309: loss improved from 0.93572 to 0.93390, saving model to ./Checkpoints\\weights-improvement-epoch_309-loss_0.9339-accuracy_0.6927.hdf5\n",
      "555/555 [==============================] - 35s 62ms/step - loss: 0.9339 - accuracy: 0.6927 - val_loss: 0.5453 - val_accuracy: 0.8372\n",
      "Epoch 310/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9292 - accuracy: 0.6923 ETA: 0s - loss: 0.9291 - accu\n",
      "Epoch 00310: loss improved from 0.93390 to 0.92927, saving model to ./Checkpoints\\weights-improvement-epoch_310-loss_0.9293-accuracy_0.6923.hdf5\n",
      "555/555 [==============================] - 35s 63ms/step - loss: 0.9293 - accuracy: 0.6923 - val_loss: 0.5494 - val_accuracy: 0.8360\n",
      "Epoch 311/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9313 - accuracy: 0.6907\n",
      "Epoch 00311: loss did not improve from 0.92927\n",
      "555/555 [==============================] - 36s 64ms/step - loss: 0.9313 - accuracy: 0.6907 - val_loss: 0.5498 - val_accuracy: 0.8335\n",
      "Epoch 312/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9290 - accuracy: 0.6905\n",
      "Epoch 00312: loss improved from 0.92927 to 0.92903, saving model to ./Checkpoints\\weights-improvement-epoch_312-loss_0.9290-accuracy_0.6905.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 0.9290 - accuracy: 0.6905 - val_loss: 0.5329 - val_accuracy: 0.8384\n",
      "Epoch 313/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9214 - accuracy: 0.6929\n",
      "Epoch 00313: loss improved from 0.92903 to 0.92136, saving model to ./Checkpoints\\weights-improvement-epoch_313-loss_0.9214-accuracy_0.6929.hdf5\n",
      "555/555 [==============================] - 34s 62ms/step - loss: 0.9214 - accuracy: 0.6929 - val_loss: 0.5357 - val_accuracy: 0.8382\n",
      "Epoch 314/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9251 - accuracy: 0.6934\n",
      "Epoch 00314: loss did not improve from 0.92136\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 0.9250 - accuracy: 0.6934 - val_loss: 0.5364 - val_accuracy: 0.8407\n",
      "Epoch 315/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9209 - accuracy: 0.6949\n",
      "Epoch 00315: loss improved from 0.92136 to 0.92089, saving model to ./Checkpoints\\weights-improvement-epoch_315-loss_0.9209-accuracy_0.6949.hdf5\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 0.9209 - accuracy: 0.6949 - val_loss: 0.5286 - val_accuracy: 0.8441\n",
      "Epoch 316/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9311 - accuracy: 0.6925\n",
      "Epoch 00316: loss did not improve from 0.92089\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 0.9311 - accuracy: 0.6925 - val_loss: 0.5408 - val_accuracy: 0.8361\n",
      "Epoch 317/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9253 - accuracy: 0.6923\n",
      "Epoch 00317: loss did not improve from 0.92089\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.9253 - accuracy: 0.6923 - val_loss: 0.5570 - val_accuracy: 0.8283\n",
      "Epoch 318/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9229 - accuracy: 0.6964\n",
      "Epoch 00318: loss did not improve from 0.92089\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.9229 - accuracy: 0.6964 - val_loss: 0.5257 - val_accuracy: 0.8436\n",
      "Epoch 319/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9186 - accuracy: 0.6958\n",
      "Epoch 00319: loss improved from 0.92089 to 0.91864, saving model to ./Checkpoints\\weights-improvement-epoch_319-loss_0.9186-accuracy_0.6958.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.9186 - accuracy: 0.6958 - val_loss: 0.5277 - val_accuracy: 0.8409\n",
      "Epoch 320/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9201 - accuracy: 0.6932\n",
      "Epoch 00320: loss did not improve from 0.91864\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.9200 - accuracy: 0.6932 - val_loss: 0.5370 - val_accuracy: 0.8388\n",
      "Epoch 321/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9103 - accuracy: 0.6972\n",
      "Epoch 00321: loss improved from 0.91864 to 0.91025, saving model to ./Checkpoints\\weights-improvement-epoch_321-loss_0.9103-accuracy_0.6972.hdf5\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 0.9103 - accuracy: 0.6972 - val_loss: 0.5222 - val_accuracy: 0.8443\n",
      "Epoch 322/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9182 - accuracy: 0.6972\n",
      "Epoch 00322: loss did not improve from 0.91025\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.9182 - accuracy: 0.6972 - val_loss: 0.5268 - val_accuracy: 0.8430\n",
      "Epoch 323/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.9067 - accuracy: 0.6986\n",
      "Epoch 00323: loss improved from 0.91025 to 0.90671, saving model to ./Checkpoints\\weights-improvement-epoch_323-loss_0.9067-accuracy_0.6986.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.9067 - accuracy: 0.6986 - val_loss: 0.5174 - val_accuracy: 0.8441\n",
      "Epoch 324/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9137 - accuracy: 0.6970 ETA: 1s - loss: 0.9125 -  - ETA: 0s - l\n",
      "Epoch 00324: loss did not improve from 0.90671\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 0.9137 - accuracy: 0.6970 - val_loss: 0.5183 - val_accuracy: 0.8458\n",
      "Epoch 325/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9101 - accuracy: 0.6997 ETA: 0s - loss: 0.9099 - accuracy\n",
      "Epoch 00325: loss did not improve from 0.90671\n",
      "555/555 [==============================] - 41s 74ms/step - loss: 0.9100 - accuracy: 0.6997 - val_loss: 0.5194 - val_accuracy: 0.8456\n",
      "Epoch 326/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9013 - accuracy: 0.6981\n",
      "Epoch 00326: loss improved from 0.90671 to 0.90129, saving model to ./Checkpoints\\weights-improvement-epoch_326-loss_0.9013-accuracy_0.6981.hdf5\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.9013 - accuracy: 0.6981 - val_loss: 0.5169 - val_accuracy: 0.8467\n",
      "Epoch 327/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9071 - accuracy: 0.6985\n",
      "Epoch 00327: loss did not improve from 0.90129\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 0.9071 - accuracy: 0.6985 - val_loss: 0.5150 - val_accuracy: 0.8454\n",
      "Epoch 328/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9047 - accuracy: 0.6994\n",
      "Epoch 00328: loss did not improve from 0.90129\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 0.9047 - accuracy: 0.6994 - val_loss: 0.5204 - val_accuracy: 0.8447\n",
      "Epoch 329/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9036 - accuracy: 0.7003\n",
      "Epoch 00329: loss did not improve from 0.90129\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 0.9037 - accuracy: 0.7003 - val_loss: 0.5150 - val_accuracy: 0.8451\n",
      "Epoch 330/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.9015 - accuracy: 0.7009\n",
      "Epoch 00330: loss did not improve from 0.90129\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.9015 - accuracy: 0.7009 - val_loss: 0.5104 - val_accuracy: 0.8468\n",
      "Epoch 331/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/555 [============================>.] - ETA: 0s - loss: 0.9014 - accuracy: 0.7002\n",
      "Epoch 00331: loss did not improve from 0.90129\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 0.9013 - accuracy: 0.7002 - val_loss: 0.5064 - val_accuracy: 0.8500\n",
      "Epoch 332/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8940 - accuracy: 0.7018\n",
      "Epoch 00332: loss improved from 0.90129 to 0.89396, saving model to ./Checkpoints\\weights-improvement-epoch_332-loss_0.8940-accuracy_0.7018.hdf5\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.8940 - accuracy: 0.7018 - val_loss: 0.5065 - val_accuracy: 0.8484\n",
      "Epoch 333/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8984 - accuracy: 0.7020\n",
      "Epoch 00333: loss did not improve from 0.89396\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 0.8984 - accuracy: 0.7020 - val_loss: 0.5174 - val_accuracy: 0.8435\n",
      "Epoch 334/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8980 - accuracy: 0.7005\n",
      "Epoch 00334: loss did not improve from 0.89396\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 0.8981 - accuracy: 0.7005 - val_loss: 0.4966 - val_accuracy: 0.8510\n",
      "Epoch 335/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8945 - accuracy: 0.7005\n",
      "Epoch 00335: loss did not improve from 0.89396\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.8946 - accuracy: 0.7005 - val_loss: 0.4990 - val_accuracy: 0.8518\n",
      "Epoch 336/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8957 - accuracy: 0.7023\n",
      "Epoch 00336: loss did not improve from 0.89396\n",
      "555/555 [==============================] - 42s 75ms/step - loss: 0.8958 - accuracy: 0.7022 - val_loss: 0.4991 - val_accuracy: 0.8500\n",
      "Epoch 337/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8964 - accuracy: 0.7029\n",
      "Epoch 00337: loss did not improve from 0.89396\n",
      "555/555 [==============================] - 38s 68ms/step - loss: 0.8963 - accuracy: 0.7029 - val_loss: 0.5077 - val_accuracy: 0.8491\n",
      "Epoch 338/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8857 - accuracy: 0.7055\n",
      "Epoch 00338: loss improved from 0.89396 to 0.88565, saving model to ./Checkpoints\\weights-improvement-epoch_338-loss_0.8856-accuracy_0.7056.hdf5\n",
      "555/555 [==============================] - 41s 74ms/step - loss: 0.8856 - accuracy: 0.7056 - val_loss: 0.4962 - val_accuracy: 0.8518\n",
      "Epoch 339/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8893 - accuracy: 0.7020\n",
      "Epoch 00339: loss did not improve from 0.88565\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 0.8893 - accuracy: 0.7020 - val_loss: 0.4999 - val_accuracy: 0.8510\n",
      "Epoch 340/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8965 - accuracy: 0.7024\n",
      "Epoch 00340: loss did not improve from 0.88565\n",
      "555/555 [==============================] - 50s 90ms/step - loss: 0.8965 - accuracy: 0.7024 - val_loss: 0.4942 - val_accuracy: 0.8545\n",
      "Epoch 341/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8865 - accuracy: 0.7062\n",
      "Epoch 00341: loss did not improve from 0.88565\n",
      "555/555 [==============================] - 51s 91ms/step - loss: 0.8865 - accuracy: 0.7062 - val_loss: 0.4944 - val_accuracy: 0.8564\n",
      "Epoch 342/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8863 - accuracy: 0.7054\n",
      "Epoch 00342: loss did not improve from 0.88565\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 0.8863 - accuracy: 0.7054 - val_loss: 0.4891 - val_accuracy: 0.8559\n",
      "Epoch 343/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8808 - accuracy: 0.7071\n",
      "Epoch 00343: loss improved from 0.88565 to 0.88075, saving model to ./Checkpoints\\weights-improvement-epoch_343-loss_0.8807-accuracy_0.7071.hdf5\n",
      "555/555 [==============================] - 51s 92ms/step - loss: 0.8807 - accuracy: 0.7071 - val_loss: 0.4927 - val_accuracy: 0.8528\n",
      "Epoch 344/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8819 - accuracy: 0.7071\n",
      "Epoch 00344: loss did not improve from 0.88075\n",
      "555/555 [==============================] - 65s 117ms/step - loss: 0.8820 - accuracy: 0.7071 - val_loss: 0.4949 - val_accuracy: 0.8526\n",
      "Epoch 345/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8862 - accuracy: 0.7045\n",
      "Epoch 00345: loss did not improve from 0.88075\n",
      "555/555 [==============================] - 61s 109ms/step - loss: 0.8862 - accuracy: 0.7045 - val_loss: 0.4923 - val_accuracy: 0.8552\n",
      "Epoch 346/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8724 - accuracy: 0.7085\n",
      "Epoch 00346: loss improved from 0.88075 to 0.87242, saving model to ./Checkpoints\\weights-improvement-epoch_346-loss_0.8724-accuracy_0.7085.hdf5\n",
      "555/555 [==============================] - 64s 115ms/step - loss: 0.8724 - accuracy: 0.7085 - val_loss: 0.4816 - val_accuracy: 0.8562\n",
      "Epoch 347/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8791 - accuracy: 0.7067\n",
      "Epoch 00347: loss did not improve from 0.87242\n",
      "555/555 [==============================] - 53s 95ms/step - loss: 0.8791 - accuracy: 0.7067 - val_loss: 0.4945 - val_accuracy: 0.8528\n",
      "Epoch 348/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8779 - accuracy: 0.7078\n",
      "Epoch 00348: loss did not improve from 0.87242\n",
      "555/555 [==============================] - 64s 115ms/step - loss: 0.8779 - accuracy: 0.7078 - val_loss: 0.4793 - val_accuracy: 0.8598\n",
      "Epoch 349/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8724 - accuracy: 0.7119 ETA: 0s - l\n",
      "Epoch 00349: loss did not improve from 0.87242\n",
      "555/555 [==============================] - 42s 75ms/step - loss: 0.8724 - accuracy: 0.7119 - val_loss: 0.4842 - val_accuracy: 0.8537\n",
      "Epoch 350/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8774 - accuracy: 0.7055 ETA: 1s - loss: 0.8 - ETA: 0s - loss: 0.8770 \n",
      "Epoch 00350: loss did not improve from 0.87242\n",
      "555/555 [==============================] - 40s 72ms/step - loss: 0.8774 - accuracy: 0.7055 - val_loss: 0.4861 - val_accuracy: 0.8550\n",
      "Epoch 351/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8735 - accuracy: 0.7087\n",
      "Epoch 00351: loss did not improve from 0.87242\n",
      "555/555 [==============================] - 50s 90ms/step - loss: 0.8735 - accuracy: 0.7087 - val_loss: 0.4762 - val_accuracy: 0.8574\n",
      "Epoch 352/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8773 - accuracy: 0.7060\n",
      "Epoch 00352: loss did not improve from 0.87242\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.8773 - accuracy: 0.7060 - val_loss: 0.4810 - val_accuracy: 0.8585\n",
      "Epoch 353/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8677 - accuracy: 0.7086\n",
      "Epoch 00353: loss improved from 0.87242 to 0.86771, saving model to ./Checkpoints\\weights-improvement-epoch_353-loss_0.8677-accuracy_0.7086.hdf5\n",
      "555/555 [==============================] - 50s 89ms/step - loss: 0.8677 - accuracy: 0.7086 - val_loss: 0.4719 - val_accuracy: 0.8612\n",
      "Epoch 354/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8696 - accuracy: 0.7097\n",
      "Epoch 00354: loss did not improve from 0.86771\n",
      "555/555 [==============================] - 55s 99ms/step - loss: 0.8696 - accuracy: 0.7097 - val_loss: 0.4701 - val_accuracy: 0.8615\n",
      "Epoch 355/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8661 - accuracy: 0.7112\n",
      "Epoch 00355: loss improved from 0.86771 to 0.86626, saving model to ./Checkpoints\\weights-improvement-epoch_355-loss_0.8663-accuracy_0.7112.hdf5\n",
      "555/555 [==============================] - 57s 103ms/step - loss: 0.8663 - accuracy: 0.7112 - val_loss: 0.4687 - val_accuracy: 0.8618\n",
      "Epoch 356/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8613 - accuracy: 0.7149\n",
      "Epoch 00356: loss improved from 0.86626 to 0.86127, saving model to ./Checkpoints\\weights-improvement-epoch_356-loss_0.8613-accuracy_0.7149.hdf5\n",
      "555/555 [==============================] - 55s 100ms/step - loss: 0.8613 - accuracy: 0.7149 - val_loss: 0.4647 - val_accuracy: 0.8618\n",
      "Epoch 357/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8632 - accuracy: 0.7134\n",
      "Epoch 00357: loss did not improve from 0.86127\n",
      "555/555 [==============================] - 58s 104ms/step - loss: 0.8633 - accuracy: 0.7133 - val_loss: 0.4707 - val_accuracy: 0.8608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 358/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8712 - accuracy: 0.7116\n",
      "Epoch 00358: loss did not improve from 0.86127\n",
      "555/555 [==============================] - 78s 141ms/step - loss: 0.8714 - accuracy: 0.7115 - val_loss: 0.4761 - val_accuracy: 0.8595\n",
      "Epoch 359/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8647 - accuracy: 0.7121\n",
      "Epoch 00359: loss did not improve from 0.86127\n",
      "555/555 [==============================] - 60s 109ms/step - loss: 0.8647 - accuracy: 0.7121 - val_loss: 0.4641 - val_accuracy: 0.8626\n",
      "Epoch 360/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8577 - accuracy: 0.7152\n",
      "Epoch 00360: loss improved from 0.86127 to 0.85771, saving model to ./Checkpoints\\weights-improvement-epoch_360-loss_0.8577-accuracy_0.7152.hdf5\n",
      "555/555 [==============================] - 62s 112ms/step - loss: 0.8577 - accuracy: 0.7152 - val_loss: 0.4608 - val_accuracy: 0.8635\n",
      "Epoch 361/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8662 - accuracy: 0.7100\n",
      "Epoch 00361: loss did not improve from 0.85771\n",
      "555/555 [==============================] - 61s 109ms/step - loss: 0.8662 - accuracy: 0.7100 - val_loss: 0.4579 - val_accuracy: 0.8641\n",
      "Epoch 362/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8623 - accuracy: 0.7134\n",
      "Epoch 00362: loss did not improve from 0.85771\n",
      "555/555 [==============================] - 80s 144ms/step - loss: 0.8623 - accuracy: 0.7134 - val_loss: 0.4629 - val_accuracy: 0.8629\n",
      "Epoch 363/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8560 - accuracy: 0.7133\n",
      "Epoch 00363: loss improved from 0.85771 to 0.85597, saving model to ./Checkpoints\\weights-improvement-epoch_363-loss_0.8560-accuracy_0.7133.hdf5\n",
      "555/555 [==============================] - 70s 126ms/step - loss: 0.8560 - accuracy: 0.7133 - val_loss: 0.4632 - val_accuracy: 0.8625\n",
      "Epoch 364/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8534 - accuracy: 0.7163\n",
      "Epoch 00364: loss improved from 0.85597 to 0.85334, saving model to ./Checkpoints\\weights-improvement-epoch_364-loss_0.8533-accuracy_0.7163.hdf5\n",
      "555/555 [==============================] - 69s 124ms/step - loss: 0.8533 - accuracy: 0.7163 - val_loss: 0.4633 - val_accuracy: 0.8613\n",
      "Epoch 365/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8560 - accuracy: 0.7158\n",
      "Epoch 00365: loss did not improve from 0.85334\n",
      "555/555 [==============================] - 73s 131ms/step - loss: 0.8560 - accuracy: 0.7158 - val_loss: 0.4553 - val_accuracy: 0.8658\n",
      "Epoch 366/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8517 - accuracy: 0.7146\n",
      "Epoch 00366: loss improved from 0.85334 to 0.85143, saving model to ./Checkpoints\\weights-improvement-epoch_366-loss_0.8514-accuracy_0.7147.hdf5\n",
      "555/555 [==============================] - 91s 165ms/step - loss: 0.8514 - accuracy: 0.7147 - val_loss: 0.4519 - val_accuracy: 0.8657\n",
      "Epoch 367/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8555 - accuracy: 0.7148\n",
      "Epoch 00367: loss did not improve from 0.85143\n",
      "555/555 [==============================] - 74s 134ms/step - loss: 0.8556 - accuracy: 0.7147 - val_loss: 0.4592 - val_accuracy: 0.8673\n",
      "Epoch 368/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8547 - accuracy: 0.7136\n",
      "Epoch 00368: loss did not improve from 0.85143\n",
      "555/555 [==============================] - 65s 118ms/step - loss: 0.8546 - accuracy: 0.7136 - val_loss: 0.4581 - val_accuracy: 0.8656\n",
      "Epoch 369/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8458 - accuracy: 0.7189\n",
      "Epoch 00369: loss improved from 0.85143 to 0.84580, saving model to ./Checkpoints\\weights-improvement-epoch_369-loss_0.8458-accuracy_0.7189.hdf5\n",
      "555/555 [==============================] - 68s 123ms/step - loss: 0.8458 - accuracy: 0.7189 - val_loss: 0.4542 - val_accuracy: 0.8656\n",
      "Epoch 370/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8488 - accuracy: 0.7171 ETA: 2s - loss: 0.8491 -  - ETA: 1s - loss: 0.8485 -  - ETA: 1s - l - ETA: 0s - loss: 0.8489 - accuracy: 0.71\n",
      "Epoch 00370: loss did not improve from 0.84580\n",
      "555/555 [==============================] - 59s 107ms/step - loss: 0.8488 - accuracy: 0.7171 - val_loss: 0.4523 - val_accuracy: 0.8672\n",
      "Epoch 371/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8477 - accuracy: 0.7169\n",
      "Epoch 00371: loss did not improve from 0.84580\n",
      "555/555 [==============================] - 62s 111ms/step - loss: 0.8478 - accuracy: 0.7169 - val_loss: 0.4579 - val_accuracy: 0.8653\n",
      "Epoch 372/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8477 - accuracy: 0.7170\n",
      "Epoch 00372: loss did not improve from 0.84580\n",
      "555/555 [==============================] - 50s 90ms/step - loss: 0.8477 - accuracy: 0.7170 - val_loss: 0.4534 - val_accuracy: 0.8668\n",
      "Epoch 373/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8399 - accuracy: 0.7187\n",
      "Epoch 00373: loss improved from 0.84580 to 0.83991, saving model to ./Checkpoints\\weights-improvement-epoch_373-loss_0.8399-accuracy_0.7187.hdf5\n",
      "555/555 [==============================] - 37s 66ms/step - loss: 0.8399 - accuracy: 0.7187 - val_loss: 0.4481 - val_accuracy: 0.8691\n",
      "Epoch 374/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8444 - accuracy: 0.7187\n",
      "Epoch 00374: loss did not improve from 0.83991\n",
      "555/555 [==============================] - 57s 103ms/step - loss: 0.8444 - accuracy: 0.7187 - val_loss: 0.4578 - val_accuracy: 0.8647\n",
      "Epoch 375/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8464 - accuracy: 0.7170\n",
      "Epoch 00375: loss did not improve from 0.83991\n",
      "555/555 [==============================] - 87s 157ms/step - loss: 0.8464 - accuracy: 0.7170 - val_loss: 0.4499 - val_accuracy: 0.8680\n",
      "Epoch 376/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8467 - accuracy: 0.7170\n",
      "Epoch 00376: loss did not improve from 0.83991\n",
      "555/555 [==============================] - 40s 72ms/step - loss: 0.8467 - accuracy: 0.7170 - val_loss: 0.4483 - val_accuracy: 0.8683\n",
      "Epoch 377/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8380 - accuracy: 0.7214\n",
      "Epoch 00377: loss improved from 0.83991 to 0.83787, saving model to ./Checkpoints\\weights-improvement-epoch_377-loss_0.8379-accuracy_0.7214.hdf5\n",
      "555/555 [==============================] - 66s 118ms/step - loss: 0.8379 - accuracy: 0.7214 - val_loss: 0.4426 - val_accuracy: 0.8710\n",
      "Epoch 378/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8415 - accuracy: 0.7203\n",
      "Epoch 00378: loss did not improve from 0.83787\n",
      "555/555 [==============================] - 66s 120ms/step - loss: 0.8416 - accuracy: 0.7203 - val_loss: 0.4473 - val_accuracy: 0.8687\n",
      "Epoch 379/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8465 - accuracy: 0.7180\n",
      "Epoch 00379: loss did not improve from 0.83787\n",
      "555/555 [==============================] - 43s 78ms/step - loss: 0.8465 - accuracy: 0.7180 - val_loss: 0.4360 - val_accuracy: 0.8746\n",
      "Epoch 380/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8367 - accuracy: 0.7221\n",
      "Epoch 00380: loss improved from 0.83787 to 0.83675, saving model to ./Checkpoints\\weights-improvement-epoch_380-loss_0.8367-accuracy_0.7221.hdf5\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.8367 - accuracy: 0.7221 - val_loss: 0.4401 - val_accuracy: 0.8709\n",
      "Epoch 381/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8372 - accuracy: 0.7217\n",
      "Epoch 00381: loss did not improve from 0.83675\n",
      "555/555 [==============================] - 50s 90ms/step - loss: 0.8372 - accuracy: 0.7217 - val_loss: 0.4471 - val_accuracy: 0.8703\n",
      "Epoch 382/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8411 - accuracy: 0.7195\n",
      "Epoch 00382: loss did not improve from 0.83675\n",
      "555/555 [==============================] - 68s 123ms/step - loss: 0.8410 - accuracy: 0.7196 - val_loss: 0.4341 - val_accuracy: 0.8734\n",
      "Epoch 383/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8327 - accuracy: 0.7218\n",
      "Epoch 00383: loss improved from 0.83675 to 0.83280, saving model to ./Checkpoints\\weights-improvement-epoch_383-loss_0.8328-accuracy_0.7217.hdf5\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 0.8328 - accuracy: 0.7217 - val_loss: 0.4378 - val_accuracy: 0.8714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8394 - accuracy: 0.7203\n",
      "Epoch 00384: loss did not improve from 0.83280\n",
      "555/555 [==============================] - 92s 166ms/step - loss: 0.8394 - accuracy: 0.7203 - val_loss: 0.4383 - val_accuracy: 0.8696\n",
      "Epoch 385/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.7240\n",
      "Epoch 00385: loss improved from 0.83280 to 0.82698, saving model to ./Checkpoints\\weights-improvement-epoch_385-loss_0.8270-accuracy_0.7240.hdf5\n",
      "555/555 [==============================] - 72s 130ms/step - loss: 0.8270 - accuracy: 0.7240 - val_loss: 0.4422 - val_accuracy: 0.8704\n",
      "Epoch 386/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8380 - accuracy: 0.7195\n",
      "Epoch 00386: loss did not improve from 0.82698\n",
      "555/555 [==============================] - 82s 148ms/step - loss: 0.8380 - accuracy: 0.7195 - val_loss: 0.4412 - val_accuracy: 0.8723\n",
      "Epoch 387/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8262 - accuracy: 0.7250\n",
      "Epoch 00387: loss improved from 0.82698 to 0.82622, saving model to ./Checkpoints\\weights-improvement-epoch_387-loss_0.8262-accuracy_0.7250.hdf5\n",
      "555/555 [==============================] - 60s 108ms/step - loss: 0.8262 - accuracy: 0.7250 - val_loss: 0.4209 - val_accuracy: 0.8784\n",
      "Epoch 388/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8359 - accuracy: 0.7230\n",
      "Epoch 00388: loss did not improve from 0.82622\n",
      "555/555 [==============================] - 37s 66ms/step - loss: 0.8359 - accuracy: 0.7230 - val_loss: 0.4394 - val_accuracy: 0.8715\n",
      "Epoch 389/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8331 - accuracy: 0.7226 ETA: 2s - - ETA: 1s - loss: 0.8325 - accu - ETA: 0s - loss: 0.8\n",
      "Epoch 00389: loss did not improve from 0.82622\n",
      "555/555 [==============================] - 41s 73ms/step - loss: 0.8331 - accuracy: 0.7225 - val_loss: 0.4279 - val_accuracy: 0.8748\n",
      "Epoch 390/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8207 - accuracy: 0.7267\n",
      "Epoch 00390: loss improved from 0.82622 to 0.82061, saving model to ./Checkpoints\\weights-improvement-epoch_390-loss_0.8206-accuracy_0.7267.hdf5\n",
      "555/555 [==============================] - 47s 84ms/step - loss: 0.8206 - accuracy: 0.7267 - val_loss: 0.4220 - val_accuracy: 0.8762\n",
      "Epoch 391/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8275 - accuracy: 0.7228\n",
      "Epoch 00391: loss did not improve from 0.82061\n",
      "555/555 [==============================] - 110s 198ms/step - loss: 0.8275 - accuracy: 0.7228 - val_loss: 0.4358 - val_accuracy: 0.8742\n",
      "Epoch 392/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8308 - accuracy: 0.7211\n",
      "Epoch 00392: loss did not improve from 0.82061\n",
      "555/555 [==============================] - 71s 127ms/step - loss: 0.8308 - accuracy: 0.7211 - val_loss: 0.4196 - val_accuracy: 0.8795\n",
      "Epoch 393/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8253 - accuracy: 0.7248\n",
      "Epoch 00393: loss did not improve from 0.82061\n",
      "555/555 [==============================] - 53s 95ms/step - loss: 0.8253 - accuracy: 0.7248 - val_loss: 0.4307 - val_accuracy: 0.8752\n",
      "Epoch 394/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8227 - accuracy: 0.7253\n",
      "Epoch 00394: loss did not improve from 0.82061\n",
      "555/555 [==============================] - 70s 125ms/step - loss: 0.8227 - accuracy: 0.7253 - val_loss: 0.4234 - val_accuracy: 0.8765\n",
      "Epoch 395/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8187 - accuracy: 0.7266\n",
      "Epoch 00395: loss improved from 0.82061 to 0.81870, saving model to ./Checkpoints\\weights-improvement-epoch_395-loss_0.8187-accuracy_0.7266.hdf5\n",
      "555/555 [==============================] - 59s 107ms/step - loss: 0.8187 - accuracy: 0.7266 - val_loss: 0.4196 - val_accuracy: 0.8786\n",
      "Epoch 396/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8192 - accuracy: 0.7277\n",
      "Epoch 00396: loss did not improve from 0.81870\n",
      "555/555 [==============================] - 43s 78ms/step - loss: 0.8194 - accuracy: 0.7276 - val_loss: 0.4264 - val_accuracy: 0.8744\n",
      "Epoch 397/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8229 - accuracy: 0.7258\n",
      "Epoch 00397: loss did not improve from 0.81870\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 0.8228 - accuracy: 0.7258 - val_loss: 0.4188 - val_accuracy: 0.8793\n",
      "Epoch 398/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8144 - accuracy: 0.7281\n",
      "Epoch 00398: loss improved from 0.81870 to 0.81430, saving model to ./Checkpoints\\weights-improvement-epoch_398-loss_0.8143-accuracy_0.7281.hdf5\n",
      "555/555 [==============================] - 38s 68ms/step - loss: 0.8143 - accuracy: 0.7281 - val_loss: 0.4142 - val_accuracy: 0.8808\n",
      "Epoch 399/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8119 - accuracy: 0.7303\n",
      "Epoch 00399: loss improved from 0.81430 to 0.81196, saving model to ./Checkpoints\\weights-improvement-epoch_399-loss_0.8120-accuracy_0.7303.hdf5\n",
      "555/555 [==============================] - 53s 96ms/step - loss: 0.8120 - accuracy: 0.7303 - val_loss: 0.4122 - val_accuracy: 0.8796\n",
      "Epoch 400/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8189 - accuracy: 0.7274\n",
      "Epoch 00400: loss did not improve from 0.81196\n",
      "555/555 [==============================] - 37s 67ms/step - loss: 0.8189 - accuracy: 0.7274 - val_loss: 0.4153 - val_accuracy: 0.8810\n",
      "Epoch 401/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8075 - accuracy: 0.7290\n",
      "Epoch 00401: loss improved from 0.81196 to 0.80735, saving model to ./Checkpoints\\weights-improvement-epoch_401-loss_0.8074-accuracy_0.7290.hdf5\n",
      "555/555 [==============================] - 65s 116ms/step - loss: 0.8074 - accuracy: 0.7290 - val_loss: 0.4081 - val_accuracy: 0.8808\n",
      "Epoch 402/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8094 - accuracy: 0.7294\n",
      "Epoch 00402: loss did not improve from 0.80735\n",
      "555/555 [==============================] - 41s 74ms/step - loss: 0.8095 - accuracy: 0.7294 - val_loss: 0.4141 - val_accuracy: 0.8795\n",
      "Epoch 403/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8131 - accuracy: 0.7278\n",
      "Epoch 00403: loss did not improve from 0.80735\n",
      "555/555 [==============================] - 39s 69ms/step - loss: 0.8131 - accuracy: 0.7278 - val_loss: 0.4209 - val_accuracy: 0.8780\n",
      "Epoch 404/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8084 - accuracy: 0.7268\n",
      "Epoch 00404: loss did not improve from 0.80735\n",
      "555/555 [==============================] - 36s 64ms/step - loss: 0.8084 - accuracy: 0.7268 - val_loss: 0.4032 - val_accuracy: 0.8838\n",
      "Epoch 405/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8125 - accuracy: 0.7275\n",
      "Epoch 00405: loss did not improve from 0.80735\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 0.8125 - accuracy: 0.7275 - val_loss: 0.4069 - val_accuracy: 0.8824\n",
      "Epoch 406/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8138 - accuracy: 0.7280\n",
      "Epoch 00406: loss did not improve from 0.80735\n",
      "555/555 [==============================] - 40s 72ms/step - loss: 0.8138 - accuracy: 0.7280 - val_loss: 0.4069 - val_accuracy: 0.8829\n",
      "Epoch 407/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8110 - accuracy: 0.7319 ETA: 0s - loss:\n",
      "Epoch 00407: loss did not improve from 0.80735\n",
      "555/555 [==============================] - 43s 77ms/step - loss: 0.8111 - accuracy: 0.7319 - val_loss: 0.4056 - val_accuracy: 0.8837\n",
      "Epoch 408/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8083 - accuracy: 0.7313 ETA: 0s - l\n",
      "Epoch 00408: loss did not improve from 0.80735\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 0.8083 - accuracy: 0.7313 - val_loss: 0.4046 - val_accuracy: 0.8818\n",
      "Epoch 409/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.8042 - accuracy: 0.7336\n",
      "Epoch 00409: loss improved from 0.80735 to 0.80424, saving model to ./Checkpoints\\weights-improvement-epoch_409-loss_0.8042-accuracy_0.7335.hdf5\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 0.8042 - accuracy: 0.7335 - val_loss: 0.4078 - val_accuracy: 0.8847\n",
      "Epoch 410/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7930 - accuracy: 0.7369\n",
      "Epoch 00410: loss improved from 0.80424 to 0.79299, saving model to ./Checkpoints\\weights-improvement-epoch_410-loss_0.7930-accuracy_0.7369.hdf5\n",
      "555/555 [==============================] - 45s 82ms/step - loss: 0.7930 - accuracy: 0.7369 - val_loss: 0.3998 - val_accuracy: 0.8836\n",
      "Epoch 411/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8024 - accuracy: 0.7328\n",
      "Epoch 00411: loss did not improve from 0.79299\n",
      "555/555 [==============================] - 44s 78ms/step - loss: 0.8024 - accuracy: 0.7328 - val_loss: 0.3969 - val_accuracy: 0.8855\n",
      "Epoch 412/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8002 - accuracy: 0.7326\n",
      "Epoch 00412: loss did not improve from 0.79299\n",
      "555/555 [==============================] - 40s 72ms/step - loss: 0.8002 - accuracy: 0.7326 - val_loss: 0.3981 - val_accuracy: 0.8839\n",
      "Epoch 413/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8024 - accuracy: 0.7319\n",
      "Epoch 00413: loss did not improve from 0.79299\n",
      "555/555 [==============================] - 42s 75ms/step - loss: 0.8024 - accuracy: 0.7319 - val_loss: 0.3919 - val_accuracy: 0.8895\n",
      "Epoch 414/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7948 - accuracy: 0.7328\n",
      "Epoch 00414: loss did not improve from 0.79299\n",
      "555/555 [==============================] - 40s 73ms/step - loss: 0.7948 - accuracy: 0.7328 - val_loss: 0.4002 - val_accuracy: 0.8866\n",
      "Epoch 415/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7940 - accuracy: 0.7356\n",
      "Epoch 00415: loss did not improve from 0.79299\n",
      "555/555 [==============================] - 43s 77ms/step - loss: 0.7940 - accuracy: 0.7356 - val_loss: 0.3976 - val_accuracy: 0.8859\n",
      "Epoch 416/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7892 - accuracy: 0.7367\n",
      "Epoch 00416: loss improved from 0.79299 to 0.78912, saving model to ./Checkpoints\\weights-improvement-epoch_416-loss_0.7891-accuracy_0.7367.hdf5\n",
      "555/555 [==============================] - 41s 74ms/step - loss: 0.7891 - accuracy: 0.7367 - val_loss: 0.3961 - val_accuracy: 0.8860\n",
      "Epoch 417/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7985 - accuracy: 0.7342\n",
      "Epoch 00417: loss did not improve from 0.78912\n",
      "555/555 [==============================] - 43s 77ms/step - loss: 0.7987 - accuracy: 0.7342 - val_loss: 0.4000 - val_accuracy: 0.8857\n",
      "Epoch 418/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7875 - accuracy: 0.7361 ETA: 2s - loss: - ETA: 0s - loss: 0.7\n",
      "Epoch 00418: loss improved from 0.78912 to 0.78752, saving model to ./Checkpoints\\weights-improvement-epoch_418-loss_0.7875-accuracy_0.7361.hdf5\n",
      "555/555 [==============================] - 43s 77ms/step - loss: 0.7875 - accuracy: 0.7361 - val_loss: 0.3914 - val_accuracy: 0.8875\n",
      "Epoch 419/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7938 - accuracy: 0.7344\n",
      "Epoch 00419: loss did not improve from 0.78752\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.7938 - accuracy: 0.7344 - val_loss: 0.3976 - val_accuracy: 0.8859\n",
      "Epoch 420/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7999 - accuracy: 0.7341\n",
      "Epoch 00420: loss did not improve from 0.78752\n",
      "555/555 [==============================] - 42s 75ms/step - loss: 0.7998 - accuracy: 0.7341 - val_loss: 0.3969 - val_accuracy: 0.8867\n",
      "Epoch 421/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7901 - accuracy: 0.7380\n",
      "Epoch 00421: loss did not improve from 0.78752\n",
      "555/555 [==============================] - 39s 69ms/step - loss: 0.7901 - accuracy: 0.7380 - val_loss: 0.3951 - val_accuracy: 0.8887\n",
      "Epoch 422/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7863 - accuracy: 0.7368\n",
      "Epoch 00422: loss improved from 0.78752 to 0.78631, saving model to ./Checkpoints\\weights-improvement-epoch_422-loss_0.7863-accuracy_0.7368.hdf5\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 0.7863 - accuracy: 0.7368 - val_loss: 0.3835 - val_accuracy: 0.8887\n",
      "Epoch 423/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7812 - accuracy: 0.7385\n",
      "Epoch 00423: loss improved from 0.78631 to 0.78119, saving model to ./Checkpoints\\weights-improvement-epoch_423-loss_0.7812-accuracy_0.7385.hdf5\n",
      "555/555 [==============================] - 37s 66ms/step - loss: 0.7812 - accuracy: 0.7385 - val_loss: 0.3899 - val_accuracy: 0.8875\n",
      "Epoch 424/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7888 - accuracy: 0.7351 ETA: 5s - loss: 0.7852 - accu - ETA: 4s - loss: 0.7853 - ac -\n",
      "Epoch 00424: loss did not improve from 0.78119\n",
      "555/555 [==============================] - 34s 61ms/step - loss: 0.7888 - accuracy: 0.7351 - val_loss: 0.3861 - val_accuracy: 0.8903\n",
      "Epoch 425/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7864 - accuracy: 0.7383 ETA: 0s - loss: 0.7866 - accura\n",
      "Epoch 00425: loss did not improve from 0.78119\n",
      "555/555 [==============================] - 37s 67ms/step - loss: 0.7864 - accuracy: 0.7383 - val_loss: 0.3900 - val_accuracy: 0.8850\n",
      "Epoch 426/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7878 - accuracy: 0.7379\n",
      "Epoch 00426: loss did not improve from 0.78119\n",
      "555/555 [==============================] - 40s 73ms/step - loss: 0.7878 - accuracy: 0.7379 - val_loss: 0.3879 - val_accuracy: 0.8892\n",
      "Epoch 427/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7869 - accuracy: 0.7368\n",
      "Epoch 00427: loss did not improve from 0.78119\n",
      "555/555 [==============================] - 35s 64ms/step - loss: 0.7869 - accuracy: 0.7368 - val_loss: 0.3949 - val_accuracy: 0.8876\n",
      "Epoch 428/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7947 - accuracy: 0.7376\n",
      "Epoch 00428: loss did not improve from 0.78119\n",
      "555/555 [==============================] - 41s 74ms/step - loss: 0.7947 - accuracy: 0.7376 - val_loss: 0.3908 - val_accuracy: 0.8886\n",
      "Epoch 429/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7866 - accuracy: 0.7369 E\n",
      "Epoch 00429: loss did not improve from 0.78119\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.7865 - accuracy: 0.7369 - val_loss: 0.3797 - val_accuracy: 0.8932\n",
      "Epoch 430/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7730 - accuracy: 0.7413\n",
      "Epoch 00430: loss improved from 0.78119 to 0.77297, saving model to ./Checkpoints\\weights-improvement-epoch_430-loss_0.7730-accuracy_0.7413.hdf5\n",
      "555/555 [==============================] - 43s 77ms/step - loss: 0.7730 - accuracy: 0.7413 - val_loss: 0.3829 - val_accuracy: 0.8893\n",
      "Epoch 431/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7804 - accuracy: 0.7385\n",
      "Epoch 00431: loss did not improve from 0.77297\n",
      "555/555 [==============================] - 43s 77ms/step - loss: 0.7804 - accuracy: 0.7385 - val_loss: 0.3812 - val_accuracy: 0.8911\n",
      "Epoch 432/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7812 - accuracy: 0.7390\n",
      "Epoch 00432: loss did not improve from 0.77297\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 0.7812 - accuracy: 0.7390 - val_loss: 0.3862 - val_accuracy: 0.8902\n",
      "Epoch 433/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7814 - accuracy: 0.7384\n",
      "Epoch 00433: loss did not improve from 0.77297\n",
      "555/555 [==============================] - 47s 84ms/step - loss: 0.7815 - accuracy: 0.7383 - val_loss: 0.3811 - val_accuracy: 0.8934\n",
      "Epoch 434/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7822 - accuracy: 0.7394\n",
      "Epoch 00434: loss did not improve from 0.77297\n",
      "555/555 [==============================] - 43s 78ms/step - loss: 0.7822 - accuracy: 0.7394 - val_loss: 0.3856 - val_accuracy: 0.8917\n",
      "Epoch 435/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7831 - accuracy: 0.7394\n",
      "Epoch 00435: loss did not improve from 0.77297\n",
      "555/555 [==============================] - 46s 84ms/step - loss: 0.7832 - accuracy: 0.7394 - val_loss: 0.3745 - val_accuracy: 0.8924\n",
      "Epoch 436/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7752 - accuracy: 0.7429\n",
      "Epoch 00436: loss did not improve from 0.77297\n",
      "555/555 [==============================] - 43s 78ms/step - loss: 0.7753 - accuracy: 0.7429 - val_loss: 0.3782 - val_accuracy: 0.8916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7696 - accuracy: 0.7437\n",
      "Epoch 00437: loss improved from 0.77297 to 0.76960, saving model to ./Checkpoints\\weights-improvement-epoch_437-loss_0.7696-accuracy_0.7437.hdf5\n",
      "555/555 [==============================] - 43s 77ms/step - loss: 0.7696 - accuracy: 0.7437 - val_loss: 0.3660 - val_accuracy: 0.8984\n",
      "Epoch 438/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7711 - accuracy: 0.7431\n",
      "Epoch 00438: loss did not improve from 0.76960\n",
      "555/555 [==============================] - 40s 73ms/step - loss: 0.7711 - accuracy: 0.7431 - val_loss: 0.3737 - val_accuracy: 0.8949\n",
      "Epoch 439/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7777 - accuracy: 0.7414\n",
      "Epoch 00439: loss did not improve from 0.76960\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 0.7777 - accuracy: 0.7414 - val_loss: 0.3798 - val_accuracy: 0.8922\n",
      "Epoch 440/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7748 - accuracy: 0.7419\n",
      "Epoch 00440: loss did not improve from 0.76960\n",
      "555/555 [==============================] - 45s 82ms/step - loss: 0.7748 - accuracy: 0.7418 - val_loss: 0.3692 - val_accuracy: 0.8965\n",
      "Epoch 441/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7670 - accuracy: 0.7432\n",
      "Epoch 00441: loss improved from 0.76960 to 0.76701, saving model to ./Checkpoints\\weights-improvement-epoch_441-loss_0.7670-accuracy_0.7432.hdf5\n",
      "555/555 [==============================] - 43s 77ms/step - loss: 0.7670 - accuracy: 0.7432 - val_loss: 0.3801 - val_accuracy: 0.8898\n",
      "Epoch 442/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7797 - accuracy: 0.7394\n",
      "Epoch 00442: loss did not improve from 0.76701\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.7796 - accuracy: 0.7394 - val_loss: 0.3664 - val_accuracy: 0.8963\n",
      "Epoch 443/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7709 - accuracy: 0.7431\n",
      "Epoch 00443: loss did not improve from 0.76701\n",
      "555/555 [==============================] - 41s 75ms/step - loss: 0.7708 - accuracy: 0.7431 - val_loss: 0.3670 - val_accuracy: 0.8971\n",
      "Epoch 444/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7758 - accuracy: 0.7407\n",
      "Epoch 00444: loss did not improve from 0.76701\n",
      "555/555 [==============================] - 42s 76ms/step - loss: 0.7757 - accuracy: 0.7408 - val_loss: 0.3662 - val_accuracy: 0.8975\n",
      "Epoch 445/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7741 - accuracy: 0.7405\n",
      "Epoch 00445: loss did not improve from 0.76701\n",
      "555/555 [==============================] - 42s 76ms/step - loss: 0.7741 - accuracy: 0.7405 - val_loss: 0.3618 - val_accuracy: 0.8982\n",
      "Epoch 446/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7730 - accuracy: 0.7416\n",
      "Epoch 00446: loss did not improve from 0.76701\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 0.7730 - accuracy: 0.7416 - val_loss: 0.3653 - val_accuracy: 0.8977\n",
      "Epoch 447/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7649 - accuracy: 0.7412\n",
      "Epoch 00447: loss improved from 0.76701 to 0.76477, saving model to ./Checkpoints\\weights-improvement-epoch_447-loss_0.7648-accuracy_0.7412.hdf5\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 0.7648 - accuracy: 0.7412 - val_loss: 0.3601 - val_accuracy: 0.8989\n",
      "Epoch 448/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7642 - accuracy: 0.7444\n",
      "Epoch 00448: loss improved from 0.76477 to 0.76415, saving model to ./Checkpoints\\weights-improvement-epoch_448-loss_0.7642-accuracy_0.7444.hdf5\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 0.7642 - accuracy: 0.7444 - val_loss: 0.3662 - val_accuracy: 0.8960\n",
      "Epoch 449/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7764 - accuracy: 0.7407\n",
      "Epoch 00449: loss did not improve from 0.76415\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 0.7765 - accuracy: 0.7407 - val_loss: 0.3714 - val_accuracy: 0.8991\n",
      "Epoch 450/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7581 - accuracy: 0.7438\n",
      "Epoch 00450: loss improved from 0.76415 to 0.75813, saving model to ./Checkpoints\\weights-improvement-epoch_450-loss_0.7581-accuracy_0.7438.hdf5\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 0.7581 - accuracy: 0.7438 - val_loss: 0.3641 - val_accuracy: 0.8957\n",
      "Epoch 451/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7613 - accuracy: 0.7459\n",
      "Epoch 00451: loss did not improve from 0.75813\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.7613 - accuracy: 0.7459 - val_loss: 0.3654 - val_accuracy: 0.8987\n",
      "Epoch 452/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7563 - accuracy: 0.7484\n",
      "Epoch 00452: loss improved from 0.75813 to 0.75624, saving model to ./Checkpoints\\weights-improvement-epoch_452-loss_0.7562-accuracy_0.7484.hdf5\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.7562 - accuracy: 0.7484 - val_loss: 0.3655 - val_accuracy: 0.8962\n",
      "Epoch 453/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7607 - accuracy: 0.7450\n",
      "Epoch 00453: loss did not improve from 0.75624\n",
      "555/555 [==============================] - 49s 88ms/step - loss: 0.7607 - accuracy: 0.7450 - val_loss: 0.3644 - val_accuracy: 0.8962\n",
      "Epoch 454/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7555 - accuracy: 0.7464\n",
      "Epoch 00454: loss improved from 0.75624 to 0.75553, saving model to ./Checkpoints\\weights-improvement-epoch_454-loss_0.7555-accuracy_0.7464.hdf5\n",
      "555/555 [==============================] - 49s 89ms/step - loss: 0.7555 - accuracy: 0.7464 - val_loss: 0.3535 - val_accuracy: 0.8997\n",
      "Epoch 455/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7543 - accuracy: 0.7475\n",
      "Epoch 00455: loss improved from 0.75553 to 0.75423, saving model to ./Checkpoints\\weights-improvement-epoch_455-loss_0.7542-accuracy_0.7475.hdf5\n",
      "555/555 [==============================] - 49s 89ms/step - loss: 0.7542 - accuracy: 0.7475 - val_loss: 0.3514 - val_accuracy: 0.9037\n",
      "Epoch 456/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7508 - accuracy: 0.7489\n",
      "Epoch 00456: loss improved from 0.75423 to 0.75077, saving model to ./Checkpoints\\weights-improvement-epoch_456-loss_0.7508-accuracy_0.7489.hdf5\n",
      "555/555 [==============================] - 49s 88ms/step - loss: 0.7508 - accuracy: 0.7489 - val_loss: 0.3589 - val_accuracy: 0.8991\n",
      "Epoch 457/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7631 - accuracy: 0.7467\n",
      "Epoch 00457: loss did not improve from 0.75077\n",
      "555/555 [==============================] - 49s 89ms/step - loss: 0.7631 - accuracy: 0.7467 - val_loss: 0.3570 - val_accuracy: 0.9008\n",
      "Epoch 458/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7554 - accuracy: 0.7464\n",
      "Epoch 00458: loss did not improve from 0.75077\n",
      "555/555 [==============================] - 49s 89ms/step - loss: 0.7554 - accuracy: 0.7464 - val_loss: 0.3603 - val_accuracy: 0.8970\n",
      "Epoch 459/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7553 - accuracy: 0.7481\n",
      "Epoch 00459: loss did not improve from 0.75077\n",
      "555/555 [==============================] - 50s 90ms/step - loss: 0.7553 - accuracy: 0.7482 - val_loss: 0.3612 - val_accuracy: 0.8993\n",
      "Epoch 460/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7557 - accuracy: 0.7490\n",
      "Epoch 00460: loss did not improve from 0.75077\n",
      "555/555 [==============================] - 61s 109ms/step - loss: 0.7556 - accuracy: 0.7490 - val_loss: 0.3516 - val_accuracy: 0.9025\n",
      "Epoch 461/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7526 - accuracy: 0.7466\n",
      "Epoch 00461: loss did not improve from 0.75077\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 0.7526 - accuracy: 0.7466 - val_loss: 0.3574 - val_accuracy: 0.9021\n",
      "Epoch 462/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7510 - accuracy: 0.7492\n",
      "Epoch 00462: loss did not improve from 0.75077\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.7509 - accuracy: 0.7493 - val_loss: 0.3518 - val_accuracy: 0.9018\n",
      "Epoch 463/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7431 - accuracy: 0.7508\n",
      "Epoch 00463: loss improved from 0.75077 to 0.74316, saving model to ./Checkpoints\\weights-improvement-epoch_463-loss_0.7432-accuracy_0.7508.hdf5\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.7432 - accuracy: 0.7508 - val_loss: 0.3502 - val_accuracy: 0.9035\n",
      "Epoch 464/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7490 - accuracy: 0.7492\n",
      "Epoch 00464: loss did not improve from 0.74316\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.7491 - accuracy: 0.7492 - val_loss: 0.3436 - val_accuracy: 0.9041\n",
      "Epoch 465/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7429 - accuracy: 0.7523\n",
      "Epoch 00465: loss improved from 0.74316 to 0.74290, saving model to ./Checkpoints\\weights-improvement-epoch_465-loss_0.7429-accuracy_0.7523.hdf5\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 0.7429 - accuracy: 0.7523 - val_loss: 0.3531 - val_accuracy: 0.9023\n",
      "Epoch 466/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7494 - accuracy: 0.7502\n",
      "Epoch 00466: loss did not improve from 0.74290\n",
      "555/555 [==============================] - 46s 84ms/step - loss: 0.7494 - accuracy: 0.7502 - val_loss: 0.3410 - val_accuracy: 0.9051\n",
      "Epoch 467/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7521 - accuracy: 0.7495\n",
      "Epoch 00467: loss did not improve from 0.74290\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.7521 - accuracy: 0.7495 - val_loss: 0.3514 - val_accuracy: 0.9017\n",
      "Epoch 468/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7446 - accuracy: 0.7522\n",
      "Epoch 00468: loss did not improve from 0.74290\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.7446 - accuracy: 0.7522 - val_loss: 0.3523 - val_accuracy: 0.9015\n",
      "Epoch 469/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7423 - accuracy: 0.7523\n",
      "Epoch 00469: loss improved from 0.74290 to 0.74225, saving model to ./Checkpoints\\weights-improvement-epoch_469-loss_0.7423-accuracy_0.7523.hdf5\n",
      "555/555 [==============================] - 42s 76ms/step - loss: 0.7423 - accuracy: 0.7523 - val_loss: 0.3448 - val_accuracy: 0.9048\n",
      "Epoch 470/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7407 - accuracy: 0.7519\n",
      "Epoch 00470: loss improved from 0.74225 to 0.74075, saving model to ./Checkpoints\\weights-improvement-epoch_470-loss_0.7408-accuracy_0.7519.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7408 - accuracy: 0.7519 - val_loss: 0.3397 - val_accuracy: 0.9065\n",
      "Epoch 471/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7415 - accuracy: 0.7502\n",
      "Epoch 00471: loss did not improve from 0.74075\n",
      "555/555 [==============================] - 30s 53ms/step - loss: 0.7415 - accuracy: 0.7501 - val_loss: 0.3431 - val_accuracy: 0.9034\n",
      "Epoch 472/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.7514\n",
      "Epoch 00472: loss did not improve from 0.74075\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.7419 - accuracy: 0.7514 - val_loss: 0.3430 - val_accuracy: 0.9043\n",
      "Epoch 473/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7374 - accuracy: 0.7534\n",
      "Epoch 00473: loss improved from 0.74075 to 0.73744, saving model to ./Checkpoints\\weights-improvement-epoch_473-loss_0.7374-accuracy_0.7534.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7374 - accuracy: 0.7534 - val_loss: 0.3385 - val_accuracy: 0.9064\n",
      "Epoch 474/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7444 - accuracy: 0.7536\n",
      "Epoch 00474: loss did not improve from 0.73744\n",
      "555/555 [==============================] - 30s 55ms/step - loss: 0.7444 - accuracy: 0.7536 - val_loss: 0.3434 - val_accuracy: 0.9053\n",
      "Epoch 475/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7403 - accuracy: 0.7520\n",
      "Epoch 00475: loss did not improve from 0.73744\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.7403 - accuracy: 0.7520 - val_loss: 0.3375 - val_accuracy: 0.9048\n",
      "Epoch 476/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7435 - accuracy: 0.7502\n",
      "Epoch 00476: loss did not improve from 0.73744\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7435 - accuracy: 0.7502 - val_loss: 0.3407 - val_accuracy: 0.9069\n",
      "Epoch 477/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7375 - accuracy: 0.7533\n",
      "Epoch 00477: loss did not improve from 0.73744\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7375 - accuracy: 0.7533 - val_loss: 0.3387 - val_accuracy: 0.9050\n",
      "Epoch 478/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7398 - accuracy: 0.7536\n",
      "Epoch 00478: loss did not improve from 0.73744\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7397 - accuracy: 0.7536 - val_loss: 0.3379 - val_accuracy: 0.9055\n",
      "Epoch 479/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7281 - accuracy: 0.7555\n",
      "Epoch 00479: loss improved from 0.73744 to 0.72810, saving model to ./Checkpoints\\weights-improvement-epoch_479-loss_0.7281-accuracy_0.7555.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7281 - accuracy: 0.7555 - val_loss: 0.3352 - val_accuracy: 0.9075\n",
      "Epoch 480/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7289 - accuracy: 0.7569\n",
      "Epoch 00480: loss did not improve from 0.72810\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7289 - accuracy: 0.7569 - val_loss: 0.3327 - val_accuracy: 0.9092\n",
      "Epoch 481/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7338 - accuracy: 0.7531\n",
      "Epoch 00481: loss did not improve from 0.72810\n",
      "555/555 [==============================] - 30s 53ms/step - loss: 0.7338 - accuracy: 0.7531 - val_loss: 0.3321 - val_accuracy: 0.9088\n",
      "Epoch 482/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7393 - accuracy: 0.7533\n",
      "Epoch 00482: loss did not improve from 0.72810\n",
      "555/555 [==============================] - 30s 53ms/step - loss: 0.7393 - accuracy: 0.7533 - val_loss: 0.3364 - val_accuracy: 0.9060\n",
      "Epoch 483/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7356 - accuracy: 0.7531\n",
      "Epoch 00483: loss did not improve from 0.72810\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 0.7356 - accuracy: 0.7531 - val_loss: 0.3342 - val_accuracy: 0.9084\n",
      "Epoch 484/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7376 - accuracy: 0.7523\n",
      "Epoch 00484: loss did not improve from 0.72810\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7376 - accuracy: 0.7523 - val_loss: 0.3315 - val_accuracy: 0.9096\n",
      "Epoch 485/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7359 - accuracy: 0.7553\n",
      "Epoch 00485: loss did not improve from 0.72810\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7359 - accuracy: 0.7553 - val_loss: 0.3299 - val_accuracy: 0.9087\n",
      "Epoch 486/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7258 - accuracy: 0.7559\n",
      "Epoch 00486: loss improved from 0.72810 to 0.72583, saving model to ./Checkpoints\\weights-improvement-epoch_486-loss_0.7258-accuracy_0.7559.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7258 - accuracy: 0.7559 - val_loss: 0.3230 - val_accuracy: 0.9122\n",
      "Epoch 487/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7302 - accuracy: 0.7556\n",
      "Epoch 00487: loss did not improve from 0.72583\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 0.7302 - accuracy: 0.7556 - val_loss: 0.3326 - val_accuracy: 0.9060\n",
      "Epoch 488/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7245 - accuracy: 0.7588\n",
      "Epoch 00488: loss improved from 0.72583 to 0.72445, saving model to ./Checkpoints\\weights-improvement-epoch_488-loss_0.7245-accuracy_0.7588.hdf5\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.7245 - accuracy: 0.7588 - val_loss: 0.3254 - val_accuracy: 0.9122\n",
      "Epoch 489/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7240 - accuracy: 0.7587\n",
      "Epoch 00489: loss improved from 0.72445 to 0.72396, saving model to ./Checkpoints\\weights-improvement-epoch_489-loss_0.7240-accuracy_0.7587.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.7240 - accuracy: 0.7587 - val_loss: 0.3208 - val_accuracy: 0.9114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7288 - accuracy: 0.7568\n",
      "Epoch 00490: loss did not improve from 0.72396\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7288 - accuracy: 0.7568 - val_loss: 0.3222 - val_accuracy: 0.9125\n",
      "Epoch 491/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7358 - accuracy: 0.7547\n",
      "Epoch 00491: loss did not improve from 0.72396\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.7358 - accuracy: 0.7547 - val_loss: 0.3328 - val_accuracy: 0.9084\n",
      "Epoch 492/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7210 - accuracy: 0.7587\n",
      "Epoch 00492: loss improved from 0.72396 to 0.72112, saving model to ./Checkpoints\\weights-improvement-epoch_492-loss_0.7211-accuracy_0.7586.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7211 - accuracy: 0.7586 - val_loss: 0.3251 - val_accuracy: 0.9107\n",
      "Epoch 493/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7262 - accuracy: 0.7575\n",
      "Epoch 00493: loss did not improve from 0.72112\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7262 - accuracy: 0.7575 - val_loss: 0.3210 - val_accuracy: 0.9133\n",
      "Epoch 494/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7249 - accuracy: 0.7576\n",
      "Epoch 00494: loss did not improve from 0.72112\n",
      "555/555 [==============================] - 29s 51ms/step - loss: 0.7249 - accuracy: 0.7576 - val_loss: 0.3221 - val_accuracy: 0.9120\n",
      "Epoch 495/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7164 - accuracy: 0.7596\n",
      "Epoch 00495: loss improved from 0.72112 to 0.71644, saving model to ./Checkpoints\\weights-improvement-epoch_495-loss_0.7164-accuracy_0.7596.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7164 - accuracy: 0.7596 - val_loss: 0.3108 - val_accuracy: 0.9130\n",
      "Epoch 496/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7159 - accuracy: 0.7593\n",
      "Epoch 00496: loss improved from 0.71644 to 0.71595, saving model to ./Checkpoints\\weights-improvement-epoch_496-loss_0.7159-accuracy_0.7593.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7159 - accuracy: 0.7593 - val_loss: 0.3241 - val_accuracy: 0.9114\n",
      "Epoch 497/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7202 - accuracy: 0.7578\n",
      "Epoch 00497: loss did not improve from 0.71595\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7202 - accuracy: 0.7578 - val_loss: 0.3197 - val_accuracy: 0.9103\n",
      "Epoch 498/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7175 - accuracy: 0.7588\n",
      "Epoch 00498: loss did not improve from 0.71595\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.7175 - accuracy: 0.7588 - val_loss: 0.3220 - val_accuracy: 0.9096\n",
      "Epoch 499/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7203 - accuracy: 0.7581\n",
      "Epoch 00499: loss did not improve from 0.71595\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7203 - accuracy: 0.7581 - val_loss: 0.3149 - val_accuracy: 0.9146\n",
      "Epoch 500/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7196 - accuracy: 0.7598\n",
      "Epoch 00500: loss did not improve from 0.71595\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.7196 - accuracy: 0.7598 - val_loss: 0.3128 - val_accuracy: 0.9143\n",
      "Epoch 501/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7103 - accuracy: 0.7638\n",
      "Epoch 00501: loss improved from 0.71595 to 0.71031, saving model to ./Checkpoints\\weights-improvement-epoch_501-loss_0.7103-accuracy_0.7638.hdf5\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 0.7103 - accuracy: 0.7638 - val_loss: 0.3263 - val_accuracy: 0.9099\n",
      "Epoch 502/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7193 - accuracy: 0.7615\n",
      "Epoch 00502: loss did not improve from 0.71031\n",
      "555/555 [==============================] - 30s 53ms/step - loss: 0.7193 - accuracy: 0.7615 - val_loss: 0.3132 - val_accuracy: 0.9143\n",
      "Epoch 503/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7199 - accuracy: 0.7588\n",
      "Epoch 00503: loss did not improve from 0.71031\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7199 - accuracy: 0.7588 - val_loss: 0.3155 - val_accuracy: 0.9148\n",
      "Epoch 504/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7081 - accuracy: 0.7627\n",
      "Epoch 00504: loss improved from 0.71031 to 0.70805, saving model to ./Checkpoints\\weights-improvement-epoch_504-loss_0.7081-accuracy_0.7627.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7081 - accuracy: 0.7627 - val_loss: 0.3128 - val_accuracy: 0.9125\n",
      "Epoch 505/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7183 - accuracy: 0.7597\n",
      "Epoch 00505: loss did not improve from 0.70805\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.7183 - accuracy: 0.7597 - val_loss: 0.3087 - val_accuracy: 0.9161\n",
      "Epoch 506/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7060 - accuracy: 0.7647\n",
      "Epoch 00506: loss improved from 0.70805 to 0.70600, saving model to ./Checkpoints\\weights-improvement-epoch_506-loss_0.7060-accuracy_0.7647.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7060 - accuracy: 0.7647 - val_loss: 0.3105 - val_accuracy: 0.9159\n",
      "Epoch 507/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7054 - accuracy: 0.7636\n",
      "Epoch 00507: loss improved from 0.70600 to 0.70541, saving model to ./Checkpoints\\weights-improvement-epoch_507-loss_0.7054-accuracy_0.7636.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7054 - accuracy: 0.7636 - val_loss: 0.3110 - val_accuracy: 0.9157\n",
      "Epoch 508/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7125 - accuracy: 0.7621\n",
      "Epoch 00508: loss did not improve from 0.70541\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7125 - accuracy: 0.7621 - val_loss: 0.3107 - val_accuracy: 0.9163\n",
      "Epoch 509/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7158 - accuracy: 0.7633\n",
      "Epoch 00509: loss did not improve from 0.70541\n",
      "555/555 [==============================] - 29s 51ms/step - loss: 0.7158 - accuracy: 0.7633 - val_loss: 0.3155 - val_accuracy: 0.9163\n",
      "Epoch 510/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7126 - accuracy: 0.7616\n",
      "Epoch 00510: loss did not improve from 0.70541\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7126 - accuracy: 0.7616 - val_loss: 0.3058 - val_accuracy: 0.9164\n",
      "Epoch 511/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7092 - accuracy: 0.7623\n",
      "Epoch 00511: loss did not improve from 0.70541\n",
      "555/555 [==============================] - 29s 51ms/step - loss: 0.7092 - accuracy: 0.7623 - val_loss: 0.3059 - val_accuracy: 0.9160\n",
      "Epoch 512/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7054 - accuracy: 0.7629\n",
      "Epoch 00512: loss improved from 0.70541 to 0.70536, saving model to ./Checkpoints\\weights-improvement-epoch_512-loss_0.7054-accuracy_0.7629.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.7054 - accuracy: 0.7629 - val_loss: 0.3051 - val_accuracy: 0.9172\n",
      "Epoch 513/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7097 - accuracy: 0.7622\n",
      "Epoch 00513: loss did not improve from 0.70536\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7097 - accuracy: 0.7622 - val_loss: 0.3107 - val_accuracy: 0.9140\n",
      "Epoch 514/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7140 - accuracy: 0.7595\n",
      "Epoch 00514: loss did not improve from 0.70536\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.7140 - accuracy: 0.7595 - val_loss: 0.3049 - val_accuracy: 0.9167\n",
      "Epoch 515/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7096 - accuracy: 0.7618\n",
      "Epoch 00515: loss did not improve from 0.70536\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.7096 - accuracy: 0.7618 - val_loss: 0.2984 - val_accuracy: 0.9170\n",
      "Epoch 516/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6968 - accuracy: 0.7665\n",
      "Epoch 00516: loss improved from 0.70536 to 0.69682, saving model to ./Checkpoints\\weights-improvement-epoch_516-loss_0.6968-accuracy_0.7665.hdf5\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.6968 - accuracy: 0.7665 - val_loss: 0.3095 - val_accuracy: 0.9134\n",
      "Epoch 517/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7033 - accuracy: 0.7658\n",
      "Epoch 00517: loss did not improve from 0.69682\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 0.7032 - accuracy: 0.7658 - val_loss: 0.3062 - val_accuracy: 0.9148\n",
      "Epoch 518/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.7667\n",
      "Epoch 00518: loss did not improve from 0.69682\n",
      "555/555 [==============================] - 30s 55ms/step - loss: 0.6991 - accuracy: 0.7667 - val_loss: 0.3034 - val_accuracy: 0.9179\n",
      "Epoch 519/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7057 - accuracy: 0.7651\n",
      "Epoch 00519: loss did not improve from 0.69682\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 0.7057 - accuracy: 0.7651 - val_loss: 0.3125 - val_accuracy: 0.9130\n",
      "Epoch 520/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7003 - accuracy: 0.7638\n",
      "Epoch 00520: loss did not improve from 0.69682\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 0.7004 - accuracy: 0.7637 - val_loss: 0.2970 - val_accuracy: 0.9182\n",
      "Epoch 521/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7056 - accuracy: 0.7637\n",
      "Epoch 00521: loss did not improve from 0.69682\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 0.7055 - accuracy: 0.7637 - val_loss: 0.3009 - val_accuracy: 0.9191\n",
      "Epoch 522/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.7659\n",
      "Epoch 00522: loss did not improve from 0.69682\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.7004 - accuracy: 0.7659 - val_loss: 0.3049 - val_accuracy: 0.9170\n",
      "Epoch 523/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.7681\n",
      "Epoch 00523: loss improved from 0.69682 to 0.69429, saving model to ./Checkpoints\\weights-improvement-epoch_523-loss_0.6943-accuracy_0.7681.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6943 - accuracy: 0.7681 - val_loss: 0.2986 - val_accuracy: 0.9186\n",
      "Epoch 524/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7066 - accuracy: 0.7635\n",
      "Epoch 00524: loss did not improve from 0.69429\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.7066 - accuracy: 0.7636 - val_loss: 0.2980 - val_accuracy: 0.9180\n",
      "Epoch 525/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6985 - accuracy: 0.7640\n",
      "Epoch 00525: loss did not improve from 0.69429\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6985 - accuracy: 0.7640 - val_loss: 0.3008 - val_accuracy: 0.9184\n",
      "Epoch 526/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.7692\n",
      "Epoch 00526: loss improved from 0.69429 to 0.69351, saving model to ./Checkpoints\\weights-improvement-epoch_526-loss_0.6935-accuracy_0.7692.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6935 - accuracy: 0.7692 - val_loss: 0.2913 - val_accuracy: 0.9218\n",
      "Epoch 527/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7025 - accuracy: 0.7644\n",
      "Epoch 00527: loss did not improve from 0.69351\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.7025 - accuracy: 0.7644 - val_loss: 0.3068 - val_accuracy: 0.9167\n",
      "Epoch 528/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7002 - accuracy: 0.7667\n",
      "Epoch 00528: loss did not improve from 0.69351\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.7002 - accuracy: 0.7667 - val_loss: 0.2928 - val_accuracy: 0.9207\n",
      "Epoch 529/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.7682\n",
      "Epoch 00529: loss improved from 0.69351 to 0.69133, saving model to ./Checkpoints\\weights-improvement-epoch_529-loss_0.6913-accuracy_0.7682.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6913 - accuracy: 0.7682 - val_loss: 0.2918 - val_accuracy: 0.9225\n",
      "Epoch 530/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.7662\n",
      "Epoch 00530: loss did not improve from 0.69133\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.6932 - accuracy: 0.7662 - val_loss: 0.2910 - val_accuracy: 0.9227\n",
      "Epoch 531/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.7700\n",
      "Epoch 00531: loss improved from 0.69133 to 0.68901, saving model to ./Checkpoints\\weights-improvement-epoch_531-loss_0.6890-accuracy_0.7700.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6890 - accuracy: 0.7700 - val_loss: 0.3003 - val_accuracy: 0.9194\n",
      "Epoch 532/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.7689\n",
      "Epoch 00532: loss did not improve from 0.68901\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6912 - accuracy: 0.7689 - val_loss: 0.3090 - val_accuracy: 0.9148\n",
      "Epoch 533/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.7681\n",
      "Epoch 00533: loss did not improve from 0.68901\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.6911 - accuracy: 0.7681 - val_loss: 0.2881 - val_accuracy: 0.9227\n",
      "Epoch 534/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.7706\n",
      "Epoch 00534: loss improved from 0.68901 to 0.68496, saving model to ./Checkpoints\\weights-improvement-epoch_534-loss_0.6850-accuracy_0.7706.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6850 - accuracy: 0.7706 - val_loss: 0.2880 - val_accuracy: 0.9232\n",
      "Epoch 535/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.7684\n",
      "Epoch 00535: loss did not improve from 0.68496\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.6939 - accuracy: 0.7684 - val_loss: 0.2989 - val_accuracy: 0.9200\n",
      "Epoch 536/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.7706\n",
      "Epoch 00536: loss did not improve from 0.68496\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 0.6875 - accuracy: 0.7706 - val_loss: 0.2888 - val_accuracy: 0.9216\n",
      "Epoch 537/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.7693\n",
      "Epoch 00537: loss did not improve from 0.68496\n",
      "555/555 [==============================] - 30s 55ms/step - loss: 0.6911 - accuracy: 0.7693 - val_loss: 0.2874 - val_accuracy: 0.9237\n",
      "Epoch 538/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6857 - accuracy: 0.7706\n",
      "Epoch 00538: loss did not improve from 0.68496\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.6857 - accuracy: 0.7706 - val_loss: 0.2838 - val_accuracy: 0.9246\n",
      "Epoch 539/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.7702\n",
      "Epoch 00539: loss did not improve from 0.68496\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.6874 - accuracy: 0.7702 - val_loss: 0.2842 - val_accuracy: 0.9255\n",
      "Epoch 540/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.7715\n",
      "Epoch 00540: loss did not improve from 0.68496\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6867 - accuracy: 0.7715 - val_loss: 0.2865 - val_accuracy: 0.9236\n",
      "Epoch 541/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.7703\n",
      "Epoch 00541: loss improved from 0.68496 to 0.68490, saving model to ./Checkpoints\\weights-improvement-epoch_541-loss_0.6849-accuracy_0.7703.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6849 - accuracy: 0.7703 - val_loss: 0.2786 - val_accuracy: 0.9258\n",
      "Epoch 542/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.7722\n",
      "Epoch 00542: loss improved from 0.68490 to 0.68215, saving model to ./Checkpoints\\weights-improvement-epoch_542-loss_0.6821-accuracy_0.7721.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6821 - accuracy: 0.7721 - val_loss: 0.2886 - val_accuracy: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 543/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.7699\n",
      "Epoch 00543: loss did not improve from 0.68215\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6891 - accuracy: 0.7699 - val_loss: 0.2888 - val_accuracy: 0.9233\n",
      "Epoch 544/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.7704\n",
      "Epoch 00544: loss did not improve from 0.68215\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6859 - accuracy: 0.7704 - val_loss: 0.2882 - val_accuracy: 0.9239\n",
      "Epoch 545/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6856 - accuracy: 0.7704\n",
      "Epoch 00545: loss did not improve from 0.68215\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6856 - accuracy: 0.7704 - val_loss: 0.2828 - val_accuracy: 0.9251\n",
      "Epoch 546/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6876 - accuracy: 0.7707 ETA: 1s -\n",
      "Epoch 00546: loss did not improve from 0.68215\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6876 - accuracy: 0.7707 - val_loss: 0.2926 - val_accuracy: 0.9194\n",
      "Epoch 547/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.7683\n",
      "Epoch 00547: loss did not improve from 0.68215\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6944 - accuracy: 0.7683 - val_loss: 0.2824 - val_accuracy: 0.9262\n",
      "Epoch 548/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.7735\n",
      "Epoch 00548: loss improved from 0.68215 to 0.67640, saving model to ./Checkpoints\\weights-improvement-epoch_548-loss_0.6764-accuracy_0.7735.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6764 - accuracy: 0.7735 - val_loss: 0.2877 - val_accuracy: 0.9227\n",
      "Epoch 549/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.7720\n",
      "Epoch 00549: loss did not improve from 0.67640\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6764 - accuracy: 0.7720 - val_loss: 0.2771 - val_accuracy: 0.9267\n",
      "Epoch 550/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.7725\n",
      "Epoch 00550: loss did not improve from 0.67640\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6888 - accuracy: 0.7725 - val_loss: 0.2787 - val_accuracy: 0.9257\n",
      "Epoch 551/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.7724\n",
      "Epoch 00551: loss did not improve from 0.67640\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6838 - accuracy: 0.7724 - val_loss: 0.2884 - val_accuracy: 0.9251\n",
      "Epoch 552/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.7735\n",
      "Epoch 00552: loss improved from 0.67640 to 0.67638, saving model to ./Checkpoints\\weights-improvement-epoch_552-loss_0.6764-accuracy_0.7735.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6764 - accuracy: 0.7735 - val_loss: 0.2806 - val_accuracy: 0.9239\n",
      "Epoch 553/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6760 - accuracy: 0.7736\n",
      "Epoch 00553: loss improved from 0.67638 to 0.67600, saving model to ./Checkpoints\\weights-improvement-epoch_553-loss_0.6760-accuracy_0.7736.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6760 - accuracy: 0.7736 - val_loss: 0.2803 - val_accuracy: 0.9249\n",
      "Epoch 554/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6698 - accuracy: 0.7747\n",
      "Epoch 00554: loss improved from 0.67600 to 0.66985, saving model to ./Checkpoints\\weights-improvement-epoch_554-loss_0.6698-accuracy_0.7747.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6698 - accuracy: 0.7747 - val_loss: 0.2780 - val_accuracy: 0.9247\n",
      "Epoch 555/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6730 - accuracy: 0.7743\n",
      "Epoch 00555: loss did not improve from 0.66985\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6730 - accuracy: 0.7743 - val_loss: 0.2788 - val_accuracy: 0.9265\n",
      "Epoch 556/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.7705\n",
      "Epoch 00556: loss did not improve from 0.66985\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6860 - accuracy: 0.7705 - val_loss: 0.2831 - val_accuracy: 0.9259\n",
      "Epoch 557/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6784 - accuracy: 0.7726\n",
      "Epoch 00557: loss did not improve from 0.66985\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6784 - accuracy: 0.7726 - val_loss: 0.2810 - val_accuracy: 0.9254\n",
      "Epoch 558/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6711 - accuracy: 0.7732\n",
      "Epoch 00558: loss did not improve from 0.66985\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6711 - accuracy: 0.7732 - val_loss: 0.2789 - val_accuracy: 0.9262\n",
      "Epoch 559/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.7726\n",
      "Epoch 00559: loss did not improve from 0.66985\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6798 - accuracy: 0.7726 - val_loss: 0.2753 - val_accuracy: 0.9268\n",
      "Epoch 560/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6693 - accuracy: 0.7748\n",
      "Epoch 00560: loss improved from 0.66985 to 0.66926, saving model to ./Checkpoints\\weights-improvement-epoch_560-loss_0.6693-accuracy_0.7748.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6693 - accuracy: 0.7748 - val_loss: 0.2715 - val_accuracy: 0.9277\n",
      "Epoch 561/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 0.7744\n",
      "Epoch 00561: loss did not improve from 0.66926\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6706 - accuracy: 0.7744 - val_loss: 0.2804 - val_accuracy: 0.9267\n",
      "Epoch 562/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6719 - accuracy: 0.7755\n",
      "Epoch 00562: loss did not improve from 0.66926\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6719 - accuracy: 0.7755 - val_loss: 0.2764 - val_accuracy: 0.9271\n",
      "Epoch 563/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6731 - accuracy: 0.7769\n",
      "Epoch 00563: loss did not improve from 0.66926\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6731 - accuracy: 0.7769 - val_loss: 0.2755 - val_accuracy: 0.9285\n",
      "Epoch 564/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6715 - accuracy: 0.7751\n",
      "Epoch 00564: loss did not improve from 0.66926\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6715 - accuracy: 0.7751 - val_loss: 0.2735 - val_accuracy: 0.9284\n",
      "Epoch 565/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.6702 - accuracy: 0.7754\n",
      "Epoch 00565: loss did not improve from 0.66926\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6700 - accuracy: 0.7755 - val_loss: 0.2695 - val_accuracy: 0.9281\n",
      "Epoch 566/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6697 - accuracy: 0.7755\n",
      "Epoch 00566: loss did not improve from 0.66926\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6697 - accuracy: 0.7755 - val_loss: 0.2701 - val_accuracy: 0.9295\n",
      "Epoch 567/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6655 - accuracy: 0.7775\n",
      "Epoch 00567: loss improved from 0.66926 to 0.66554, saving model to ./Checkpoints\\weights-improvement-epoch_567-loss_0.6655-accuracy_0.7775.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6655 - accuracy: 0.7775 - val_loss: 0.2688 - val_accuracy: 0.9282\n",
      "Epoch 568/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6656 - accuracy: 0.7772\n",
      "Epoch 00568: loss did not improve from 0.66554\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6656 - accuracy: 0.7772 - val_loss: 0.2714 - val_accuracy: 0.9291\n",
      "Epoch 569/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6714 - accuracy: 0.7759\n",
      "Epoch 00569: loss did not improve from 0.66554\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 0.6714 - accuracy: 0.7759 - val_loss: 0.2663 - val_accuracy: 0.9315\n",
      "Epoch 570/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6650 - accuracy: 0.7764\n",
      "Epoch 00570: loss improved from 0.66554 to 0.66498, saving model to ./Checkpoints\\weights-improvement-epoch_570-loss_0.6650-accuracy_0.7764.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6650 - accuracy: 0.7764 - val_loss: 0.2727 - val_accuracy: 0.9271\n",
      "Epoch 571/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6638 - accuracy: 0.7783\n",
      "Epoch 00571: loss improved from 0.66498 to 0.66386, saving model to ./Checkpoints\\weights-improvement-epoch_571-loss_0.6639-accuracy_0.7783.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6639 - accuracy: 0.7783 - val_loss: 0.2661 - val_accuracy: 0.9299\n",
      "Epoch 572/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6637 - accuracy: 0.7763\n",
      "Epoch 00572: loss improved from 0.66386 to 0.66367, saving model to ./Checkpoints\\weights-improvement-epoch_572-loss_0.6637-accuracy_0.7763.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6637 - accuracy: 0.7763 - val_loss: 0.2718 - val_accuracy: 0.9279\n",
      "Epoch 573/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.6630 - accuracy: 0.7777\n",
      "Epoch 00573: loss improved from 0.66367 to 0.66289, saving model to ./Checkpoints\\weights-improvement-epoch_573-loss_0.6629-accuracy_0.7777.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6629 - accuracy: 0.7777 - val_loss: 0.2693 - val_accuracy: 0.9299\n",
      "Epoch 574/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.7767\n",
      "Epoch 00574: loss did not improve from 0.66289\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6676 - accuracy: 0.7767 - val_loss: 0.2719 - val_accuracy: 0.9286\n",
      "Epoch 575/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6646 - accuracy: 0.7791\n",
      "Epoch 00575: loss did not improve from 0.66289\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6646 - accuracy: 0.7791 - val_loss: 0.2637 - val_accuracy: 0.9317\n",
      "Epoch 576/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6529 - accuracy: 0.7807\n",
      "Epoch 00576: loss improved from 0.66289 to 0.65294, saving model to ./Checkpoints\\weights-improvement-epoch_576-loss_0.6529-accuracy_0.7807.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6529 - accuracy: 0.7807 - val_loss: 0.2721 - val_accuracy: 0.9272\n",
      "Epoch 577/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6657 - accuracy: 0.7781\n",
      "Epoch 00577: loss did not improve from 0.65294\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6657 - accuracy: 0.7781 - val_loss: 0.2604 - val_accuracy: 0.9324\n",
      "Epoch 578/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6653 - accuracy: 0.7768\n",
      "Epoch 00578: loss did not improve from 0.65294\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6653 - accuracy: 0.7768 - val_loss: 0.2683 - val_accuracy: 0.9285\n",
      "Epoch 579/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6614 - accuracy: 0.7790\n",
      "Epoch 00579: loss did not improve from 0.65294\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6614 - accuracy: 0.7790 - val_loss: 0.2621 - val_accuracy: 0.9315\n",
      "Epoch 580/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6576 - accuracy: 0.7804\n",
      "Epoch 00580: loss did not improve from 0.65294\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6576 - accuracy: 0.7804 - val_loss: 0.2648 - val_accuracy: 0.9300\n",
      "Epoch 581/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.7771\n",
      "Epoch 00581: loss did not improve from 0.65294\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6627 - accuracy: 0.7771 - val_loss: 0.2622 - val_accuracy: 0.9300\n",
      "Epoch 582/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.6600 - accuracy: 0.7779\n",
      "Epoch 00582: loss did not improve from 0.65294\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6600 - accuracy: 0.7780 - val_loss: 0.2648 - val_accuracy: 0.9304\n",
      "Epoch 583/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6540 - accuracy: 0.7840\n",
      "Epoch 00583: loss did not improve from 0.65294\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6540 - accuracy: 0.7840 - val_loss: 0.2570 - val_accuracy: 0.9325\n",
      "Epoch 584/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.6597 - accuracy: 0.7777\n",
      "Epoch 00584: loss did not improve from 0.65294\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6601 - accuracy: 0.7776 - val_loss: 0.2642 - val_accuracy: 0.9306\n",
      "Epoch 585/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6534 - accuracy: 0.7819\n",
      "Epoch 00585: loss did not improve from 0.65294\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6534 - accuracy: 0.7819 - val_loss: 0.2622 - val_accuracy: 0.9323\n",
      "Epoch 586/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6526 - accuracy: 0.7815\n",
      "Epoch 00586: loss improved from 0.65294 to 0.65258, saving model to ./Checkpoints\\weights-improvement-epoch_586-loss_0.6526-accuracy_0.7815.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6526 - accuracy: 0.7815 - val_loss: 0.2583 - val_accuracy: 0.9344\n",
      "Epoch 587/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6581 - accuracy: 0.7807\n",
      "Epoch 00587: loss did not improve from 0.65258\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6581 - accuracy: 0.7807 - val_loss: 0.2696 - val_accuracy: 0.9292\n",
      "Epoch 588/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6566 - accuracy: 0.7810\n",
      "Epoch 00588: loss did not improve from 0.65258\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6566 - accuracy: 0.7810 - val_loss: 0.2626 - val_accuracy: 0.9311\n",
      "Epoch 589/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6549 - accuracy: 0.7804\n",
      "Epoch 00589: loss did not improve from 0.65258\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6549 - accuracy: 0.7804 - val_loss: 0.2553 - val_accuracy: 0.9334\n",
      "Epoch 590/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6506 - accuracy: 0.7823\n",
      "Epoch 00590: loss improved from 0.65258 to 0.65055, saving model to ./Checkpoints\\weights-improvement-epoch_590-loss_0.6506-accuracy_0.7823.hdf5\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 0.6506 - accuracy: 0.7823 - val_loss: 0.2561 - val_accuracy: 0.9351\n",
      "Epoch 591/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6506 - accuracy: 0.7805\n",
      "Epoch 00591: loss did not improve from 0.65055\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6506 - accuracy: 0.7805 - val_loss: 0.2676 - val_accuracy: 0.9292\n",
      "Epoch 592/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6517 - accuracy: 0.7816\n",
      "Epoch 00592: loss did not improve from 0.65055\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6517 - accuracy: 0.7816 - val_loss: 0.2553 - val_accuracy: 0.9340\n",
      "Epoch 593/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6446 - accuracy: 0.7823\n",
      "Epoch 00593: loss improved from 0.65055 to 0.64462, saving model to ./Checkpoints\\weights-improvement-epoch_593-loss_0.6446-accuracy_0.7823.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6446 - accuracy: 0.7823 - val_loss: 0.2553 - val_accuracy: 0.9344\n",
      "Epoch 594/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6523 - accuracy: 0.7816\n",
      "Epoch 00594: loss did not improve from 0.64462\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6523 - accuracy: 0.7816 - val_loss: 0.2474 - val_accuracy: 0.9358\n",
      "Epoch 595/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6473 - accuracy: 0.7831\n",
      "Epoch 00595: loss did not improve from 0.64462\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6473 - accuracy: 0.7831 - val_loss: 0.2507 - val_accuracy: 0.9352\n",
      "Epoch 596/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.7833\n",
      "Epoch 00596: loss did not improve from 0.64462\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6528 - accuracy: 0.7833 - val_loss: 0.2565 - val_accuracy: 0.9337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 597/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.7809\n",
      "Epoch 00597: loss did not improve from 0.64462\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6594 - accuracy: 0.7809 - val_loss: 0.2545 - val_accuracy: 0.9350\n",
      "Epoch 598/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6506 - accuracy: 0.7840\n",
      "Epoch 00598: loss did not improve from 0.64462\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6507 - accuracy: 0.7840 - val_loss: 0.2436 - val_accuracy: 0.9383\n",
      "Epoch 599/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6408 - accuracy: 0.7829\n",
      "Epoch 00599: loss improved from 0.64462 to 0.64075, saving model to ./Checkpoints\\weights-improvement-epoch_599-loss_0.6408-accuracy_0.7830.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6408 - accuracy: 0.7830 - val_loss: 0.2493 - val_accuracy: 0.9354\n",
      "Epoch 600/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6463 - accuracy: 0.7839\n",
      "Epoch 00600: loss did not improve from 0.64075\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6463 - accuracy: 0.7839 - val_loss: 0.2513 - val_accuracy: 0.9337\n",
      "Epoch 601/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6376 - accuracy: 0.7862\n",
      "Epoch 00601: loss improved from 0.64075 to 0.63757, saving model to ./Checkpoints\\weights-improvement-epoch_601-loss_0.6376-accuracy_0.7863.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6376 - accuracy: 0.7863 - val_loss: 0.2485 - val_accuracy: 0.9353\n",
      "Epoch 602/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6572 - accuracy: 0.7813\n",
      "Epoch 00602: loss did not improve from 0.63757\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6572 - accuracy: 0.7813 - val_loss: 0.2575 - val_accuracy: 0.9334\n",
      "Epoch 603/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.7843\n",
      "Epoch 00603: loss did not improve from 0.63757\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 0.6474 - accuracy: 0.7843 - val_loss: 0.2497 - val_accuracy: 0.9347\n",
      "Epoch 604/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6403 - accuracy: 0.7871\n",
      "Epoch 00604: loss did not improve from 0.63757\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6404 - accuracy: 0.7871 - val_loss: 0.2486 - val_accuracy: 0.9353\n",
      "Epoch 605/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6441 - accuracy: 0.7859\n",
      "Epoch 00605: loss did not improve from 0.63757\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6441 - accuracy: 0.7859 - val_loss: 0.2494 - val_accuracy: 0.9355\n",
      "Epoch 606/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.7853\n",
      "Epoch 00606: loss did not improve from 0.63757\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6430 - accuracy: 0.7853 - val_loss: 0.2480 - val_accuracy: 0.9353\n",
      "Epoch 607/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6397 - accuracy: 0.7851\n",
      "Epoch 00607: loss did not improve from 0.63757\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6397 - accuracy: 0.7851 - val_loss: 0.2463 - val_accuracy: 0.9366\n",
      "Epoch 608/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.6425 - accuracy: 0.7852\n",
      "Epoch 00608: loss did not improve from 0.63757\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6425 - accuracy: 0.7852 - val_loss: 0.2572 - val_accuracy: 0.9315\n",
      "Epoch 609/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6417 - accuracy: 0.7848\n",
      "Epoch 00609: loss did not improve from 0.63757\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6417 - accuracy: 0.7848 - val_loss: 0.2423 - val_accuracy: 0.9372\n",
      "Epoch 610/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6400 - accuracy: 0.7853\n",
      "Epoch 00610: loss did not improve from 0.63757\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6400 - accuracy: 0.7853 - val_loss: 0.2500 - val_accuracy: 0.9358\n",
      "Epoch 611/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6318 - accuracy: 0.7881\n",
      "Epoch 00611: loss improved from 0.63757 to 0.63175, saving model to ./Checkpoints\\weights-improvement-epoch_611-loss_0.6318-accuracy_0.7881.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6318 - accuracy: 0.7881 - val_loss: 0.2437 - val_accuracy: 0.9368\n",
      "Epoch 612/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6381 - accuracy: 0.7836\n",
      "Epoch 00612: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6381 - accuracy: 0.7836 - val_loss: 0.2479 - val_accuracy: 0.9354\n",
      "Epoch 613/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6383 - accuracy: 0.7862\n",
      "Epoch 00613: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6383 - accuracy: 0.7862 - val_loss: 0.2431 - val_accuracy: 0.9381\n",
      "Epoch 614/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6347 - accuracy: 0.7884\n",
      "Epoch 00614: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6347 - accuracy: 0.7884 - val_loss: 0.2385 - val_accuracy: 0.9378\n",
      "Epoch 615/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6623 - accuracy: 0.7789\n",
      "Epoch 00615: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6623 - accuracy: 0.7789 - val_loss: 0.2521 - val_accuracy: 0.9358\n",
      "Epoch 616/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6358 - accuracy: 0.7871\n",
      "Epoch 00616: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6359 - accuracy: 0.7871 - val_loss: 0.2412 - val_accuracy: 0.9378\n",
      "Epoch 617/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.7873\n",
      "Epoch 00617: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6328 - accuracy: 0.7873 - val_loss: 0.2430 - val_accuracy: 0.9372\n",
      "Epoch 618/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6400 - accuracy: 0.7870\n",
      "Epoch 00618: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6400 - accuracy: 0.7870 - val_loss: 0.2354 - val_accuracy: 0.9403\n",
      "Epoch 619/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6333 - accuracy: 0.7878\n",
      "Epoch 00619: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6333 - accuracy: 0.7878 - val_loss: 0.2436 - val_accuracy: 0.9370\n",
      "Epoch 620/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6318 - accuracy: 0.7858\n",
      "Epoch 00620: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6318 - accuracy: 0.7858 - val_loss: 0.2407 - val_accuracy: 0.9367\n",
      "Epoch 621/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6358 - accuracy: 0.7864\n",
      "Epoch 00621: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6359 - accuracy: 0.7864 - val_loss: 0.2388 - val_accuracy: 0.9395\n",
      "Epoch 622/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6379 - accuracy: 0.7873\n",
      "Epoch 00622: loss did not improve from 0.63175\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6379 - accuracy: 0.7873 - val_loss: 0.2444 - val_accuracy: 0.9378\n",
      "Epoch 623/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6299 - accuracy: 0.7876\n",
      "Epoch 00623: loss improved from 0.63175 to 0.62986, saving model to ./Checkpoints\\weights-improvement-epoch_623-loss_0.6299-accuracy_0.7875.hdf5\n",
      "555/555 [==============================] - 29s 51ms/step - loss: 0.6299 - accuracy: 0.7875 - val_loss: 0.2397 - val_accuracy: 0.9396\n",
      "Epoch 624/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6324 - accuracy: 0.7882\n",
      "Epoch 00624: loss did not improve from 0.62986\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6324 - accuracy: 0.7882 - val_loss: 0.2326 - val_accuracy: 0.9405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 625/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6398 - accuracy: 0.7860\n",
      "Epoch 00625: loss did not improve from 0.62986\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6398 - accuracy: 0.7860 - val_loss: 0.2369 - val_accuracy: 0.9399\n",
      "Epoch 626/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6381 - accuracy: 0.7852\n",
      "Epoch 00626: loss did not improve from 0.62986\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6381 - accuracy: 0.7852 - val_loss: 0.2391 - val_accuracy: 0.9388\n",
      "Epoch 627/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6307 - accuracy: 0.7886\n",
      "Epoch 00627: loss did not improve from 0.62986\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6307 - accuracy: 0.7886 - val_loss: 0.2373 - val_accuracy: 0.9390\n",
      "Epoch 628/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6342 - accuracy: 0.7883\n",
      "Epoch 00628: loss did not improve from 0.62986\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6342 - accuracy: 0.7883 - val_loss: 0.2365 - val_accuracy: 0.9386\n",
      "Epoch 629/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6316 - accuracy: 0.7887\n",
      "Epoch 00629: loss did not improve from 0.62986\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6316 - accuracy: 0.7887 - val_loss: 0.2315 - val_accuracy: 0.9409\n",
      "Epoch 630/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.7907\n",
      "Epoch 00630: loss improved from 0.62986 to 0.62240, saving model to ./Checkpoints\\weights-improvement-epoch_630-loss_0.6224-accuracy_0.7907.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6224 - accuracy: 0.7907 - val_loss: 0.2420 - val_accuracy: 0.9368\n",
      "Epoch 631/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6312 - accuracy: 0.7895\n",
      "Epoch 00631: loss did not improve from 0.62240\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6312 - accuracy: 0.7895 - val_loss: 0.2388 - val_accuracy: 0.9378\n",
      "Epoch 632/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6247 - accuracy: 0.7910\n",
      "Epoch 00632: loss did not improve from 0.62240\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6247 - accuracy: 0.7910 - val_loss: 0.2298 - val_accuracy: 0.9400\n",
      "Epoch 633/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6312 - accuracy: 0.7899\n",
      "Epoch 00633: loss did not improve from 0.62240\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6312 - accuracy: 0.7899 - val_loss: 0.2313 - val_accuracy: 0.9420\n",
      "Epoch 634/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.7899\n",
      "Epoch 00634: loss did not improve from 0.62240\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.6249 - accuracy: 0.7899 - val_loss: 0.2329 - val_accuracy: 0.9400\n",
      "Epoch 635/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6286 - accuracy: 0.7876\n",
      "Epoch 00635: loss did not improve from 0.62240\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6286 - accuracy: 0.7877 - val_loss: 0.2397 - val_accuracy: 0.9381\n",
      "Epoch 636/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6304 - accuracy: 0.7899\n",
      "Epoch 00636: loss did not improve from 0.62240\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6304 - accuracy: 0.7899 - val_loss: 0.2334 - val_accuracy: 0.9416\n",
      "Epoch 637/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.7950\n",
      "Epoch 00637: loss improved from 0.62240 to 0.62172, saving model to ./Checkpoints\\weights-improvement-epoch_637-loss_0.6217-accuracy_0.7950.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6217 - accuracy: 0.7950 - val_loss: 0.2253 - val_accuracy: 0.9433\n",
      "Epoch 638/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6276 - accuracy: 0.7917\n",
      "Epoch 00638: loss did not improve from 0.62172\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6276 - accuracy: 0.7917 - val_loss: 0.2339 - val_accuracy: 0.9415\n",
      "Epoch 639/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6225 - accuracy: 0.7906\n",
      "Epoch 00639: loss did not improve from 0.62172\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6225 - accuracy: 0.7906 - val_loss: 0.2308 - val_accuracy: 0.9416\n",
      "Epoch 640/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.7938\n",
      "Epoch 00640: loss improved from 0.62172 to 0.61693, saving model to ./Checkpoints\\weights-improvement-epoch_640-loss_0.6169-accuracy_0.7938.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6169 - accuracy: 0.7938 - val_loss: 0.2444 - val_accuracy: 0.9365\n",
      "Epoch 641/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6232 - accuracy: 0.7910\n",
      "Epoch 00641: loss did not improve from 0.61693\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6232 - accuracy: 0.7910 - val_loss: 0.2364 - val_accuracy: 0.9395\n",
      "Epoch 642/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.7938\n",
      "Epoch 00642: loss did not improve from 0.61693\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6192 - accuracy: 0.7938 - val_loss: 0.2252 - val_accuracy: 0.9432\n",
      "Epoch 643/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.7942\n",
      "Epoch 00643: loss improved from 0.61693 to 0.61535, saving model to ./Checkpoints\\weights-improvement-epoch_643-loss_0.6153-accuracy_0.7942.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6153 - accuracy: 0.7942 - val_loss: 0.2250 - val_accuracy: 0.9430\n",
      "Epoch 644/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6181 - accuracy: 0.7926\n",
      "Epoch 00644: loss did not improve from 0.61535\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6181 - accuracy: 0.7926 - val_loss: 0.2264 - val_accuracy: 0.9425\n",
      "Epoch 645/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6261 - accuracy: 0.7918\n",
      "Epoch 00645: loss did not improve from 0.61535\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6261 - accuracy: 0.7918 - val_loss: 0.2300 - val_accuracy: 0.9416\n",
      "Epoch 646/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6202 - accuracy: 0.7945\n",
      "Epoch 00646: loss did not improve from 0.61535\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6202 - accuracy: 0.7945 - val_loss: 0.2274 - val_accuracy: 0.9415\n",
      "Epoch 647/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6203 - accuracy: 0.7911\n",
      "Epoch 00647: loss did not improve from 0.61535\n",
      "555/555 [==============================] - 29s 51ms/step - loss: 0.6203 - accuracy: 0.7911 - val_loss: 0.2281 - val_accuracy: 0.9420\n",
      "Epoch 648/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6211 - accuracy: 0.7920\n",
      "Epoch 00648: loss did not improve from 0.61535\n",
      "555/555 [==============================] - 27s 48ms/step - loss: 0.6211 - accuracy: 0.7920 - val_loss: 0.2294 - val_accuracy: 0.9408\n",
      "Epoch 649/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6245 - accuracy: 0.7902\n",
      "Epoch 00649: loss did not improve from 0.61535\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6245 - accuracy: 0.7902 - val_loss: 0.2344 - val_accuracy: 0.9387\n",
      "Epoch 650/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6160 - accuracy: 0.7936\n",
      "Epoch 00650: loss did not improve from 0.61535\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6160 - accuracy: 0.7936 - val_loss: 0.2195 - val_accuracy: 0.9460\n",
      "Epoch 651/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6161 - accuracy: 0.7935\n",
      "Epoch 00651: loss did not improve from 0.61535\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6161 - accuracy: 0.7935 - val_loss: 0.2203 - val_accuracy: 0.9454\n",
      "Epoch 652/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6150 - accuracy: 0.7948\n",
      "Epoch 00652: loss improved from 0.61535 to 0.61498, saving model to ./Checkpoints\\weights-improvement-epoch_652-loss_0.6150-accuracy_0.7948.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6150 - accuracy: 0.7948 - val_loss: 0.2213 - val_accuracy: 0.9446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 653/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.6205 - accuracy: 0.7921\n",
      "Epoch 00653: loss did not improve from 0.61498\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 0.6202 - accuracy: 0.7922 - val_loss: 0.2256 - val_accuracy: 0.9432\n",
      "Epoch 654/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6241 - accuracy: 0.7901\n",
      "Epoch 00654: loss did not improve from 0.61498\n",
      "555/555 [==============================] - 27s 48ms/step - loss: 0.6241 - accuracy: 0.7901 - val_loss: 0.2214 - val_accuracy: 0.9449\n",
      "Epoch 655/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6128 - accuracy: 0.7948\n",
      "Epoch 00655: loss improved from 0.61498 to 0.61284, saving model to ./Checkpoints\\weights-improvement-epoch_655-loss_0.6128-accuracy_0.7948.hdf5\n",
      "555/555 [==============================] - 27s 48ms/step - loss: 0.6128 - accuracy: 0.7948 - val_loss: 0.2184 - val_accuracy: 0.9459\n",
      "Epoch 656/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6211 - accuracy: 0.7925\n",
      "Epoch 00656: loss did not improve from 0.61284\n",
      "555/555 [==============================] - 27s 48ms/step - loss: 0.6211 - accuracy: 0.7925 - val_loss: 0.2174 - val_accuracy: 0.9460\n",
      "Epoch 657/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6420 - accuracy: 0.7861\n",
      "Epoch 00657: loss did not improve from 0.61284\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6420 - accuracy: 0.7861 - val_loss: 0.2192 - val_accuracy: 0.9452\n",
      "Epoch 658/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.7956\n",
      "Epoch 00658: loss did not improve from 0.61284\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6153 - accuracy: 0.7956 - val_loss: 0.2215 - val_accuracy: 0.9451\n",
      "Epoch 659/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.7972\n",
      "Epoch 00659: loss improved from 0.61284 to 0.60723, saving model to ./Checkpoints\\weights-improvement-epoch_659-loss_0.6072-accuracy_0.7972.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6072 - accuracy: 0.7972 - val_loss: 0.2206 - val_accuracy: 0.9462\n",
      "Epoch 660/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6117 - accuracy: 0.7966\n",
      "Epoch 00660: loss did not improve from 0.60723\n",
      "555/555 [==============================] - 29s 51ms/step - loss: 0.6116 - accuracy: 0.7966 - val_loss: 0.2184 - val_accuracy: 0.9444\n",
      "Epoch 661/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.6071 - accuracy: 0.7961 ETA: 1s - l\n",
      "Epoch 00661: loss improved from 0.60723 to 0.60692, saving model to ./Checkpoints\\weights-improvement-epoch_661-loss_0.6069-accuracy_0.7961.hdf5\n",
      "555/555 [==============================] - 27s 48ms/step - loss: 0.6069 - accuracy: 0.7961 - val_loss: 0.2195 - val_accuracy: 0.9456\n",
      "Epoch 662/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6109 - accuracy: 0.7948\n",
      "Epoch 00662: loss did not improve from 0.60692\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6109 - accuracy: 0.7948 - val_loss: 0.2193 - val_accuracy: 0.9461\n",
      "Epoch 663/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6119 - accuracy: 0.7963\n",
      "Epoch 00663: loss did not improve from 0.60692\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6120 - accuracy: 0.7963 - val_loss: 0.2188 - val_accuracy: 0.9454\n",
      "Epoch 664/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6126 - accuracy: 0.7962\n",
      "Epoch 00664: loss did not improve from 0.60692\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6126 - accuracy: 0.7962 - val_loss: 0.2285 - val_accuracy: 0.9412\n",
      "Epoch 665/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6167 - accuracy: 0.7946\n",
      "Epoch 00665: loss did not improve from 0.60692\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6167 - accuracy: 0.7946 - val_loss: 0.2282 - val_accuracy: 0.9438\n",
      "Epoch 666/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6100 - accuracy: 0.7962\n",
      "Epoch 00666: loss did not improve from 0.60692\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6100 - accuracy: 0.7962 - val_loss: 0.2122 - val_accuracy: 0.9478\n",
      "Epoch 667/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6081 - accuracy: 0.7960\n",
      "Epoch 00667: loss did not improve from 0.60692\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6081 - accuracy: 0.7960 - val_loss: 0.2153 - val_accuracy: 0.9458\n",
      "Epoch 668/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.7977\n",
      "Epoch 00668: loss improved from 0.60692 to 0.60548, saving model to ./Checkpoints\\weights-improvement-epoch_668-loss_0.6055-accuracy_0.7977.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6055 - accuracy: 0.7977 - val_loss: 0.2130 - val_accuracy: 0.9468\n",
      "Epoch 669/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.7944\n",
      "Epoch 00669: loss did not improve from 0.60548\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6072 - accuracy: 0.7944 - val_loss: 0.2140 - val_accuracy: 0.9470\n",
      "Epoch 670/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.7985\n",
      "Epoch 00670: loss improved from 0.60548 to 0.60249, saving model to ./Checkpoints\\weights-improvement-epoch_670-loss_0.6025-accuracy_0.7985.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6025 - accuracy: 0.7985 - val_loss: 0.2154 - val_accuracy: 0.9468\n",
      "Epoch 671/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7221 - accuracy: 0.7654\n",
      "Epoch 00671: loss did not improve from 0.60249\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.7221 - accuracy: 0.7654 - val_loss: 0.2898 - val_accuracy: 0.9243\n",
      "Epoch 672/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6469 - accuracy: 0.7854\n",
      "Epoch 00672: loss did not improve from 0.60249\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6469 - accuracy: 0.7854 - val_loss: 0.2269 - val_accuracy: 0.9441\n",
      "Epoch 673/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.7962\n",
      "Epoch 00673: loss did not improve from 0.60249\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6065 - accuracy: 0.7962 - val_loss: 0.2172 - val_accuracy: 0.9484\n",
      "Epoch 674/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6036 - accuracy: 0.7981\n",
      "Epoch 00674: loss did not improve from 0.60249\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6036 - accuracy: 0.7981 - val_loss: 0.2133 - val_accuracy: 0.9478\n",
      "Epoch 675/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5997 - accuracy: 0.7982\n",
      "Epoch 00675: loss improved from 0.60249 to 0.59969, saving model to ./Checkpoints\\weights-improvement-epoch_675-loss_0.5997-accuracy_0.7982.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5997 - accuracy: 0.7982 - val_loss: 0.2146 - val_accuracy: 0.9469\n",
      "Epoch 676/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5992 - accuracy: 0.7982\n",
      "Epoch 00676: loss improved from 0.59969 to 0.59921, saving model to ./Checkpoints\\weights-improvement-epoch_676-loss_0.5992-accuracy_0.7982.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5992 - accuracy: 0.7982 - val_loss: 0.2126 - val_accuracy: 0.9478\n",
      "Epoch 677/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6052 - accuracy: 0.7969\n",
      "Epoch 00677: loss did not improve from 0.59921\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6052 - accuracy: 0.7969 - val_loss: 0.2109 - val_accuracy: 0.9479\n",
      "Epoch 678/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5985 - accuracy: 0.7977\n",
      "Epoch 00678: loss improved from 0.59921 to 0.59852, saving model to ./Checkpoints\\weights-improvement-epoch_678-loss_0.5985-accuracy_0.7977.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5985 - accuracy: 0.7977 - val_loss: 0.2123 - val_accuracy: 0.9475\n",
      "Epoch 679/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.7968\n",
      "Epoch 00679: loss did not improve from 0.59852\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6065 - accuracy: 0.7968 - val_loss: 0.2144 - val_accuracy: 0.9464\n",
      "Epoch 680/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6001 - accuracy: 0.7974\n",
      "Epoch 00680: loss did not improve from 0.59852\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.6001 - accuracy: 0.7974 - val_loss: 0.2129 - val_accuracy: 0.9462\n",
      "Epoch 681/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6082 - accuracy: 0.7964\n",
      "Epoch 00681: loss did not improve from 0.59852\n",
      "555/555 [==============================] - 30s 53ms/step - loss: 0.6082 - accuracy: 0.7964 - val_loss: 0.2170 - val_accuracy: 0.9457\n",
      "Epoch 682/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5914 - accuracy: 0.8037\n",
      "Epoch 00682: loss improved from 0.59852 to 0.59141, saving model to ./Checkpoints\\weights-improvement-epoch_682-loss_0.5914-accuracy_0.8037.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5914 - accuracy: 0.8037 - val_loss: 0.2118 - val_accuracy: 0.9471\n",
      "Epoch 683/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6041 - accuracy: 0.7990\n",
      "Epoch 00683: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.6041 - accuracy: 0.7989 - val_loss: 0.2109 - val_accuracy: 0.9462\n",
      "Epoch 684/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6082 - accuracy: 0.7955\n",
      "Epoch 00684: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.6083 - accuracy: 0.7955 - val_loss: 0.2086 - val_accuracy: 0.9481\n",
      "Epoch 685/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6094 - accuracy: 0.7951\n",
      "Epoch 00685: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.6094 - accuracy: 0.7951 - val_loss: 0.2127 - val_accuracy: 0.9480\n",
      "Epoch 686/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.7988\n",
      "Epoch 00686: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.5984 - accuracy: 0.7988 - val_loss: 0.2085 - val_accuracy: 0.9475\n",
      "Epoch 687/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6025 - accuracy: 0.7976\n",
      "Epoch 00687: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.6024 - accuracy: 0.7976 - val_loss: 0.2203 - val_accuracy: 0.9439\n",
      "Epoch 688/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5969 - accuracy: 0.7983\n",
      "Epoch 00688: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.5969 - accuracy: 0.7983 - val_loss: 0.2127 - val_accuracy: 0.9462\n",
      "Epoch 689/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5986 - accuracy: 0.7998\n",
      "Epoch 00689: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5986 - accuracy: 0.7998 - val_loss: 0.2098 - val_accuracy: 0.9480\n",
      "Epoch 690/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5961 - accuracy: 0.8027\n",
      "Epoch 00690: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5961 - accuracy: 0.8027 - val_loss: 0.2057 - val_accuracy: 0.9501\n",
      "Epoch 691/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6007 - accuracy: 0.7994\n",
      "Epoch 00691: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6007 - accuracy: 0.7994 - val_loss: 0.2040 - val_accuracy: 0.9492\n",
      "Epoch 692/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.8013\n",
      "Epoch 00692: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5919 - accuracy: 0.8013 - val_loss: 0.2097 - val_accuracy: 0.9467\n",
      "Epoch 693/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5991 - accuracy: 0.7984\n",
      "Epoch 00693: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 0.5991 - accuracy: 0.7984 - val_loss: 0.2094 - val_accuracy: 0.9465\n",
      "Epoch 694/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.7967\n",
      "Epoch 00694: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 0.6028 - accuracy: 0.7967 - val_loss: 0.2080 - val_accuracy: 0.9475\n",
      "Epoch 695/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5970 - accuracy: 0.7989\n",
      "Epoch 00695: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5970 - accuracy: 0.7989 - val_loss: 0.2034 - val_accuracy: 0.9499\n",
      "Epoch 696/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6040 - accuracy: 0.7978\n",
      "Epoch 00696: loss did not improve from 0.59141\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.6040 - accuracy: 0.7978 - val_loss: 0.2031 - val_accuracy: 0.9501\n",
      "Epoch 697/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5829 - accuracy: 0.8054\n",
      "Epoch 00697: loss improved from 0.59141 to 0.58291, saving model to ./Checkpoints\\weights-improvement-epoch_697-loss_0.5829-accuracy_0.8054.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5829 - accuracy: 0.8054 - val_loss: 0.2072 - val_accuracy: 0.9479\n",
      "Epoch 698/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.8042\n",
      "Epoch 00698: loss did not improve from 0.58291\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5883 - accuracy: 0.8042 - val_loss: 0.2029 - val_accuracy: 0.9500\n",
      "Epoch 699/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5960 - accuracy: 0.8004\n",
      "Epoch 00699: loss did not improve from 0.58291\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5960 - accuracy: 0.8004 - val_loss: 0.2033 - val_accuracy: 0.9503\n",
      "Epoch 700/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5977 - accuracy: 0.8012\n",
      "Epoch 00700: loss did not improve from 0.58291\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5977 - accuracy: 0.8012 - val_loss: 0.2110 - val_accuracy: 0.9467\n",
      "Epoch 701/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.8037\n",
      "Epoch 00701: loss did not improve from 0.58291\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5883 - accuracy: 0.8037 - val_loss: 0.2023 - val_accuracy: 0.9499\n",
      "Epoch 702/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.8060\n",
      "Epoch 00702: loss improved from 0.58291 to 0.58103, saving model to ./Checkpoints\\weights-improvement-epoch_702-loss_0.5810-accuracy_0.8060.hdf5\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 0.5810 - accuracy: 0.8060 - val_loss: 0.1993 - val_accuracy: 0.9510\n",
      "Epoch 703/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.8043\n",
      "Epoch 00703: loss did not improve from 0.58103\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5881 - accuracy: 0.8043 - val_loss: 0.2046 - val_accuracy: 0.9509\n",
      "Epoch 704/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5930 - accuracy: 0.8019\n",
      "Epoch 00704: loss did not improve from 0.58103\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5930 - accuracy: 0.8019 - val_loss: 0.1992 - val_accuracy: 0.9510\n",
      "Epoch 705/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.5871 - accuracy: 0.8035\n",
      "Epoch 00705: loss did not improve from 0.58103\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5873 - accuracy: 0.8034 - val_loss: 0.1987 - val_accuracy: 0.9512\n",
      "Epoch 706/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.8039\n",
      "Epoch 00706: loss did not improve from 0.58103\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5939 - accuracy: 0.8039 - val_loss: 0.2038 - val_accuracy: 0.9507\n",
      "Epoch 707/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/555 [==============================] - ETA: 0s - loss: 0.5832 - accuracy: 0.8031\n",
      "Epoch 00707: loss did not improve from 0.58103\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5832 - accuracy: 0.8031 - val_loss: 0.1966 - val_accuracy: 0.9521\n",
      "Epoch 708/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5907 - accuracy: 0.8015\n",
      "Epoch 00708: loss did not improve from 0.58103\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5907 - accuracy: 0.8015 - val_loss: 0.1996 - val_accuracy: 0.9512\n",
      "Epoch 709/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5856 - accuracy: 0.8032\n",
      "Epoch 00709: loss did not improve from 0.58103\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 0.5856 - accuracy: 0.8032 - val_loss: 0.1987 - val_accuracy: 0.9513\n",
      "Epoch 710/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.8051\n",
      "Epoch 00710: loss improved from 0.58103 to 0.58068, saving model to ./Checkpoints\\weights-improvement-epoch_710-loss_0.5807-accuracy_0.8051.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5807 - accuracy: 0.8051 - val_loss: 0.1926 - val_accuracy: 0.9542\n",
      "Epoch 711/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.8019\n",
      "Epoch 00711: loss did not improve from 0.58068\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5880 - accuracy: 0.8019 - val_loss: 0.1982 - val_accuracy: 0.9524\n",
      "Epoch 712/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5866 - accuracy: 0.8055\n",
      "Epoch 00712: loss did not improve from 0.58068\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5866 - accuracy: 0.8055 - val_loss: 0.1960 - val_accuracy: 0.9525\n",
      "Epoch 713/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5814 - accuracy: 0.8063\n",
      "Epoch 00713: loss did not improve from 0.58068\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5814 - accuracy: 0.8063 - val_loss: 0.1967 - val_accuracy: 0.9523\n",
      "Epoch 714/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5863 - accuracy: 0.8043\n",
      "Epoch 00714: loss did not improve from 0.58068\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5863 - accuracy: 0.8043 - val_loss: 0.2004 - val_accuracy: 0.9513\n",
      "Epoch 715/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5911 - accuracy: 0.8014\n",
      "Epoch 00715: loss did not improve from 0.58068\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5911 - accuracy: 0.8014 - val_loss: 0.2343 - val_accuracy: 0.9361\n",
      "Epoch 716/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5869 - accuracy: 0.8040\n",
      "Epoch 00716: loss did not improve from 0.58068\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 0.5869 - accuracy: 0.8040 - val_loss: 0.1951 - val_accuracy: 0.9541\n",
      "Epoch 717/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.8065\n",
      "Epoch 00717: loss improved from 0.58068 to 0.57760, saving model to ./Checkpoints\\weights-improvement-epoch_717-loss_0.5776-accuracy_0.8065.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5776 - accuracy: 0.8065 - val_loss: 0.1947 - val_accuracy: 0.9532\n",
      "Epoch 718/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 0.5839 - accuracy: 0.8056\n",
      "Epoch 00718: loss did not improve from 0.57760\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5841 - accuracy: 0.8056 - val_loss: 0.1967 - val_accuracy: 0.9522\n",
      "Epoch 719/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.8047\n",
      "Epoch 00719: loss did not improve from 0.57760\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5888 - accuracy: 0.8047 - val_loss: 0.1973 - val_accuracy: 0.9509\n",
      "Epoch 720/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5907 - accuracy: 0.8037\n",
      "Epoch 00720: loss did not improve from 0.57760\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5907 - accuracy: 0.8037 - val_loss: 0.1933 - val_accuracy: 0.9525\n",
      "Epoch 721/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.8081\n",
      "Epoch 00721: loss improved from 0.57760 to 0.57558, saving model to ./Checkpoints\\weights-improvement-epoch_721-loss_0.5756-accuracy_0.8081.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5756 - accuracy: 0.8081 - val_loss: 0.1967 - val_accuracy: 0.9520\n",
      "Epoch 722/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.8030\n",
      "Epoch 00722: loss did not improve from 0.57558\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5853 - accuracy: 0.8030 - val_loss: 0.2042 - val_accuracy: 0.9483\n",
      "Epoch 723/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.8027\n",
      "Epoch 00723: loss did not improve from 0.57558\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5862 - accuracy: 0.8027 - val_loss: 0.1910 - val_accuracy: 0.9541\n",
      "Epoch 724/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5794 - accuracy: 0.8079\n",
      "Epoch 00724: loss did not improve from 0.57558\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5794 - accuracy: 0.8079 - val_loss: 0.1948 - val_accuracy: 0.9530\n",
      "Epoch 725/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5835 - accuracy: 0.8060\n",
      "Epoch 00725: loss did not improve from 0.57558\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5835 - accuracy: 0.8060 - val_loss: 0.1995 - val_accuracy: 0.9506\n",
      "Epoch 726/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5798 - accuracy: 0.8061\n",
      "Epoch 00726: loss did not improve from 0.57558\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5798 - accuracy: 0.8061 - val_loss: 0.1964 - val_accuracy: 0.9515\n",
      "Epoch 727/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.8049\n",
      "Epoch 00727: loss did not improve from 0.57558\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5838 - accuracy: 0.8049 - val_loss: 0.1941 - val_accuracy: 0.9528\n",
      "Epoch 728/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5746 - accuracy: 0.8062\n",
      "Epoch 00728: loss improved from 0.57558 to 0.57464, saving model to ./Checkpoints\\weights-improvement-epoch_728-loss_0.5746-accuracy_0.8062.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5746 - accuracy: 0.8062 - val_loss: 0.1889 - val_accuracy: 0.9537\n",
      "Epoch 729/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5721 - accuracy: 0.8092\n",
      "Epoch 00729: loss improved from 0.57464 to 0.57209, saving model to ./Checkpoints\\weights-improvement-epoch_729-loss_0.5721-accuracy_0.8092.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5721 - accuracy: 0.8092 - val_loss: 0.1955 - val_accuracy: 0.9532\n",
      "Epoch 730/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.8097\n",
      "Epoch 00730: loss improved from 0.57209 to 0.56786, saving model to ./Checkpoints\\weights-improvement-epoch_730-loss_0.5679-accuracy_0.8097.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5679 - accuracy: 0.8097 - val_loss: 0.1939 - val_accuracy: 0.9536\n",
      "Epoch 731/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5903 - accuracy: 0.8038\n",
      "Epoch 00731: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5903 - accuracy: 0.8038 - val_loss: 0.1999 - val_accuracy: 0.9500\n",
      "Epoch 732/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.8034\n",
      "Epoch 00732: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 0.5877 - accuracy: 0.8034 - val_loss: 0.1977 - val_accuracy: 0.9506\n",
      "Epoch 733/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5775 - accuracy: 0.8061\n",
      "Epoch 00733: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5775 - accuracy: 0.8061 - val_loss: 0.1967 - val_accuracy: 0.9521\n",
      "Epoch 734/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5734 - accuracy: 0.8063\n",
      "Epoch 00734: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5734 - accuracy: 0.8063 - val_loss: 0.1861 - val_accuracy: 0.9551\n",
      "Epoch 735/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5983 - accuracy: 0.7996\n",
      "Epoch 00735: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5983 - accuracy: 0.7996 - val_loss: 0.1994 - val_accuracy: 0.9517\n",
      "Epoch 736/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5815 - accuracy: 0.8054\n",
      "Epoch 00736: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5815 - accuracy: 0.8054 - val_loss: 0.1905 - val_accuracy: 0.9549\n",
      "Epoch 737/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5860 - accuracy: 0.8038\n",
      "Epoch 00737: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5860 - accuracy: 0.8038 - val_loss: 0.2227 - val_accuracy: 0.9433\n",
      "Epoch 738/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5811 - accuracy: 0.8071\n",
      "Epoch 00738: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5811 - accuracy: 0.8071 - val_loss: 0.1924 - val_accuracy: 0.9536\n",
      "Epoch 739/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.7991\n",
      "Epoch 00739: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.6078 - accuracy: 0.7991 - val_loss: 0.1941 - val_accuracy: 0.9539\n",
      "Epoch 740/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5733 - accuracy: 0.8103\n",
      "Epoch 00740: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5733 - accuracy: 0.8103 - val_loss: 0.1860 - val_accuracy: 0.9569\n",
      "Epoch 741/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5685 - accuracy: 0.8099\n",
      "Epoch 00741: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5685 - accuracy: 0.8099 - val_loss: 0.1908 - val_accuracy: 0.9540\n",
      "Epoch 742/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5710 - accuracy: 0.8076\n",
      "Epoch 00742: loss did not improve from 0.56786\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5710 - accuracy: 0.8076 - val_loss: 0.1895 - val_accuracy: 0.9539\n",
      "Epoch 743/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5668 - accuracy: 0.8118\n",
      "Epoch 00743: loss improved from 0.56786 to 0.56681, saving model to ./Checkpoints\\weights-improvement-epoch_743-loss_0.5668-accuracy_0.8118.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5668 - accuracy: 0.8118 - val_loss: 0.1900 - val_accuracy: 0.9530\n",
      "Epoch 744/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5782 - accuracy: 0.8069\n",
      "Epoch 00744: loss did not improve from 0.56681\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5782 - accuracy: 0.8069 - val_loss: 0.1923 - val_accuracy: 0.9528\n",
      "Epoch 745/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7031 - accuracy: 0.7704\n",
      "Epoch 00745: loss did not improve from 0.56681\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.7031 - accuracy: 0.7704 - val_loss: 0.2323 - val_accuracy: 0.9411\n",
      "Epoch 746/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.8023\n",
      "Epoch 00746: loss did not improve from 0.56681\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5965 - accuracy: 0.8023 - val_loss: 0.2016 - val_accuracy: 0.9508\n",
      "Epoch 747/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5721 - accuracy: 0.8075\n",
      "Epoch 00747: loss did not improve from 0.56681\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5721 - accuracy: 0.8075 - val_loss: 0.1858 - val_accuracy: 0.9553\n",
      "Epoch 748/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6329 - accuracy: 0.7895\n",
      "Epoch 00748: loss did not improve from 0.56681\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6329 - accuracy: 0.7895 - val_loss: 0.2015 - val_accuracy: 0.9514\n",
      "Epoch 749/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5742 - accuracy: 0.8076\n",
      "Epoch 00749: loss did not improve from 0.56681\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5742 - accuracy: 0.8076 - val_loss: 0.1826 - val_accuracy: 0.9569\n",
      "Epoch 750/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5536 - accuracy: 0.8156\n",
      "Epoch 00750: loss improved from 0.56681 to 0.55358, saving model to ./Checkpoints\\weights-improvement-epoch_750-loss_0.5536-accuracy_0.8156.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5536 - accuracy: 0.8156 - val_loss: 0.1812 - val_accuracy: 0.9568\n",
      "Epoch 751/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6290 - accuracy: 0.7911\n",
      "Epoch 00751: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6290 - accuracy: 0.7911 - val_loss: 0.2094 - val_accuracy: 0.9483\n",
      "Epoch 752/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.8099\n",
      "Epoch 00752: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5673 - accuracy: 0.8099 - val_loss: 0.1840 - val_accuracy: 0.9563\n",
      "Epoch 753/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.8119\n",
      "Epoch 00753: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5621 - accuracy: 0.8119 - val_loss: 0.1896 - val_accuracy: 0.9549\n",
      "Epoch 754/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.8111\n",
      "Epoch 00754: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5644 - accuracy: 0.8111 - val_loss: 0.1822 - val_accuracy: 0.9564\n",
      "Epoch 755/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5732 - accuracy: 0.8087\n",
      "Epoch 00755: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5732 - accuracy: 0.8087 - val_loss: 0.1758 - val_accuracy: 0.9593\n",
      "Epoch 756/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5577 - accuracy: 0.8134\n",
      "Epoch 00756: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5577 - accuracy: 0.8134 - val_loss: 0.1797 - val_accuracy: 0.9570\n",
      "Epoch 757/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.8115\n",
      "Epoch 00757: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5637 - accuracy: 0.8115 - val_loss: 0.1805 - val_accuracy: 0.9571\n",
      "Epoch 758/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5615 - accuracy: 0.8115\n",
      "Epoch 00758: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5615 - accuracy: 0.8115 - val_loss: 0.1773 - val_accuracy: 0.9567\n",
      "Epoch 759/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5728 - accuracy: 0.8087 ETA: 0s - los\n",
      "Epoch 00759: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5728 - accuracy: 0.8087 - val_loss: 0.1825 - val_accuracy: 0.9563\n",
      "Epoch 760/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5622 - accuracy: 0.8105\n",
      "Epoch 00760: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5622 - accuracy: 0.8105 - val_loss: 0.1778 - val_accuracy: 0.9580\n",
      "Epoch 761/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5663 - accuracy: 0.8086\n",
      "Epoch 00761: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5663 - accuracy: 0.8086 - val_loss: 0.1865 - val_accuracy: 0.9539\n",
      "Epoch 762/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.8086\n",
      "Epoch 00762: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5716 - accuracy: 0.8086 - val_loss: 0.1859 - val_accuracy: 0.9564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 763/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.8084\n",
      "Epoch 00763: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5696 - accuracy: 0.8084 - val_loss: 0.1819 - val_accuracy: 0.9567\n",
      "Epoch 764/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5568 - accuracy: 0.8123\n",
      "Epoch 00764: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5568 - accuracy: 0.8123 - val_loss: 0.1788 - val_accuracy: 0.9583\n",
      "Epoch 765/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.8146\n",
      "Epoch 00765: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5617 - accuracy: 0.8146 - val_loss: 0.1840 - val_accuracy: 0.9571\n",
      "Epoch 766/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5649 - accuracy: 0.8107\n",
      "Epoch 00766: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5649 - accuracy: 0.8107 - val_loss: 0.1833 - val_accuracy: 0.9557\n",
      "Epoch 767/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5652 - accuracy: 0.8105\n",
      "Epoch 00767: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5652 - accuracy: 0.8105 - val_loss: 0.1778 - val_accuracy: 0.9568\n",
      "Epoch 768/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5585 - accuracy: 0.8118 ETA: 1s - l\n",
      "Epoch 00768: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5585 - accuracy: 0.8118 - val_loss: 0.1781 - val_accuracy: 0.9568\n",
      "Epoch 769/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.8104\n",
      "Epoch 00769: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5628 - accuracy: 0.8104 - val_loss: 0.1813 - val_accuracy: 0.9576\n",
      "Epoch 770/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5641 - accuracy: 0.8111\n",
      "Epoch 00770: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5641 - accuracy: 0.8111 - val_loss: 0.1809 - val_accuracy: 0.9569\n",
      "Epoch 771/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.8142\n",
      "Epoch 00771: loss did not improve from 0.55358\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5554 - accuracy: 0.8142 - val_loss: 0.1768 - val_accuracy: 0.9562\n",
      "Epoch 772/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5499 - accuracy: 0.8166\n",
      "Epoch 00772: loss improved from 0.55358 to 0.54990, saving model to ./Checkpoints\\weights-improvement-epoch_772-loss_0.5499-accuracy_0.8166.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5499 - accuracy: 0.8166 - val_loss: 0.1770 - val_accuracy: 0.9572\n",
      "Epoch 773/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5574 - accuracy: 0.8135\n",
      "Epoch 00773: loss did not improve from 0.54990\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5574 - accuracy: 0.8135 - val_loss: 0.1820 - val_accuracy: 0.9557\n",
      "Epoch 774/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5662 - accuracy: 0.8118\n",
      "Epoch 00774: loss did not improve from 0.54990\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5662 - accuracy: 0.8118 - val_loss: 0.1840 - val_accuracy: 0.9546\n",
      "Epoch 775/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.8126\n",
      "Epoch 00775: loss did not improve from 0.54990\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5582 - accuracy: 0.8126 - val_loss: 0.1783 - val_accuracy: 0.9589\n",
      "Epoch 776/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5583 - accuracy: 0.8140\n",
      "Epoch 00776: loss did not improve from 0.54990\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5583 - accuracy: 0.8140 - val_loss: 0.1793 - val_accuracy: 0.9573\n",
      "Epoch 777/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.8146\n",
      "Epoch 00777: loss did not improve from 0.54990\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5597 - accuracy: 0.8146 - val_loss: 0.1803 - val_accuracy: 0.9571\n",
      "Epoch 778/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5511 - accuracy: 0.8159\n",
      "Epoch 00778: loss did not improve from 0.54990\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5511 - accuracy: 0.8159 - val_loss: 0.1767 - val_accuracy: 0.9579\n",
      "Epoch 779/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5500 - accuracy: 0.8167 ETA: 0s - loss:\n",
      "Epoch 00779: loss did not improve from 0.54990\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5500 - accuracy: 0.8167 - val_loss: 0.1724 - val_accuracy: 0.9585\n",
      "Epoch 780/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.8145\n",
      "Epoch 00780: loss did not improve from 0.54990\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5572 - accuracy: 0.8145 - val_loss: 0.1750 - val_accuracy: 0.9577\n",
      "Epoch 781/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5491 - accuracy: 0.8154\n",
      "Epoch 00781: loss improved from 0.54990 to 0.54916, saving model to ./Checkpoints\\weights-improvement-epoch_781-loss_0.5492-accuracy_0.8154.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5492 - accuracy: 0.8154 - val_loss: 0.1801 - val_accuracy: 0.9559\n",
      "Epoch 782/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5560 - accuracy: 0.8157\n",
      "Epoch 00782: loss did not improve from 0.54916\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5560 - accuracy: 0.8157 - val_loss: 0.1721 - val_accuracy: 0.9584\n",
      "Epoch 783/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5471 - accuracy: 0.8157\n",
      "Epoch 00783: loss improved from 0.54916 to 0.54711, saving model to ./Checkpoints\\weights-improvement-epoch_783-loss_0.5471-accuracy_0.8157.hdf5\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.5471 - accuracy: 0.8157 - val_loss: 0.1748 - val_accuracy: 0.9589\n",
      "Epoch 784/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.8144\n",
      "Epoch 00784: loss did not improve from 0.54711\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5554 - accuracy: 0.8144 - val_loss: 0.1722 - val_accuracy: 0.9590\n",
      "Epoch 785/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.8148\n",
      "Epoch 00785: loss did not improve from 0.54711\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5551 - accuracy: 0.8148 - val_loss: 0.1722 - val_accuracy: 0.9589\n",
      "Epoch 786/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.8116\n",
      "Epoch 00786: loss did not improve from 0.54711\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5609 - accuracy: 0.8116 - val_loss: 0.1793 - val_accuracy: 0.9574\n",
      "Epoch 787/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6101 - accuracy: 0.7984\n",
      "Epoch 00787: loss did not improve from 0.54711\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.6101 - accuracy: 0.7984 - val_loss: 0.1921 - val_accuracy: 0.9524\n",
      "Epoch 788/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.8126\n",
      "Epoch 00788: loss did not improve from 0.54711\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5623 - accuracy: 0.8126 - val_loss: 0.1716 - val_accuracy: 0.9611\n",
      "Epoch 789/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5441 - accuracy: 0.8180\n",
      "Epoch 00789: loss improved from 0.54711 to 0.54411, saving model to ./Checkpoints\\weights-improvement-epoch_789-loss_0.5441-accuracy_0.8180.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5441 - accuracy: 0.8180 - val_loss: 0.1639 - val_accuracy: 0.9630\n",
      "Epoch 790/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5434 - accuracy: 0.8182\n",
      "Epoch 00790: loss improved from 0.54411 to 0.54341, saving model to ./Checkpoints\\weights-improvement-epoch_790-loss_0.5434-accuracy_0.8182.hdf5\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5434 - accuracy: 0.8182 - val_loss: 0.1729 - val_accuracy: 0.9589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 791/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.8155\n",
      "Epoch 00791: loss did not improve from 0.54341\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5531 - accuracy: 0.8155 - val_loss: 0.1715 - val_accuracy: 0.9588\n",
      "Epoch 792/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5441 - accuracy: 0.8171\n",
      "Epoch 00792: loss did not improve from 0.54341\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5440 - accuracy: 0.8171 - val_loss: 0.1715 - val_accuracy: 0.9590\n",
      "Epoch 793/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5479 - accuracy: 0.8168\n",
      "Epoch 00793: loss did not improve from 0.54341\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5479 - accuracy: 0.8168 - val_loss: 0.1669 - val_accuracy: 0.9623\n",
      "Epoch 794/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.8172\n",
      "Epoch 00794: loss improved from 0.54341 to 0.53893, saving model to ./Checkpoints\\weights-improvement-epoch_794-loss_0.5389-accuracy_0.8172.hdf5\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5389 - accuracy: 0.8172 - val_loss: 0.1708 - val_accuracy: 0.9589\n",
      "Epoch 795/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.8176\n",
      "Epoch 00795: loss did not improve from 0.53893\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 0.5437 - accuracy: 0.8176 - val_loss: 0.1627 - val_accuracy: 0.9623\n",
      "Epoch 796/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5484 - accuracy: 0.8170\n",
      "Epoch 00796: loss did not improve from 0.53893\n",
      "555/555 [==============================] - 32s 59ms/step - loss: 0.5484 - accuracy: 0.8170 - val_loss: 0.1715 - val_accuracy: 0.9617\n",
      "Epoch 797/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5490 - accuracy: 0.8164\n",
      "Epoch 00797: loss did not improve from 0.53893\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 0.5490 - accuracy: 0.8164 - val_loss: 0.1693 - val_accuracy: 0.9602\n",
      "Epoch 798/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5397 - accuracy: 0.8186\n",
      "Epoch 00798: loss did not improve from 0.53893\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 0.5396 - accuracy: 0.8186 - val_loss: 0.1674 - val_accuracy: 0.9607\n",
      "Epoch 799/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.8117\n",
      "Epoch 00799: loss did not improve from 0.53893\n",
      "555/555 [==============================] - 30s 53ms/step - loss: 0.5588 - accuracy: 0.8117 - val_loss: 0.1731 - val_accuracy: 0.9590\n",
      "Epoch 800/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5459 - accuracy: 0.8178\n",
      "Epoch 00800: loss did not improve from 0.53893\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5459 - accuracy: 0.8178 - val_loss: 0.1671 - val_accuracy: 0.9628\n",
      "Epoch 801/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.8169\n",
      "Epoch 00801: loss did not improve from 0.53893\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 0.5466 - accuracy: 0.8169 - val_loss: 0.1629 - val_accuracy: 0.9619\n",
      "Epoch 802/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5496 - accuracy: 0.8153 ETA: 2s - loss: 0.5484 - accu\n",
      "Epoch 00802: loss did not improve from 0.53893\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5495 - accuracy: 0.8153 - val_loss: 0.1682 - val_accuracy: 0.9596\n",
      "Epoch 803/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5494 - accuracy: 0.8169\n",
      "Epoch 00803: loss did not improve from 0.53893\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5494 - accuracy: 0.8169 - val_loss: 0.1647 - val_accuracy: 0.9619\n",
      "Epoch 804/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5354 - accuracy: 0.8200\n",
      "Epoch 00804: loss improved from 0.53893 to 0.53536, saving model to ./Checkpoints\\weights-improvement-epoch_804-loss_0.5354-accuracy_0.8200.hdf5\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 0.5354 - accuracy: 0.8200 - val_loss: 0.1658 - val_accuracy: 0.9615\n",
      "Epoch 805/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5455 - accuracy: 0.8192\n",
      "Epoch 00805: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.5455 - accuracy: 0.8192 - val_loss: 0.1693 - val_accuracy: 0.9600\n",
      "Epoch 806/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5498 - accuracy: 0.8173 ETA: 1s\n",
      "Epoch 00806: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 0.5497 - accuracy: 0.8173 - val_loss: 0.1668 - val_accuracy: 0.9607\n",
      "Epoch 807/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5427 - accuracy: 0.8180\n",
      "Epoch 00807: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 0.5426 - accuracy: 0.8180 - val_loss: 0.1611 - val_accuracy: 0.9622\n",
      "Epoch 808/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5384 - accuracy: 0.8198\n",
      "Epoch 00808: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 0.5385 - accuracy: 0.8199 - val_loss: 0.1777 - val_accuracy: 0.9571\n",
      "Epoch 809/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5428 - accuracy: 0.8187\n",
      "Epoch 00809: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 0.5428 - accuracy: 0.8187 - val_loss: 0.1670 - val_accuracy: 0.9616\n",
      "Epoch 810/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5381 - accuracy: 0.8212\n",
      "Epoch 00810: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 30s 53ms/step - loss: 0.5381 - accuracy: 0.8212 - val_loss: 0.1674 - val_accuracy: 0.9601\n",
      "Epoch 811/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5458 - accuracy: 0.8164 ETA: 0s - loss: 0.5455 \n",
      "Epoch 00811: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 0.5458 - accuracy: 0.8164 - val_loss: 0.1653 - val_accuracy: 0.9621\n",
      "Epoch 812/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5386 - accuracy: 0.8210\n",
      "Epoch 00812: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5386 - accuracy: 0.8210 - val_loss: 0.1646 - val_accuracy: 0.9626\n",
      "Epoch 813/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5431 - accuracy: 0.8188\n",
      "Epoch 00813: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5430 - accuracy: 0.8188 - val_loss: 0.1609 - val_accuracy: 0.9635\n",
      "Epoch 814/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.8199\n",
      "Epoch 00814: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 0.5397 - accuracy: 0.8199 - val_loss: 0.1658 - val_accuracy: 0.9602\n",
      "Epoch 815/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5430 - accuracy: 0.8185\n",
      "Epoch 00815: loss did not improve from 0.53536\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 0.5430 - accuracy: 0.8185 - val_loss: 0.1612 - val_accuracy: 0.9637\n",
      "Epoch 816/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5340 - accuracy: 0.8206\n",
      "Epoch 00816: loss improved from 0.53536 to 0.53397, saving model to ./Checkpoints\\weights-improvement-epoch_816-loss_0.5340-accuracy_0.8206.hdf5\n",
      "555/555 [==============================] - 42s 75ms/step - loss: 0.5340 - accuracy: 0.8206 - val_loss: 0.1582 - val_accuracy: 0.9633\n",
      "Epoch 817/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5396 - accuracy: 0.8210\n",
      "Epoch 00817: loss did not improve from 0.53397\n",
      "555/555 [==============================] - 46s 84ms/step - loss: 0.5396 - accuracy: 0.8210 - val_loss: 0.1627 - val_accuracy: 0.9623\n",
      "Epoch 818/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.8206\n",
      "Epoch 00818: loss did not improve from 0.53397\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 0.5352 - accuracy: 0.8206 - val_loss: 0.1545 - val_accuracy: 0.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 819/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5416 - accuracy: 0.8187\n",
      "Epoch 00819: loss did not improve from 0.53397\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 0.5417 - accuracy: 0.8187 - val_loss: 0.1628 - val_accuracy: 0.9620\n",
      "Epoch 820/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.8207\n",
      "Epoch 00820: loss did not improve from 0.53397\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 0.5397 - accuracy: 0.8207 - val_loss: 0.1612 - val_accuracy: 0.9619\n",
      "Epoch 821/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5314 - accuracy: 0.8224\n",
      "Epoch 00821: loss improved from 0.53397 to 0.53142, saving model to ./Checkpoints\\weights-improvement-epoch_821-loss_0.5314-accuracy_0.8224.hdf5\n",
      "555/555 [==============================] - 47s 84ms/step - loss: 0.5314 - accuracy: 0.8224 - val_loss: 0.1566 - val_accuracy: 0.9642\n",
      "Epoch 822/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5318 - accuracy: 0.8210\n",
      "Epoch 00822: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 40s 71ms/step - loss: 0.5320 - accuracy: 0.8209 - val_loss: 0.1538 - val_accuracy: 0.9648\n",
      "Epoch 823/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5358 - accuracy: 0.8185\n",
      "Epoch 00823: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 40s 72ms/step - loss: 0.5358 - accuracy: 0.8185 - val_loss: 0.1602 - val_accuracy: 0.9630\n",
      "Epoch 824/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5461 - accuracy: 0.8184\n",
      "Epoch 00824: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 0.5460 - accuracy: 0.8184 - val_loss: 0.1573 - val_accuracy: 0.9647\n",
      "Epoch 825/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5351 - accuracy: 0.8216\n",
      "Epoch 00825: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 41s 74ms/step - loss: 0.5350 - accuracy: 0.8216 - val_loss: 0.1634 - val_accuracy: 0.9605\n",
      "Epoch 826/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5509 - accuracy: 0.8161\n",
      "Epoch 00826: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 0.5511 - accuracy: 0.8161 - val_loss: 0.1749 - val_accuracy: 0.9584\n",
      "Epoch 827/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5875 - accuracy: 0.8049\n",
      "Epoch 00827: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 40s 72ms/step - loss: 0.5875 - accuracy: 0.8049 - val_loss: 0.1951 - val_accuracy: 0.9531\n",
      "Epoch 828/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.8166 - accuracy: 0.7404\n",
      "Epoch 00828: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 40s 73ms/step - loss: 0.8166 - accuracy: 0.7404 - val_loss: 0.4821 - val_accuracy: 0.8528\n",
      "Epoch 829/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.7728\n",
      "Epoch 00829: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 0.6936 - accuracy: 0.7728 - val_loss: 0.2099 - val_accuracy: 0.9506\n",
      "Epoch 830/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.7898 - accuracy: 0.7489\n",
      "Epoch 00830: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 0.7898 - accuracy: 0.7489 - val_loss: 0.2557 - val_accuracy: 0.9336\n",
      "Epoch 831/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.7913 - accuracy: 0.7445\n",
      "Epoch 00831: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 0.7912 - accuracy: 0.7445 - val_loss: 0.3122 - val_accuracy: 0.9161\n",
      "Epoch 832/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6802 - accuracy: 0.7751\n",
      "Epoch 00832: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 40s 71ms/step - loss: 0.6801 - accuracy: 0.7751 - val_loss: 0.2447 - val_accuracy: 0.9388\n",
      "Epoch 833/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.7934\n",
      "Epoch 00833: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 41s 73ms/step - loss: 0.6213 - accuracy: 0.7934 - val_loss: 0.2140 - val_accuracy: 0.9479\n",
      "Epoch 834/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6096 - accuracy: 0.7951\n",
      "Epoch 00834: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 40s 72ms/step - loss: 0.6097 - accuracy: 0.7951 - val_loss: 0.2376 - val_accuracy: 0.9401\n",
      "Epoch 835/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6124 - accuracy: 0.79 - ETA: 0s - loss: 0.6121 - accuracy: 0.7960\n",
      "Epoch 00835: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 39s 71ms/step - loss: 0.6121 - accuracy: 0.7960 - val_loss: 0.1984 - val_accuracy: 0.9531\n",
      "Epoch 836/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5946 - accuracy: 0.8010\n",
      "Epoch 00836: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 0.5946 - accuracy: 0.8010 - val_loss: 0.2246 - val_accuracy: 0.9426\n",
      "Epoch 837/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.6335 - accuracy: 0.7882\n",
      "Epoch 00837: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 0.6334 - accuracy: 0.7882 - val_loss: 0.2049 - val_accuracy: 0.9513\n",
      "Epoch 838/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5854 - accuracy: 0.8025\n",
      "Epoch 00838: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 45s 82ms/step - loss: 0.5854 - accuracy: 0.8025 - val_loss: 0.1972 - val_accuracy: 0.9531\n",
      "Epoch 839/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5734 - accuracy: 0.8081\n",
      "Epoch 00839: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 43s 78ms/step - loss: 0.5734 - accuracy: 0.8081 - val_loss: 0.1855 - val_accuracy: 0.9564\n",
      "Epoch 840/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5817 - accuracy: 0.8064\n",
      "Epoch 00840: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.5817 - accuracy: 0.8064 - val_loss: 0.1830 - val_accuracy: 0.9587\n",
      "Epoch 841/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.8096 ETA: 0s - loss: 0.5631 - accu - ETA: 0s - loss: 0.563\n",
      "Epoch 00841: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.5642 - accuracy: 0.8096 - val_loss: 0.1787 - val_accuracy: 0.9591\n",
      "Epoch 842/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5546 - accuracy: 0.8155 ETA\n",
      "Epoch 00842: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.5545 - accuracy: 0.8156 - val_loss: 0.1739 - val_accuracy: 0.9599\n",
      "Epoch 843/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5511 - accuracy: 0.8154\n",
      "Epoch 00843: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 0.5512 - accuracy: 0.8154 - val_loss: 0.1722 - val_accuracy: 0.9616\n",
      "Epoch 844/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5477 - accuracy: 0.8160 ETA: 2s - loss: 0\n",
      "Epoch 00844: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 45s 82ms/step - loss: 0.5477 - accuracy: 0.8160 - val_loss: 0.1689 - val_accuracy: 0.9619\n",
      "Epoch 845/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5467 - accuracy: 0.8153\n",
      "Epoch 00845: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 41s 73ms/step - loss: 0.5467 - accuracy: 0.8153 - val_loss: 0.1761 - val_accuracy: 0.9580\n",
      "Epoch 846/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5479 - accuracy: 0.8175\n",
      "Epoch 00846: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 38s 69ms/step - loss: 0.5479 - accuracy: 0.8175 - val_loss: 0.1660 - val_accuracy: 0.9614\n",
      "Epoch 847/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5458 - accuracy: 0.8171\n",
      "Epoch 00847: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 39s 70ms/step - loss: 0.5458 - accuracy: 0.8171 - val_loss: 0.1698 - val_accuracy: 0.9600\n",
      "Epoch 848/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5509 - accuracy: 0.8151\n",
      "Epoch 00848: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 41s 74ms/step - loss: 0.5509 - accuracy: 0.8151 - val_loss: 0.1683 - val_accuracy: 0.9617\n",
      "Epoch 849/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5536 - accuracy: 0.8165\n",
      "Epoch 00849: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 42s 75ms/step - loss: 0.5536 - accuracy: 0.8165 - val_loss: 0.1665 - val_accuracy: 0.9628\n",
      "Epoch 850/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5415 - accuracy: 0.8189\n",
      "Epoch 00850: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 42s 75ms/step - loss: 0.5415 - accuracy: 0.8189 - val_loss: 0.1718 - val_accuracy: 0.9590\n",
      "Epoch 851/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.8191\n",
      "Epoch 00851: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.5405 - accuracy: 0.8191 - val_loss: 0.1668 - val_accuracy: 0.9607\n",
      "Epoch 852/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5454 - accuracy: 0.8152 ETA: 0s - loss:\n",
      "Epoch 00852: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 43s 78ms/step - loss: 0.5453 - accuracy: 0.8152 - val_loss: 0.1636 - val_accuracy: 0.9628\n",
      "Epoch 853/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.8199\n",
      "Epoch 00853: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 43s 78ms/step - loss: 0.5344 - accuracy: 0.8199 - val_loss: 0.1586 - val_accuracy: 0.9635\n",
      "Epoch 854/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.8193\n",
      "Epoch 00854: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 47s 84ms/step - loss: 0.5398 - accuracy: 0.8193 - val_loss: 0.1600 - val_accuracy: 0.9646\n",
      "Epoch 855/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5399 - accuracy: 0.8219\n",
      "Epoch 00855: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 49s 88ms/step - loss: 0.5399 - accuracy: 0.8219 - val_loss: 0.1651 - val_accuracy: 0.9623\n",
      "Epoch 856/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.8170\n",
      "Epoch 00856: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 46s 83ms/step - loss: 0.5456 - accuracy: 0.8170 - val_loss: 0.1673 - val_accuracy: 0.9619\n",
      "Epoch 857/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5421 - accuracy: 0.8185\n",
      "Epoch 00857: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 0.5422 - accuracy: 0.8185 - val_loss: 0.1624 - val_accuracy: 0.9625\n",
      "Epoch 858/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5400 - accuracy: 0.8195\n",
      "Epoch 00858: loss did not improve from 0.53142\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 0.5399 - accuracy: 0.8196 - val_loss: 0.1591 - val_accuracy: 0.9634\n",
      "Epoch 859/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.8233 ETA: 0s\n",
      "Epoch 00859: loss improved from 0.53142 to 0.52933, saving model to ./Checkpoints\\weights-improvement-epoch_859-loss_0.5293-accuracy_0.8233.hdf5\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.5293 - accuracy: 0.8233 - val_loss: 0.1584 - val_accuracy: 0.9639\n",
      "Epoch 860/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5242 - accuracy: 0.8249\n",
      "Epoch 00860: loss improved from 0.52933 to 0.52417, saving model to ./Checkpoints\\weights-improvement-epoch_860-loss_0.5242-accuracy_0.8249.hdf5\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.5242 - accuracy: 0.8249 - val_loss: 0.1543 - val_accuracy: 0.9652\n",
      "Epoch 861/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.8193\n",
      "Epoch 00861: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.5356 - accuracy: 0.8193 - val_loss: 0.1682 - val_accuracy: 0.9605\n",
      "Epoch 862/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5597 - accuracy: 0.8125\n",
      "Epoch 00862: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.5597 - accuracy: 0.8125 - val_loss: 0.1604 - val_accuracy: 0.9628\n",
      "Epoch 863/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 0.5792 - accuracy: 0.8084 ETA: 0s - loss: 0.5792 - accuracy: 0.80\n",
      "Epoch 00863: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 47s 86ms/step - loss: 0.5792 - accuracy: 0.8084 - val_loss: 0.1678 - val_accuracy: 0.9623\n",
      "Epoch 864/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5341 - accuracy: 0.8212\n",
      "Epoch 00864: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 0.5341 - accuracy: 0.8212 - val_loss: 0.1648 - val_accuracy: 0.9618\n",
      "Epoch 865/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 0.5259 - accuracy: 0.8231\n",
      "Epoch 00865: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 0.5258 - accuracy: 0.8231 - val_loss: 0.1784 - val_accuracy: 0.9555\n",
      "Epoch 866/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6702 - accuracy: 0.5379\n",
      "Epoch 00866: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 51s 91ms/step - loss: 1.6702 - accuracy: 0.5379 - val_loss: 1.5988 - val_accuracy: 0.5097\n",
      "Epoch 867/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9245 - accuracy: 0.4317\n",
      "Epoch 00867: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 52s 94ms/step - loss: 1.9247 - accuracy: 0.4316 - val_loss: 1.8734 - val_accuracy: 0.4332\n",
      "Epoch 868/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9907 - accuracy: 0.4076\n",
      "Epoch 00868: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 50s 90ms/step - loss: 1.9907 - accuracy: 0.4076 - val_loss: 1.5592 - val_accuracy: 0.5212\n",
      "Epoch 869/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9057 - accuracy: 0.4273\n",
      "Epoch 00869: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 49s 89ms/step - loss: 1.9057 - accuracy: 0.4273 - val_loss: 1.7281 - val_accuracy: 0.4666\n",
      "Epoch 870/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9625 - accuracy: 0.4083\n",
      "Epoch 00870: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 49s 89ms/step - loss: 1.9625 - accuracy: 0.4083 - val_loss: 1.6410 - val_accuracy: 0.4928\n",
      "Epoch 871/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9032 - accuracy: 0.4220\n",
      "Epoch 00871: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 54s 97ms/step - loss: 1.9032 - accuracy: 0.4220 - val_loss: 1.5902 - val_accuracy: 0.5067\n",
      "Epoch 872/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.8609 - accuracy: 0.4301\n",
      "Epoch 00872: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 1.8608 - accuracy: 0.4301 - val_loss: 1.5457 - val_accuracy: 0.5178\n",
      "Epoch 873/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.8213 - accuracy: 0.4439\n",
      "Epoch 00873: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 1.8214 - accuracy: 0.4438 - val_loss: 1.5051 - val_accuracy: 0.5309\n",
      "Epoch 874/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7940 - accuracy: 0.4484 ETA: 1s\n",
      "Epoch 00874: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 47s 84ms/step - loss: 1.7940 - accuracy: 0.4485 - val_loss: 1.4740 - val_accuracy: 0.5406\n",
      "Epoch 875/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/555 [==============================] - ETA: 0s - loss: 1.7665 - accuracy: 0.4575\n",
      "Epoch 00875: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 47s 84ms/step - loss: 1.7665 - accuracy: 0.4575 - val_loss: 1.4429 - val_accuracy: 0.5496\n",
      "Epoch 876/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7477 - accuracy: 0.4607\n",
      "Epoch 00876: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 46s 84ms/step - loss: 1.7476 - accuracy: 0.4608 - val_loss: 1.4196 - val_accuracy: 0.5559\n",
      "Epoch 877/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.7190 - accuracy: 0.4675\n",
      "Epoch 00877: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.7191 - accuracy: 0.4674 - val_loss: 1.3928 - val_accuracy: 0.5632\n",
      "Epoch 878/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.7029 - accuracy: 0.4736\n",
      "Epoch 00878: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.7029 - accuracy: 0.4736 - val_loss: 1.3785 - val_accuracy: 0.5681\n",
      "Epoch 879/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6916 - accuracy: 0.4766\n",
      "Epoch 00879: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.6914 - accuracy: 0.4766 - val_loss: 1.3519 - val_accuracy: 0.5773\n",
      "Epoch 880/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6771 - accuracy: 0.4793\n",
      "Epoch 00880: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.6771 - accuracy: 0.4793 - val_loss: 1.3354 - val_accuracy: 0.5809\n",
      "Epoch 881/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6587 - accuracy: 0.4821\n",
      "Epoch 00881: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.6587 - accuracy: 0.4821 - val_loss: 1.3195 - val_accuracy: 0.5862\n",
      "Epoch 882/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6392 - accuracy: 0.4881\n",
      "Epoch 00882: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.6392 - accuracy: 0.4881 - val_loss: 1.3013 - val_accuracy: 0.5925\n",
      "Epoch 883/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6242 - accuracy: 0.4945\n",
      "Epoch 00883: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.6243 - accuracy: 0.4945 - val_loss: 1.2891 - val_accuracy: 0.5956\n",
      "Epoch 884/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6074 - accuracy: 0.4973\n",
      "Epoch 00884: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.6074 - accuracy: 0.4973 - val_loss: 1.2672 - val_accuracy: 0.6041\n",
      "Epoch 885/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.6018 - accuracy: 0.5007\n",
      "Epoch 00885: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 1.6018 - accuracy: 0.5007 - val_loss: 1.2570 - val_accuracy: 0.6088\n",
      "Epoch 886/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.6012 - accuracy: 0.5014\n",
      "Epoch 00886: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.6012 - accuracy: 0.5014 - val_loss: 1.2710 - val_accuracy: 0.6020\n",
      "Epoch 887/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5915 - accuracy: 0.5018\n",
      "Epoch 00887: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.5915 - accuracy: 0.5018 - val_loss: 1.2504 - val_accuracy: 0.6086\n",
      "Epoch 888/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5830 - accuracy: 0.5030\n",
      "Epoch 00888: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.5830 - accuracy: 0.5030 - val_loss: 1.2368 - val_accuracy: 0.6132\n",
      "Epoch 889/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5716 - accuracy: 0.5062\n",
      "Epoch 00889: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.5716 - accuracy: 0.5062 - val_loss: 1.2254 - val_accuracy: 0.6156\n",
      "Epoch 890/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5628 - accuracy: 0.5116\n",
      "Epoch 00890: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.5628 - accuracy: 0.5116 - val_loss: 1.2055 - val_accuracy: 0.6215\n",
      "Epoch 891/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.5470 - accuracy: 0.5159\n",
      "Epoch 00891: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.5468 - accuracy: 0.5159 - val_loss: 1.1992 - val_accuracy: 0.6227\n",
      "Epoch 892/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5373 - accuracy: 0.5183\n",
      "Epoch 00892: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.5373 - accuracy: 0.5183 - val_loss: 1.1865 - val_accuracy: 0.6283\n",
      "Epoch 893/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5284 - accuracy: 0.5193\n",
      "Epoch 00893: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 33s 60ms/step - loss: 1.5284 - accuracy: 0.5193 - val_loss: 1.1686 - val_accuracy: 0.6337\n",
      "Epoch 894/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.5186 - accuracy: 0.5218\n",
      "Epoch 00894: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 1.5186 - accuracy: 0.5218 - val_loss: 1.1657 - val_accuracy: 0.6357\n",
      "Epoch 895/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.5111 - accuracy: 0.5212\n",
      "Epoch 00895: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 1.5109 - accuracy: 0.5213 - val_loss: 1.1586 - val_accuracy: 0.6366\n",
      "Epoch 896/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.4978 - accuracy: 0.5285\n",
      "Epoch 00896: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 1.4976 - accuracy: 0.5285 - val_loss: 1.1379 - val_accuracy: 0.6442\n",
      "Epoch 897/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4842 - accuracy: 0.5324\n",
      "Epoch 00897: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.4842 - accuracy: 0.5324 - val_loss: 1.1288 - val_accuracy: 0.6475\n",
      "Epoch 898/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4770 - accuracy: 0.5325\n",
      "Epoch 00898: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.4770 - accuracy: 0.5325 - val_loss: 1.1162 - val_accuracy: 0.6526\n",
      "Epoch 899/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4679 - accuracy: 0.5350\n",
      "Epoch 00899: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.4679 - accuracy: 0.5350 - val_loss: 1.1061 - val_accuracy: 0.6542\n",
      "Epoch 900/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4521 - accuracy: 0.5383\n",
      "Epoch 00900: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.4521 - accuracy: 0.5383 - val_loss: 1.0949 - val_accuracy: 0.6568\n",
      "Epoch 901/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.4400 - accuracy: 0.5428\n",
      "Epoch 00901: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.4399 - accuracy: 0.5428 - val_loss: 1.0790 - val_accuracy: 0.6620\n",
      "Epoch 902/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4305 - accuracy: 0.5455\n",
      "Epoch 00902: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.4305 - accuracy: 0.5455 - val_loss: 1.0701 - val_accuracy: 0.6650\n",
      "Epoch 903/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4228 - accuracy: 0.5462\n",
      "Epoch 00903: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 1.4228 - accuracy: 0.5462 - val_loss: 1.0586 - val_accuracy: 0.6687\n",
      "Epoch 904/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4093 - accuracy: 0.5525\n",
      "Epoch 00904: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.4093 - accuracy: 0.5525 - val_loss: 1.0391 - val_accuracy: 0.6749\n",
      "Epoch 905/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.4013 - accuracy: 0.5554\n",
      "Epoch 00905: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.4013 - accuracy: 0.5554 - val_loss: 1.0330 - val_accuracy: 0.6745\n",
      "Epoch 906/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3873 - accuracy: 0.5598\n",
      "Epoch 00906: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3873 - accuracy: 0.5598 - val_loss: 1.0211 - val_accuracy: 0.6798\n",
      "Epoch 907/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3718 - accuracy: 0.5637\n",
      "Epoch 00907: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3718 - accuracy: 0.5637 - val_loss: 1.0093 - val_accuracy: 0.6833\n",
      "Epoch 908/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.3600 - accuracy: 0.5658\n",
      "Epoch 00908: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.3600 - accuracy: 0.5657 - val_loss: 0.9906 - val_accuracy: 0.6917\n",
      "Epoch 909/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.3404 - accuracy: 0.5730\n",
      "Epoch 00909: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.3404 - accuracy: 0.5730 - val_loss: 0.9766 - val_accuracy: 0.6949\n",
      "Epoch 910/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0530 - accuracy: 0.3901\n",
      "Epoch 00910: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0530 - accuracy: 0.3901 - val_loss: 2.0454 - val_accuracy: 0.3802\n",
      "Epoch 911/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.2244 - accuracy: 0.3364\n",
      "Epoch 00911: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.2244 - accuracy: 0.3364 - val_loss: 1.9827 - val_accuracy: 0.3972\n",
      "Epoch 912/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.1734 - accuracy: 0.3479\n",
      "Epoch 00912: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.1734 - accuracy: 0.3479 - val_loss: 1.9138 - val_accuracy: 0.4143\n",
      "Epoch 913/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.1221 - accuracy: 0.3603\n",
      "Epoch 00913: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.1221 - accuracy: 0.3603 - val_loss: 1.8708 - val_accuracy: 0.4281\n",
      "Epoch 914/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0893 - accuracy: 0.3696\n",
      "Epoch 00914: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0893 - accuracy: 0.3696 - val_loss: 1.8425 - val_accuracy: 0.4370\n",
      "Epoch 915/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0542 - accuracy: 0.3753\n",
      "Epoch 00915: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0542 - accuracy: 0.3753 - val_loss: 1.8003 - val_accuracy: 0.4469\n",
      "Epoch 916/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0331 - accuracy: 0.3827\n",
      "Epoch 00916: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0331 - accuracy: 0.3827 - val_loss: 1.7829 - val_accuracy: 0.4530\n",
      "Epoch 917/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0120 - accuracy: 0.3887\n",
      "Epoch 00917: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0120 - accuracy: 0.3887 - val_loss: 1.7550 - val_accuracy: 0.4602\n",
      "Epoch 918/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0104 - accuracy: 0.3905\n",
      "Epoch 00918: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0104 - accuracy: 0.3905 - val_loss: 1.7530 - val_accuracy: 0.4628\n",
      "Epoch 919/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9813 - accuracy: 0.3951\n",
      "Epoch 00919: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.9813 - accuracy: 0.3951 - val_loss: 1.7239 - val_accuracy: 0.4699\n",
      "Epoch 920/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9638 - accuracy: 0.4003\n",
      "Epoch 00920: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.9638 - accuracy: 0.4003 - val_loss: 1.7089 - val_accuracy: 0.4706\n",
      "Epoch 921/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9495 - accuracy: 0.4049\n",
      "Epoch 00921: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.9495 - accuracy: 0.4049 - val_loss: 1.6877 - val_accuracy: 0.4793\n",
      "Epoch 922/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9317 - accuracy: 0.4100\n",
      "Epoch 00922: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.9317 - accuracy: 0.4100 - val_loss: 1.6656 - val_accuracy: 0.4848\n",
      "Epoch 923/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9424 - accuracy: 0.4079\n",
      "Epoch 00923: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.9424 - accuracy: 0.4079 - val_loss: 1.9173 - val_accuracy: 0.4157\n",
      "Epoch 924/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.1382 - accuracy: 0.3562\n",
      "Epoch 00924: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 2.1382 - accuracy: 0.3562 - val_loss: 1.9266 - val_accuracy: 0.4141\n",
      "Epoch 925/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.5648 - accuracy: 0.2579\n",
      "Epoch 00925: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 2.5648 - accuracy: 0.2579 - val_loss: 2.6211 - val_accuracy: 0.2333\n",
      "Epoch 926/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.6767 - accuracy: 0.2243\n",
      "Epoch 00926: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 2.6765 - accuracy: 0.2243 - val_loss: 2.5504 - val_accuracy: 0.2471\n",
      "Epoch 927/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.6238 - accuracy: 0.2316\n",
      "Epoch 00927: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 32s 58ms/step - loss: 2.6239 - accuracy: 0.2316 - val_loss: 2.5473 - val_accuracy: 0.2456\n",
      "Epoch 928/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.6160 - accuracy: 0.2333\n",
      "Epoch 00928: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.6160 - accuracy: 0.2333 - val_loss: 2.4941 - val_accuracy: 0.2577\n",
      "Epoch 929/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.5769 - accuracy: 0.2416\n",
      "Epoch 00929: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 32s 57ms/step - loss: 2.5769 - accuracy: 0.2416 - val_loss: 2.4886 - val_accuracy: 0.2561\n",
      "Epoch 930/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.5594 - accuracy: 0.2458\n",
      "Epoch 00930: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.5594 - accuracy: 0.2458 - val_loss: 2.4599 - val_accuracy: 0.2657\n",
      "Epoch 931/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.5602 - accuracy: 0.2426\n",
      "Epoch 00931: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.5602 - accuracy: 0.2426 - val_loss: 2.4971 - val_accuracy: 0.2555\n",
      "Epoch 932/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.5778 - accuracy: 0.2382\n",
      "Epoch 00932: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.5778 - accuracy: 0.2382 - val_loss: 2.4879 - val_accuracy: 0.2568\n",
      "Epoch 933/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/555 [============================>.] - ETA: 0s - loss: 2.5738 - accuracy: 0.2394\n",
      "Epoch 00933: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 32s 57ms/step - loss: 2.5736 - accuracy: 0.2395 - val_loss: 2.4660 - val_accuracy: 0.2606\n",
      "Epoch 934/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.5440 - accuracy: 0.2449\n",
      "Epoch 00934: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.5440 - accuracy: 0.2449 - val_loss: 2.4254 - val_accuracy: 0.2729\n",
      "Epoch 935/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.5099 - accuracy: 0.2531\n",
      "Epoch 00935: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.5099 - accuracy: 0.2532 - val_loss: 2.3986 - val_accuracy: 0.2780\n",
      "Epoch 936/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.4996 - accuracy: 0.2564\n",
      "Epoch 00936: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.4996 - accuracy: 0.2563 - val_loss: 2.3924 - val_accuracy: 0.2812\n",
      "Epoch 937/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.4498 - accuracy: 0.2676\n",
      "Epoch 00937: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 2.4498 - accuracy: 0.2676 - val_loss: 2.1960 - val_accuracy: 0.3364\n",
      "Epoch 938/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.2959 - accuracy: 0.3086 ETA: 0s - loss: 2.2956 - accuracy\n",
      "Epoch 00938: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.2959 - accuracy: 0.3085 - val_loss: 2.1542 - val_accuracy: 0.3487\n",
      "Epoch 939/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.2826 - accuracy: 0.3125\n",
      "Epoch 00939: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 2.2826 - accuracy: 0.3125 - val_loss: 2.1385 - val_accuracy: 0.3516\n",
      "Epoch 940/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.2655 - accuracy: 0.3172\n",
      "Epoch 00940: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 2.2656 - accuracy: 0.3172 - val_loss: 2.1160 - val_accuracy: 0.3590\n",
      "Epoch 941/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.2603 - accuracy: 0.3156\n",
      "Epoch 00941: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 55ms/step - loss: 2.2604 - accuracy: 0.3156 - val_loss: 2.1101 - val_accuracy: 0.3595\n",
      "Epoch 942/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.2465 - accuracy: 0.3200\n",
      "Epoch 00942: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.2465 - accuracy: 0.3200 - val_loss: 2.0912 - val_accuracy: 0.3657\n",
      "Epoch 943/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.2340 - accuracy: 0.3279\n",
      "Epoch 00943: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.2340 - accuracy: 0.3279 - val_loss: 2.0815 - val_accuracy: 0.3688\n",
      "Epoch 944/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.2282 - accuracy: 0.3272\n",
      "Epoch 00944: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 2.2282 - accuracy: 0.3272 - val_loss: 2.0713 - val_accuracy: 0.3713\n",
      "Epoch 945/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.2171 - accuracy: 0.3324\n",
      "Epoch 00945: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 2.2171 - accuracy: 0.3324 - val_loss: 2.0569 - val_accuracy: 0.3742\n",
      "Epoch 946/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.2102 - accuracy: 0.3327\n",
      "Epoch 00946: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.2102 - accuracy: 0.3327 - val_loss: 2.0511 - val_accuracy: 0.3763\n",
      "Epoch 947/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.1994 - accuracy: 0.3362\n",
      "Epoch 00947: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 2.1993 - accuracy: 0.3362 - val_loss: 2.0698 - val_accuracy: 0.3707\n",
      "Epoch 948/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.2124 - accuracy: 0.3315\n",
      "Epoch 00948: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 33s 59ms/step - loss: 2.2124 - accuracy: 0.3315 - val_loss: 2.0482 - val_accuracy: 0.3771\n",
      "Epoch 949/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.2092 - accuracy: 0.3327\n",
      "Epoch 00949: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 57ms/step - loss: 2.2092 - accuracy: 0.3327 - val_loss: 2.0466 - val_accuracy: 0.3761\n",
      "Epoch 950/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.1991 - accuracy: 0.3366\n",
      "Epoch 00950: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 55ms/step - loss: 2.1991 - accuracy: 0.3366 - val_loss: 2.0258 - val_accuracy: 0.3837\n",
      "Epoch 951/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.1828 - accuracy: 0.3433\n",
      "Epoch 00951: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 53ms/step - loss: 2.1828 - accuracy: 0.3433 - val_loss: 2.0149 - val_accuracy: 0.3885\n",
      "Epoch 952/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.1419 - accuracy: 0.3535\n",
      "Epoch 00952: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 2.1419 - accuracy: 0.3535 - val_loss: 1.9247 - val_accuracy: 0.4124\n",
      "Epoch 953/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0875 - accuracy: 0.3658\n",
      "Epoch 00953: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 2.0877 - accuracy: 0.3658 - val_loss: 1.9048 - val_accuracy: 0.4172\n",
      "Epoch 954/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0800 - accuracy: 0.3687\n",
      "Epoch 00954: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 2.0799 - accuracy: 0.3687 - val_loss: 1.8928 - val_accuracy: 0.4207\n",
      "Epoch 955/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0669 - accuracy: 0.3713\n",
      "Epoch 00955: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0671 - accuracy: 0.3713 - val_loss: 1.9143 - val_accuracy: 0.4150\n",
      "Epoch 956/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0876 - accuracy: 0.3670\n",
      "Epoch 00956: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0876 - accuracy: 0.3670 - val_loss: 1.8981 - val_accuracy: 0.4212\n",
      "Epoch 957/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0808 - accuracy: 0.3675\n",
      "Epoch 00957: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0809 - accuracy: 0.3675 - val_loss: 1.8919 - val_accuracy: 0.4197\n",
      "Epoch 958/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0832 - accuracy: 0.3686 ETA\n",
      "Epoch 00958: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 2.0832 - accuracy: 0.3686 - val_loss: 1.8986 - val_accuracy: 0.4190\n",
      "Epoch 959/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0794 - accuracy: 0.3675\n",
      "Epoch 00959: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 2.0794 - accuracy: 0.3675 - val_loss: 1.8877 - val_accuracy: 0.4206\n",
      "Epoch 960/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0717 - accuracy: 0.3723\n",
      "Epoch 00960: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 2.0717 - accuracy: 0.3723 - val_loss: 1.8813 - val_accuracy: 0.4221\n",
      "Epoch 961/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0712 - accuracy: 0.3720\n",
      "Epoch 00961: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0712 - accuracy: 0.3720 - val_loss: 1.8647 - val_accuracy: 0.4275\n",
      "Epoch 962/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0455 - accuracy: 0.3773\n",
      "Epoch 00962: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 2.0454 - accuracy: 0.3774 - val_loss: 1.8517 - val_accuracy: 0.4334\n",
      "Epoch 963/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0369 - accuracy: 0.3820\n",
      "Epoch 00963: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0369 - accuracy: 0.3820 - val_loss: 1.8398 - val_accuracy: 0.4363\n",
      "Epoch 964/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0279 - accuracy: 0.3830\n",
      "Epoch 00964: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 27s 50ms/step - loss: 2.0279 - accuracy: 0.3830 - val_loss: 1.8293 - val_accuracy: 0.4369\n",
      "Epoch 965/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.0244 - accuracy: 0.3829\n",
      "Epoch 00965: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0244 - accuracy: 0.3830 - val_loss: 1.8112 - val_accuracy: 0.4423\n",
      "Epoch 966/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0196 - accuracy: 0.3869\n",
      "Epoch 00966: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0196 - accuracy: 0.3869 - val_loss: 1.8111 - val_accuracy: 0.4429\n",
      "Epoch 967/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 2.0171 - accuracy: 0.3837\n",
      "Epoch 00967: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 2.0171 - accuracy: 0.3837 - val_loss: 1.8286 - val_accuracy: 0.4380\n",
      "Epoch 968/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0324 - accuracy: 0.3830\n",
      "Epoch 00968: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0324 - accuracy: 0.3830 - val_loss: 1.8271 - val_accuracy: 0.4400\n",
      "Epoch 969/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0232 - accuracy: 0.3846\n",
      "Epoch 00969: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 2.0232 - accuracy: 0.3846 - val_loss: 1.8104 - val_accuracy: 0.4434\n",
      "Epoch 970/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0120 - accuracy: 0.3876\n",
      "Epoch 00970: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 29s 52ms/step - loss: 2.0120 - accuracy: 0.3876 - val_loss: 1.8159 - val_accuracy: 0.4435\n",
      "Epoch 971/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0090 - accuracy: 0.3896\n",
      "Epoch 00971: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 2.0090 - accuracy: 0.3896 - val_loss: 1.8001 - val_accuracy: 0.4489\n",
      "Epoch 972/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.0014 - accuracy: 0.3902\n",
      "Epoch 00972: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 2.0014 - accuracy: 0.3902 - val_loss: 1.7898 - val_accuracy: 0.4500\n",
      "Epoch 973/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9947 - accuracy: 0.3922\n",
      "Epoch 00973: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 53ms/step - loss: 1.9947 - accuracy: 0.3922 - val_loss: 1.7842 - val_accuracy: 0.4509\n",
      "Epoch 974/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9886 - accuracy: 0.3927\n",
      "Epoch 00974: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.9886 - accuracy: 0.3927 - val_loss: 1.7743 - val_accuracy: 0.4541\n",
      "Epoch 975/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9824 - accuracy: 0.3954\n",
      "Epoch 00975: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.9825 - accuracy: 0.3953 - val_loss: 1.7730 - val_accuracy: 0.4559\n",
      "Epoch 976/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9821 - accuracy: 0.3955\n",
      "Epoch 00976: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.9822 - accuracy: 0.3955 - val_loss: 1.8028 - val_accuracy: 0.4456\n",
      "Epoch 977/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9977 - accuracy: 0.3900\n",
      "Epoch 00977: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.9977 - accuracy: 0.3900 - val_loss: 1.7773 - val_accuracy: 0.4551\n",
      "Epoch 978/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9822 - accuracy: 0.3954\n",
      "Epoch 00978: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.9824 - accuracy: 0.3954 - val_loss: 1.7646 - val_accuracy: 0.4568\n",
      "Epoch 979/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9689 - accuracy: 0.3975\n",
      "Epoch 00979: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.9689 - accuracy: 0.3975 - val_loss: 1.7526 - val_accuracy: 0.4592\n",
      "Epoch 980/1000\n",
      "553/555 [============================>.] - ETA: 0s - loss: 1.9709 - accuracy: 0.3999\n",
      "Epoch 00980: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.9710 - accuracy: 0.3999 - val_loss: 1.7471 - val_accuracy: 0.4608\n",
      "Epoch 981/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9598 - accuracy: 0.3997\n",
      "Epoch 00981: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.9598 - accuracy: 0.3997 - val_loss: 1.7302 - val_accuracy: 0.4660\n",
      "Epoch 982/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9484 - accuracy: 0.4018\n",
      "Epoch 00982: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 55ms/step - loss: 1.9486 - accuracy: 0.4017 - val_loss: 1.7261 - val_accuracy: 0.4671\n",
      "Epoch 983/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9518 - accuracy: 0.4029\n",
      "Epoch 00983: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.9518 - accuracy: 0.4029 - val_loss: 1.7291 - val_accuracy: 0.4661\n",
      "Epoch 984/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9494 - accuracy: 0.4047\n",
      "Epoch 00984: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.9494 - accuracy: 0.4046 - val_loss: 1.7223 - val_accuracy: 0.4685\n",
      "Epoch 985/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9417 - accuracy: 0.4056\n",
      "Epoch 00985: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 57ms/step - loss: 1.9417 - accuracy: 0.4056 - val_loss: 1.7129 - val_accuracy: 0.4704\n",
      "Epoch 986/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9436 - accuracy: 0.4056\n",
      "Epoch 00986: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 36s 65ms/step - loss: 1.9436 - accuracy: 0.4056 - val_loss: 1.7160 - val_accuracy: 0.4712\n",
      "Epoch 987/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9429 - accuracy: 0.4031\n",
      "Epoch 00987: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 36s 64ms/step - loss: 1.9428 - accuracy: 0.4031 - val_loss: 1.7447 - val_accuracy: 0.4634\n",
      "Epoch 988/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9508 - accuracy: 0.4022\n",
      "Epoch 00988: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 31s 56ms/step - loss: 1.9507 - accuracy: 0.4022 - val_loss: 1.7257 - val_accuracy: 0.4677\n",
      "Epoch 989/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9456 - accuracy: 0.4048\n",
      "Epoch 00989: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.9458 - accuracy: 0.4048 - val_loss: 1.7437 - val_accuracy: 0.4616\n",
      "Epoch 990/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9953 - accuracy: 0.3932\n",
      "Epoch 00990: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.9952 - accuracy: 0.3933 - val_loss: 1.7823 - val_accuracy: 0.4511\n",
      "Epoch 991/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/555 [============================>.] - ETA: 0s - loss: 1.9900 - accuracy: 0.3954\n",
      "Epoch 00991: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.9900 - accuracy: 0.3953 - val_loss: 1.7707 - val_accuracy: 0.4531\n",
      "Epoch 992/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9783 - accuracy: 0.3939\n",
      "Epoch 00992: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.9783 - accuracy: 0.3939 - val_loss: 1.7655 - val_accuracy: 0.4586\n",
      "Epoch 993/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9713 - accuracy: 0.3981\n",
      "Epoch 00993: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 29s 53ms/step - loss: 1.9712 - accuracy: 0.3981 - val_loss: 1.7459 - val_accuracy: 0.4623\n",
      "Epoch 994/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9686 - accuracy: 0.3984\n",
      "Epoch 00994: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.9688 - accuracy: 0.3984 - val_loss: 1.7387 - val_accuracy: 0.4643\n",
      "Epoch 995/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9440 - accuracy: 0.4041\n",
      "Epoch 00995: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.9440 - accuracy: 0.4041 - val_loss: 1.7064 - val_accuracy: 0.4751\n",
      "Epoch 996/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9193 - accuracy: 0.4112\n",
      "Epoch 00996: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 32s 57ms/step - loss: 1.9193 - accuracy: 0.4112 - val_loss: 1.6865 - val_accuracy: 0.4799\n",
      "Epoch 997/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.9073 - accuracy: 0.4138\n",
      "Epoch 00997: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 30s 54ms/step - loss: 1.9073 - accuracy: 0.4138 - val_loss: 1.6674 - val_accuracy: 0.4849\n",
      "Epoch 998/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.9005 - accuracy: 0.4175\n",
      "Epoch 00998: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 51ms/step - loss: 1.9005 - accuracy: 0.4174 - val_loss: 1.6578 - val_accuracy: 0.4888\n",
      "Epoch 999/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 1.8970 - accuracy: 0.4202\n",
      "Epoch 00999: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 28s 50ms/step - loss: 1.8970 - accuracy: 0.4202 - val_loss: 1.6520 - val_accuracy: 0.4907\n",
      "Epoch 1000/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 1.8828 - accuracy: 0.4211\n",
      "Epoch 01000: loss did not improve from 0.52417\n",
      "555/555 [==============================] - 27s 49ms/step - loss: 1.8828 - accuracy: 0.4211 - val_loss: 1.6365 - val_accuracy: 0.4931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d0c093760>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=1000, batch_size=100, verbose=True, callbacks=callbacks_list, validation_data=(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficas del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_graph(model, labels):\n",
    "    fig, ax = plt.subplots(figsize=(20, 15))\n",
    "    index = 0\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot(model.history.history[labels[i]])\n",
    "        plt.title(f'{labels[i]} vs Iteraciones')\n",
    "        plt.ylabel(f'{labels[i]}')\n",
    "        plt.xlabel('Itereacion')\n",
    "\n",
    "    plt.suptitle('Resultados del entrenamiento vs Iteraciones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['loss', 'val_loss', 'val_accuracy', 'accuracy']\n",
    "\n",
    "# model.history.history['loss']\n",
    "\n",
    "simple_graph(model, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzolV_rYYz6u"
   },
   "source": [
    "## Genereando texto con una red LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya se había realizado un entrenamiento previo, previendo que fueran 10.000 epochs, pero el modelo entrenaba aproximadamente 100 epochs por hora, además de tener un alto consumo de recursos, razón por la cuál durante dicho entrenamiento solo se alcanzarón 1.698 epochs en 15 horas de entrenamiento, de los cuales 335 epochs fueron guardados como checkpoints por su mejora en el parámetro de `loss`.\n",
    "\n",
    "Como el entrenamiento no se termino de manera correcta, no se pueden mostrar las gráficas del mismo. Para generar dichas gráficas se opto por hacer un nuevo entrenamiento de solo 1.000 epochs. Las diferencias entre el primer y segundo entrenamiento se notan en la predicción de los datos, donde el primer checkpoint, aún teniendo una perdida más alta y una precisión más baja, logra generar un texto con un poco más de concordancia, difiriendo al segundo modelo guardado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './Checkpoints/weights-improvement-epoch_1674-loss_0.7068-accuracy_0.7634.hdf5'\n",
    "#filename = './Checkpoints/weights-improvement-epoch_860-loss_0.5242-accuracy_0.8249.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "3OdJFbGvZUQs"
   },
   "outputs": [],
   "source": [
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 100, 1),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'lstm_input'}},\n",
       "  {'class_name': 'LSTM',\n",
       "   'config': {'name': 'lstm',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 100, 1),\n",
       "    'dtype': 'float32',\n",
       "    'return_sequences': True,\n",
       "    'return_state': False,\n",
       "    'go_backwards': False,\n",
       "    'stateful': False,\n",
       "    'unroll': False,\n",
       "    'time_major': False,\n",
       "    'units': 256,\n",
       "    'activation': 'tanh',\n",
       "    'recurrent_activation': 'sigmoid',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'recurrent_initializer': {'class_name': 'Orthogonal',\n",
       "     'config': {'gain': 1.0, 'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'unit_forget_bias': True,\n",
       "    'kernel_regularizer': None,\n",
       "    'recurrent_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'recurrent_constraint': None,\n",
       "    'bias_constraint': None,\n",
       "    'dropout': 0.0,\n",
       "    'recurrent_dropout': 0.0,\n",
       "    'implementation': 2}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.2,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'LSTM',\n",
       "   'config': {'name': 'lstm_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'return_sequences': False,\n",
       "    'return_state': False,\n",
       "    'go_backwards': False,\n",
       "    'stateful': False,\n",
       "    'unroll': False,\n",
       "    'time_major': False,\n",
       "    'units': 128,\n",
       "    'activation': 'tanh',\n",
       "    'recurrent_activation': 'sigmoid',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'recurrent_initializer': {'class_name': 'Orthogonal',\n",
       "     'config': {'gain': 1.0, 'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'unit_forget_bias': True,\n",
       "    'kernel_regularizer': None,\n",
       "    'recurrent_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'recurrent_constraint': None,\n",
       "    'bias_constraint': None,\n",
       "    'dropout': 0.0,\n",
       "    'recurrent_dropout': 0.0,\n",
       "    'implementation': 2}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.1,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 64,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.1,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 50,\n",
       "    'activation': 'softmax',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "obRCxBMjZaWS"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapeo Inverso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso, se convierte de números a letras, con el fin de poder entender las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacer las predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma más sencilla de utilizar el modelo Keras LSTM para hacer predicciones es comenzar primero con una secuencia como entrada, generar el siguiente carácter y luego actualizar la secuencia semilla para agregar el carácter generado al final y recortar el primer carácter. Este proceso se repite mientras queramos predecir nuevos caracteres (ej., una secuencia de 1000 caracteres de longitud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Elegir una semilla al azar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_semilla():\n",
    "    start = np.random.randint(0, len(dataX) - 1)\n",
    "    pattern = dataX[start]\n",
    "    print('Semilla')\n",
    "    print(''.join([int_to_char[value] for value in pattern]))\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semilla\n",
      "so tenderles una trampa. con este proposito organizo un concurso de tiro con arco, cuyo premio seria\n"
     ]
    }
   ],
   "source": [
    "pattern = generar_semilla()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generación de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_caracteres(pattern):\n",
    "    for i in range(1000):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(n_vocab)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        seq_in = [int_to_char[value] for value in pattern]\n",
    "        sys.stdout.write(result)\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1 : len(pattern)]\n",
    "\n",
    "    print('\\n\\nDone'.ljust(80, '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entregado por la viera  duando el principe. el principe dra eutaba la casa de lo que las prddr de lo huas sio puedaba cnn una cama de ese nomerto perer para earme corriendo y peter pando de la casa y no lo fizo la casa, cuando los cerdizos,\n",
      "  csando la bola denicien\n",
      "qara   dijo el ciasmo. puent de nobo gra calicienta, por flsoresar. tendy y haca mas hermosa.\n",
      "pero el hombre de jengibre secor a los piratas, el principe juan.\n",
      "era huve della cuento de campesinas y apue muchante dontiouar hnca el pato pue le dio peter por la princesa entregar a la basa se ercoceen por la basa de lo que puedarse con ella al callena muy bores de mo que peter se acaban con sodos los cian dsan sieuio pocoe cari sienpre ee apreltaban en el entedioso encontras a la princesa cnntra los piratas a la princesa qerdade. el principe que,ninstero hacia cllo:   â¡cua, cua! â¡pueen ei hombre de jengibre  â¡quiero comerte!\n",
      "y rus hermanos de carabas,\n",
      "cuando la ciimenea donde estuvieron darling y la basa de los cnanian en a\n",
      "\n",
      "Done--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generar_caracteres(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero cabe resaltar que el modelo representado en las gráficas presenta un crecimiento logarítmico en el parámetro de `accuracy` y de `val_accuracy`, pero a partir del epoch 815 aproximadamente, pierde precisión creando un pico elevado en la gráfica de la perdida. Pero, luego la red nuevamente comienza a mejorar su puntaje de `accuracy` logrando de nueva cuenta un 50% de precisión. Cabe resaltar que por temas de consumo de recursos físicos, la red no pudo ser entrenada para obtener una precisión más alta junto a una perdida más baj109a.\n",
    "\n",
    "En el checkpoint con el que se está haciendo la carga de los pesos, se obtuvo una precisión del 76.34% y una perdida del 70.68%, lo que implica que al momento de guardar dicho checkpoint, el modelo estaba presentando un undefitting ya que la desviación entre las predicciones es bastante alta. Debemos tomar en cuenta que, al iniciar el entrenamiento, el modelo presenta una pérdida del 300%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejorando la red a través de una LSTM más grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(32, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(16, activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7obbgLUY4aD"
   },
   "source": [
    "Cargamos el último checkpoint de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "rmaT9isYYzXL"
   },
   "outputs": [],
   "source": [
    "filepath = './Checkpoints-V2/weights-improvement-epoch_{epoch:02d}-loss_{loss:.4f}-accuracy_{accuracy:.4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor=\"accuracy\", verbose=True, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9820 - accuracy: 0.1697\n",
      "Epoch 00001: accuracy improved from -inf to 0.16966, saving model to ./Checkpoints-V2\\weights-improvement-epoch_01-loss_2.9820-accuracy_0.1697.hdf5\n",
      "555/555 [==============================] - 80s 120ms/step - loss: 2.9820 - accuracy: 0.1697 - val_loss: 2.9042 - val_accuracy: 0.1784\n",
      "Epoch 2/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9189 - accuracy: 0.1777 ETA: 0s - loss: 2.9193 - accuracy: 0.\n",
      "Epoch 00002: accuracy improved from 0.16966 to 0.17767, saving model to ./Checkpoints-V2\\weights-improvement-epoch_02-loss_2.9189-accuracy_0.1777.hdf5\n",
      "555/555 [==============================] - 57s 103ms/step - loss: 2.9189 - accuracy: 0.1777 - val_loss: 2.9032 - val_accuracy: 0.1784\n",
      "Epoch 3/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9132 - accuracy: 0.1784\n",
      "Epoch 00003: accuracy improved from 0.17767 to 0.17839, saving model to ./Checkpoints-V2\\weights-improvement-epoch_03-loss_2.9132-accuracy_0.1784.hdf5\n",
      "555/555 [==============================] - 55s 100ms/step - loss: 2.9132 - accuracy: 0.1784 - val_loss: 2.9036 - val_accuracy: 0.1784\n",
      "Epoch 4/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9105 - accuracy: 0.1784\n",
      "Epoch 00004: accuracy improved from 0.17839 to 0.17843, saving model to ./Checkpoints-V2\\weights-improvement-epoch_04-loss_2.9104-accuracy_0.1784.hdf5\n",
      "555/555 [==============================] - 56s 101ms/step - loss: 2.9104 - accuracy: 0.1784 - val_loss: 2.9032 - val_accuracy: 0.1784\n",
      "Epoch 5/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9085 - accuracy: 0.1784\n",
      "Epoch 00005: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 56s 101ms/step - loss: 2.9085 - accuracy: 0.1784 - val_loss: 2.9039 - val_accuracy: 0.1784\n",
      "Epoch 6/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9073 - accuracy: 0.1784\n",
      "Epoch 00006: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 58s 105ms/step - loss: 2.9073 - accuracy: 0.1784 - val_loss: 2.9024 - val_accuracy: 0.1784\n",
      "Epoch 7/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9072 - accuracy: 0.1784\n",
      "Epoch 00007: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 57s 102ms/step - loss: 2.9072 - accuracy: 0.1784 - val_loss: 2.9018 - val_accuracy: 0.1784\n",
      "Epoch 8/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9070 - accuracy: 0.1784\n",
      "Epoch 00008: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 56s 102ms/step - loss: 2.9069 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 9/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9059 - accuracy: 0.1784\n",
      "Epoch 00009: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 58s 105ms/step - loss: 2.9060 - accuracy: 0.1784 - val_loss: 2.9013 - val_accuracy: 0.1784\n",
      "Epoch 10/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9056 - accuracy: 0.1784\n",
      "Epoch 00010: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 57s 102ms/step - loss: 2.9058 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 11/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9046 - accuracy: 0.1784\n",
      "Epoch 00011: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 58s 105ms/step - loss: 2.9046 - accuracy: 0.1784 - val_loss: 2.9019 - val_accuracy: 0.1784\n",
      "Epoch 12/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9046 - accuracy: 0.1784\n",
      "Epoch 00012: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 58s 104ms/step - loss: 2.9045 - accuracy: 0.1784 - val_loss: 2.9023 - val_accuracy: 0.1784\n",
      "Epoch 13/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9041 - accuracy: 0.1784\n",
      "Epoch 00013: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 60s 108ms/step - loss: 2.9041 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 14/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9041 - accuracy: 0.1784\n",
      "Epoch 00014: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 59s 106ms/step - loss: 2.9041 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 15/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9034 - accuracy: 0.1784\n",
      "Epoch 00015: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 59s 106ms/step - loss: 2.9036 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 16/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9040 - accuracy: 0.1784\n",
      "Epoch 00016: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 60s 108ms/step - loss: 2.9040 - accuracy: 0.1784 - val_loss: 2.9014 - val_accuracy: 0.1784\n",
      "Epoch 17/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9039 - accuracy: 0.1784\n",
      "Epoch 00017: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 60s 108ms/step - loss: 2.9039 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 18/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9033 - accuracy: 0.1784\n",
      "Epoch 00018: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 60s 109ms/step - loss: 2.9033 - accuracy: 0.1784 - val_loss: 2.9015 - val_accuracy: 0.1784\n",
      "Epoch 19/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9032 - accuracy: 0.1784\n",
      "Epoch 00019: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 60s 108ms/step - loss: 2.9033 - accuracy: 0.1784 - val_loss: 2.9014 - val_accuracy: 0.1784\n",
      "Epoch 20/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9032 - accuracy: 0.1784\n",
      "Epoch 00020: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 66s 119ms/step - loss: 2.9032 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 21/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9029 - accuracy: 0.1784\n",
      "Epoch 00021: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 64s 115ms/step - loss: 2.9030 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 22/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9029 - accuracy: 0.1784\n",
      "Epoch 00022: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 61s 110ms/step - loss: 2.9028 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 23/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9028 - accuracy: 0.1784\n",
      "Epoch 00023: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 63s 114ms/step - loss: 2.9028 - accuracy: 0.1784 - val_loss: 2.9015 - val_accuracy: 0.1784\n",
      "Epoch 24/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9030 - accuracy: 0.1784\n",
      "Epoch 00024: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 64s 115ms/step - loss: 2.9030 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 25/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9032 - accuracy: 0.1784\n",
      "Epoch 00025: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 51s 92ms/step - loss: 2.9032 - accuracy: 0.1784 - val_loss: 2.9015 - val_accuracy: 0.1784\n",
      "Epoch 26/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9026 - accuracy: 0.1784\n",
      "Epoch 00026: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 49s 89ms/step - loss: 2.9026 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 27/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9028 - accuracy: 0.1784\n",
      "Epoch 00027: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 48s 87ms/step - loss: 2.9028 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 28/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9024 - accuracy: 0.1784\n",
      "Epoch 00028: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 2.9024 - accuracy: 0.1784 - val_loss: 2.9016 - val_accuracy: 0.1784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9030 - accuracy: 0.1784\n",
      "Epoch 00029: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 2.9029 - accuracy: 0.1784 - val_loss: 2.9013 - val_accuracy: 0.1784\n",
      "Epoch 30/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9030 - accuracy: 0.1784\n",
      "Epoch 00030: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 2.9030 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 31/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9030 - accuracy: 0.1784\n",
      "Epoch 00031: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9030 - accuracy: 0.1784 - val_loss: 2.9015 - val_accuracy: 0.1784\n",
      "Epoch 32/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9028 - accuracy: 0.1784\n",
      "Epoch 00032: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9028 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 33/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00033: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 86ms/step - loss: 2.9024 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 34/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9025 - accuracy: 0.1784\n",
      "Epoch 00034: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 2.9025 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 35/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9027 - accuracy: 0.1784\n",
      "Epoch 00035: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 2.9026 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 36/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9027 - accuracy: 0.1784\n",
      "Epoch 00036: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 86ms/step - loss: 2.9027 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 37/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9027 - accuracy: 0.1784\n",
      "Epoch 00037: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 51s 92ms/step - loss: 2.9026 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 38/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9024 - accuracy: 0.1784\n",
      "Epoch 00038: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 2.9024 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 39/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9026 - accuracy: 0.1784\n",
      "Epoch 00039: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 48s 86ms/step - loss: 2.9025 - accuracy: 0.1784 - val_loss: 2.9013 - val_accuracy: 0.1784\n",
      "Epoch 40/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9028 - accuracy: 0.1783\n",
      "Epoch 00040: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 86ms/step - loss: 2.9028 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 41/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00041: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9023 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 42/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9026 - accuracy: 0.1784\n",
      "Epoch 00042: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9027 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 43/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9024 - accuracy: 0.1784\n",
      "Epoch 00043: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9024 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 44/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00044: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9023 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 45/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9027 - accuracy: 0.1784\n",
      "Epoch 00045: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9027 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 46/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784\n",
      "Epoch 00046: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 47/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9022 - accuracy: 0.1784\n",
      "Epoch 00047: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 48/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784\n",
      "Epoch 00048: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 49/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00049: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9024 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 50/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00050: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9023 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 51/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00051: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 52/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9022 - accuracy: 0.1784\n",
      "Epoch 00052: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 47s 85ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 53/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9022 - accuracy: 0.1784\n",
      "Epoch 00053: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 46s 82ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 54/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9024 - accuracy: 0.1784\n",
      "Epoch 00054: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 82ms/step - loss: 2.9023 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 55/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784\n",
      "Epoch 00055: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 56/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9025 - accuracy: 0.1784\n",
      "Epoch 00056: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 2.9025 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 57/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00057: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 2.9021 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9025 - accuracy: 0.1784\n",
      "Epoch 00058: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 2.9024 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 59/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784\n",
      "Epoch 00059: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 60/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9025 - accuracy: 0.1784\n",
      "Epoch 00060: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 2.9024 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 61/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784\n",
      "Epoch 00061: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9013 - val_accuracy: 0.1784\n",
      "Epoch 62/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9022 - accuracy: 0.1784\n",
      "Epoch 00062: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 2.9021 - accuracy: 0.1784 - val_loss: 2.9015 - val_accuracy: 0.1784\n",
      "Epoch 63/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9017 - accuracy: 0.1784\n",
      "Epoch 00063: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 64/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784\n",
      "Epoch 00064: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 65/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784\n",
      "Epoch 00065: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 66/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784\n",
      "Epoch 00066: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 67/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9022 - accuracy: 0.1784\n",
      "Epoch 00067: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 78ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 68/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00068: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 78ms/step - loss: 2.9023 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 69/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784\n",
      "Epoch 00069: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 2.9021 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 70/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784\n",
      "Epoch 00070: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 2.9021 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 71/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9022 - accuracy: 0.1784\n",
      "Epoch 00071: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 2.9021 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 72/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9022 - accuracy: 0.1784\n",
      "Epoch 00072: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 79ms/step - loss: 2.9023 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 73/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9016 - accuracy: 0.1784\n",
      "Epoch 00073: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 74/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784\n",
      "Epoch 00074: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 75/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784\n",
      "Epoch 00075: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 76/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784\n",
      "Epoch 00076: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 77/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784\n",
      "Epoch 00077: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 78/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00078: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 79/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00079: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9021 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 80/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00080: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 81/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9015 - accuracy: 0.1784\n",
      "Epoch 00081: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9016 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 82/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784\n",
      "Epoch 00082: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 83/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00083: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 84/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00084: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9022 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 85/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00085: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 86/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784\n",
      "Epoch 00086: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784\n",
      "Epoch 00087: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 88/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784\n",
      "Epoch 00088: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 89/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9017 - accuracy: 0.1784\n",
      "Epoch 00089: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 90/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9015 - accuracy: 0.1784\n",
      "Epoch 00090: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9015 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 91/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00091: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 92/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9014 - accuracy: 0.1784\n",
      "Epoch 00092: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 2.9015 - accuracy: 0.1784 - val_loss: 2.9015 - val_accuracy: 0.1784\n",
      "Epoch 93/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9023 - accuracy: 0.1784\n",
      "Epoch 00093: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9023 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 94/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784\n",
      "Epoch 00094: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9014 - val_accuracy: 0.1784\n",
      "Epoch 95/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784\n",
      "Epoch 00095: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 96/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00096: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 97/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9017 - accuracy: 0.1784\n",
      "Epoch 00097: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 98/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784\n",
      "Epoch 00098: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 99/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784\n",
      "Epoch 00099: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 44s 80ms/step - loss: 2.9018 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 100/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00100: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 101/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9016 - accuracy: 0.1784\n",
      "Epoch 00101: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 80ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 102/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00102: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 45s 81ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 103/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784 ETA: 0s - loss: 2.9022 - accuracy\n",
      "Epoch 00103: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 55s 100ms/step - loss: 2.9018 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 104/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00104: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 66s 119ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 105/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00105: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 58s 104ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9012 - val_accuracy: 0.1784\n",
      "Epoch 106/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784\n",
      "Epoch 00106: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 57s 102ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9005 - val_accuracy: 0.1784\n",
      "Epoch 107/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9017 - accuracy: 0.1784 - ETA: 39s - loss: 2.9 - ETA: 1s - loss: 2.9004 - accuracy - ETA: 0s - loss: 2.9\n",
      "Epoch 00107: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 56s 100ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 108/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9015 - accuracy: 0.1784\n",
      "Epoch 00108: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 58s 104ms/step - loss: 2.9016 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 109/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9016 - accuracy: 0.1784\n",
      "Epoch 00109: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 63s 113ms/step - loss: 2.9016 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 110/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9021 - accuracy: 0.1784 - ETA: 24s - loss: 2.9036 - - ETA: 22 - ETA: 19s - loss:  - ETA: 17s - loss - E\n",
      "Epoch 00110: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 60s 109ms/step - loss: 2.9020 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 111/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784 ETA: 5s - loss: 2.9001 - accuracy:  -\n",
      "Epoch 00111: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 61s 109ms/step - loss: 2.9018 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 112/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9016 - accuracy: 0.1784 ETA: 1s -\n",
      "Epoch 00112: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 65s 118ms/step - loss: 2.9016 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 113/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9014 - accuracy: 0.1784\n",
      "Epoch 00113: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 66s 119ms/step - loss: 2.9014 - accuracy: 0.1784 - val_loss: 2.9005 - val_accuracy: 0.1784\n",
      "Epoch 114/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9015 - accuracy: 0.1784\n",
      "Epoch 00114: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 67s 122ms/step - loss: 2.9015 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9013 - accuracy: 0.1784 ETA: 4s - loss: 2.9043 - accuracy: 0. - ETA: 4s - loss: - ETA\n",
      "Epoch 00115: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 63s 113ms/step - loss: 2.9014 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 116/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784 - ETA: 34s - loss: 2.9354 - accuracy: - ETA: 34s - loss: 2.9399 - accuracy: 0.16 - ETA: 34s - loss: 2.9315 - accuracy:  - ETA: 33s - loss: 2.9298 - accuracy: 0.1 - ETA: 33s - loss: 2.9172 - ETA: 32s - loss: 2.9137 - accurac - ETA: 32s - loss: 2.9091 - accuracy: 0.172 - ETA: 32s - loss: 2.9083 - accuracy: 0.1 - ETA: 32s - loss: 2.9095 - accuracy: 0.171 - ETA: 32s - loss: 2.9084 - accuracy: 0. - ETA: 31s - loss: 2.9097 - accuracy: 0.17 - ETA: 31s - loss: 2.9090 - accuracy: 0.17 - ETA: 31s - loss: 2.9066 - ETA: 30s - loss: 2.9012 - accurac - ETA: 29s - loss: 2.8978 - accuracy: 0.180 - ETA: 29s - loss: 2.8982 - accuracy: - ETA: 29s - loss: 2.8984 - accura - ETA: 28s - loss:  - ETA: 27s - loss: 2.9030 - accuracy: - ETA: 26s - loss: 2.9043 - accuracy: 0.18 - ETA: 26s - loss: 2.9051 - accuracy - ETA: 25s - loss: 2.9036 - ac - ETA: 25s -  - ETA: 23s - loss: 2.9038 - acc - ETA: 22s - loss: 2.9034 - accu - ETA: 21s - loss: 2.9044 - ETA: 20s - loss: 2.9056 - accuracy: 0.1 - ETA: 19s - loss: 2.9057 - accuracy: 0 - ETA: 16s - loss: 2.9 - ETA: 15s - loss: 2.9069 - accu - ETA: 14s - loss: 2.9070 - accuracy:  - ETA: 13s - loss: 2.9066 - accuracy: 0 - ETA: 13s - loss: 2 - ETA: 12s - loss: 2.9063 - accuracy: 0. - ETA: 11s - loss: 2.9063 - accuracy:  - ETA: 11s - loss: 2.9059 - accuracy:   - ETA: 7s - loss: 2.9044 - accuracy - ETA: 7s - loss: 2.9044 - accuracy: 0. - ETA: 7s - loss: 2.9043 - ac - ETA: 6s - l - E - ETA: 2s - loss: 2.9036 - accu - ETA: 2s - loss: 2.9036 - accuracy:  - ETA: 2s - loss: 2.9033 - accuracy: 0.17 - ETA - ETA: 0s - loss: 2.902 - ETA: 0s - loss: 2.9019 - accuracy: \n",
      "Epoch 00116: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 53s 96ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9010 - val_accuracy: 0.1784\n",
      "Epoch 117/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784 - ETA: 34s - loss: 2.9014 - accuracy: 0.17 - ETA: 34s - loss: 2.89 - ETA: 33s - loss: 2.8954 - accuracy: 0.17 - ETA: 32s - loss: 2.8957 - accuracy: - ETA: 32s - loss: 2.8912  - ETA: 31s - loss: 2.8852 - accuracy: 0.17 - ETA: 31s - loss: 2.8868 - acc - ETA: 30s - loss: 2.8876 - accura - ETA: 29s - loss: 2.8881 - accuracy: 0 - ETA: 29s - loss: 2.8894 - accuracy: 0. - ETA: 29s - loss: 2.8889 - accuracy: 0 - ETA: 28s - loss: 2.8890 - accura - ETA: 28s - los - ETA: 26s - loss: 2.8911 - accurac - ETA: 25s - loss: 2.8906 - - ETA: 24s - loss: 2.8913 - accuracy: 0.17 - ETA: 24s - loss: 2.8920 -  - ETA: 23s - loss: 2.8910 - accuracy:  - ETA: 22s - loss: 2.8903 - accuracy: 0. - ETA: 22s - loss: 2.8904 - accuracy: 0.180 - ETA: 22s - lo - ETA: 20s - loss: 2.8915 - accur - E - ETA: 17s - los - ETA: 15s - loss: 2.8975 - accuracy: 0.1 - ETA: 15s - loss: 2.8971 - accura - ETA: 14s - loss: 2.8982 - accuracy - ETA: 14s - loss: 2.8976 - accuracy: 0.17 - ETA: 14s - loss: 2.8980  - ETA: 12s - loss: 2.8990 - accuracy: 0 - ETA: 12s - loss: 2.8990 - accuracy: 0.1 - ETA: 12s - loss: 2.8994 - accuracy:  - ETA: 11s - loss: 2.8997 - a - ETA: 10s - loss: 2.8992 - accuracy: 0 - ETA: 10s - loss: 2.8990 - accuracy: 0. - ETA: 10s - loss: 2.8993 - accu - ETA: 9s - loss: - ETA: 8s - loss: 2.9007 - accuracy: 0. - ETA: 8s - loss: 2.9003 - accuracy - ETA: 8s - loss: 2.9002  - ETA: 7s - loss: 2.8988 - accuracy: 0.17 - ETA: 7s - loss: 2.8990 - accuracy: 0. - ETA: 7s - loss: 2.8988 - accuracy: 0. - ETA: 7s - ETA: 5s - loss: 2.9004 - accuracy: 0.17 - ETA: 4s - loss: 2.9007 -  - ETA: 4s - loss: 2.9012 - accuracy - ETA: 4s - loss: 2.901 - ETA: 3s - loss: 2.9002 - accuracy: 0. - ETA: 3s - ETA: 2s - loss: 2.9016 - accura - ETA: 1s - loss: 2.9017 - ac - ETA: 1s - loss: 2.9015 - accuracy: 0. - ETA: 1s - loss: 2.9018 -  - ETA: 0s - loss: 2.9015 - accu - ETA: 0s - loss: 2.9015 - accu\n",
      "Epoch 00117: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 53s 95ms/step - loss: 2.9018 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 118/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1785 - ETA: 33s - loss: 2.9228 - accuracy: 0.1 - ETA: 33s - loss: 2.9211 - accuracy:  - ETA: 32s - loss: 2.9233 - accuracy: 0.177 - ETA: 32s - loss: 2.9218 - accuracy: 0 - ETA: 32s - loss: 2.9 - ETA: 30s - loss: 2.9250 - accuracy: 0.17 - ETA: 30s - loss: 2.9248 - accuracy: 0.173 - ETA: 30s - loss: 2.9240 - a - ETA: 29s - loss: 2.9264 - accuracy - ETA: 29s - loss - ETA: 27s - loss: 2.92 - ETA: 23s - loss: 2.9119 - accuracy: 0.17 - ETA: 23s - loss: 2.9116 -  - ETA: 22s - loss: 2.9119 - accuracy:  - ETA: 21s - loss: 2.9119 - accuracy:  - ETA: 21s - loss: 2.9093 - accuracy: 0. - ETA: 20s - loss: 2.9105 - accuracy: - ETA: 20s - loss: 2.9105 - a - ETA: 19s  - ETA: 17s - loss: 2.9037 - accuracy - ETA: 16s - loss: 2.9037 - accura - ETA: 16s - loss: 2. - ETA: 14s - loss: 2.9033 - accuracy: 0. - ETA: 14s - loss: 2.9026 - acc - ETA: 13s - loss: 2.9043 - accur - ETA: 12s - loss: 2.9051 - accuracy - ETA: 12s - loss: 2.9053 - accuracy: 0 - ETA: 11s - loss: 2.9052 - accuracy - ETA: 11s - loss: 2.9040 - accuracy: 0.17 - ETA: 1 - ETA: 9s - loss: 2.9051  - ETA: 8s - loss: 2.9050 -  - ETA: 8s - l - ETA: 5s - loss: 2.903 - ETA: 5s - loss: 2.9048 - ac - ETA:  - ETA: 3s - loss: 2.9042 - accuracy:  - ETA: 3s - loss: - ETA: 2s - loss: 2.9024 - accuracy:  - ETA: 2s - loss: 2.9022 - accuracy: 0.17 - ETA: 2s - loss: 2.9023 - accuracy: 0.17 - ETA: 2s - loss: 2.9025 - accura - ETA: 1s - loss: 2.9020 - accu - ETA: 1s - loss: 2.9020 - accu - ETA: 0s - los\n",
      "Epoch 00118: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 53s 96ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 119/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9017 - accuracy: 0.1784 - ETA: 35s - loss: 2.9093 - accuracy: 0.1 - ETA: 35s - loss: 2.8798 - accuracy: 0. - ETA: 35s - loss: 2.9095 - ETA: 34s - loss: 2.9238 - accuracy - ETA: 33s - loss: 2.91 - ETA: 32s - loss: 2.9131 - accuracy:  - ETA: 31s - loss: 2.9102 - acc - ETA: 31s - loss: 2.9165 - accuracy: 0.17 - ETA: 30s - - ETA: 28s - loss: 2.9123 - accuracy: 0.177 - ETA: 28s - loss: 2.9126 - accuracy: 0.177 - ETA: 28s - loss: 2.9129 - a - ETA: 27s - loss: 2.9142 - accurac - ETA: 21s - loss: 2. - ETA: 19s - loss: 2.9037 - accuracy: 0.178 - ETA: 19s - loss: 2.9038 - accuracy: 0. - ETA: 19s - l - ETA: 17s - lo - ETA: 15s - loss: 2.9003 - accuracy: 0.180 - ETA: 15s - loss: 2.9005 - accuracy: 0.1 - ETA: 15s - loss: 2.8997 - accurac - ETA: 14s - loss: 2.9010 - accur  - ETA: 11s - loss: 2.9040 - accurac - ETA - ETA: 8s - loss: - ETA: 5s - loss: 2.9024 - ac - ETA: 4s - loss: 2.9 - E - ETA: 1s - loss: 2.9014 - accuracy: 0.17 - ETA: 1s - loss: 2.9016 - accuracy: 0. - ETA: 1s - loss: - ETA: 0s - loss: 2.9016 - accuracy: \n",
      "Epoch 00119: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 53s 96ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 120/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9014 - accuracy: 0.1784 - ETA: 36s - loss: 2.8368 -  - ETA: 36s - loss: 2.8460 - accuracy: 0 - ETA: 36s - loss: 2.8535 - ac - ETA: 34s - loss: 2.8624 - accuracy: 0.185 - ETA: 34s - loss: 2.8654 - accuracy: 0. - ETA: 34s - loss: 2.8638 - accuracy: 0. - ETA: 33s - loss: 2.875 - ETA: 32s - loss: 2.8897 - accuracy: 0 - ETA: 32s - loss: 2.8879 - ETA: 31s - loss: 2.8853 - accuracy: 0.1 - ETA: 30s - loss: 2.8866 - accuracy: 0.1 - ETA: 30s - loss: 2.8860 - accuracy: - ETA: 30s - loss: 2.88 - ETA: 28s - loss: 2.8792 - accuracy: 0 - ETA: 25s - loss: 2.8887 - accuracy - ETA: 24s - loss: 2.8888 - a - ETA: 23s - loss: 2.8907 - accuracy: 0.180 - ETA: 23s - loss: 2.8914 - accuracy: 0 - ETA: 23s - loss: 2.8930 - accuracy: 0.1 - ETA: 23s - loss: 2.8936 - accuracy: 0.18 - ETA: 22s - loss: 2.8924 - accur - ETA: 22s - loss: 2.8935 - accuracy:  - ETA: 21s - loss: 2.8941 - accurac - ETA: 21s - loss: 2.8938 - accurac - ETA: 20s - loss: 2.8942 - accu - ETA: 19s - loss: 2.8964 - accuracy - ETA: 19s - loss: 2.8975 - accurac - ETA: 18s - loss: 2.8972 - accuracy: 0 - ET - ETA: 15s - loss:  - ETA: 13s - loss: 2 - ETA: 12s - loss: 2.8966 - accurac - ETA: 7s - loss: 2.8995 - ac - ETA: 7s - loss: 2.8996  - ETA: 5s - loss: 2.8988 - accu - ETA: 4s - - E - ETA: 2s - loss: 2.8984  - ETA: 0s - loss: 2.9012 - accuracy - ETA: 0s - loss: 2.9014 - accuracy: \n",
      "Epoch 00120: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 53s 96ms/step - loss: 2.9015 - accuracy: 0.1784 - val_loss: 2.9019 - val_accuracy: 0.1784\n",
      "Epoch 121/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1784 - ETA: 29s - loss: 2.8978 - accuracy: 0 - ETA: 29s - loss: 2.8976 - acc - ETA: 28s - loss: 2.8 - ETA: 27s - loss: 2.9004 - accurac - ETA: 26s - loss: 2.8990 - accuracy: 0.17 - ETA: 26s - loss: 2.8992 - accuracy: 0. - ETA: 26s - loss: 2.8990 -  - ETA: 25s - l - ETA: 23s - loss: 2.8963 - accuracy: 0. - ETA: 22s - loss: 2.8964 - accuracy: 0.18 - ETA: 22s - loss: 2.8966 - accuracy: 0.18 - ETA: 22s - loss: 2.8963 - accuracy:  - ETA: 22s - loss: 2.8969 - accura - ETA: 21s - loss: 2.8963 - accu - E - ETA: 17s - loss: 2.8974 - accuracy:  - ETA: 17s - loss: 2.8967 - acc - ETA: - ETA: 14s - loss: 2.8970 - accuracy: 0.1 - ETA: 13s - loss: 2.8973 - acc - ETA: 13s - loss: 2.8978  - ETA: 11s - loss: 2.8968 - accuracy: - ETA: 11s - los - ETA: 9s - loss: 2.897 - ETA: 7s - loss: - ETA: 6s - loss: 2.8993 - accu - ETA: 3s - loss: 2.9 - ETA: 2s - loss: 2.9018 - accuracy: 0. - ETA - ETA: \n",
      "Epoch 00121: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 53s 96ms/step - loss: 2.9018 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 122/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9014 - accuracy: 0.1784 - ETA: 35s - loss: 2.8939 - accur - ETA: 35s - loss: 2.9228 - accuracy: - ETA: 3 - ETA: 32s - loss: 2.9142 - accuracy: 0 - ETA: 32s - loss: 2.9155 - accura - ETA: 31s - loss: 2.9188 - accuracy: 0. - ETA: 31s - loss: 2.91 - ETA: 30s - loss: 2.9125 - accuracy: 0.1 - ETA: 29s - loss: 2.9130 - accur - ETA: 29s - loss: - ETA: 27s - loss: 2.907 - ETA: 26s - loss: 2.9029 - accuracy:  - ETA: 25s - loss: 2.9032 - accuracy: 0.175 - ETA: 25s - loss: 2.9032 - acc - ETA: 24s - loss: 2.9033 - accuracy - ET - ETA: 21s - loss: 2.9068 - accuracy: 0 - ETA: 21s - loss: 2.9 - ETA: 19s - loss: 2.9060 - accuracy: 0.1 - ETA: 19s - loss: 2.9070 - accuracy: 0.17 - ETA: 19s - loss: 2.9069 - accuracy: 0. - ETA: 19s - loss: 2.9070 - ETA: 18s - loss: 2.9050 - accuracy: 0.1 - ETA: 17s - loss: 2.9052 - accuracy - ETA: 17s - loss: 2.9039 - accu - ETA: 16s - loss: 2.9041 - accur - ETA: 15s - loss: 2.9045 - a - ETA: 14s - loss: 2.9048 - accu - ETA: 14s - loss: 2.9029 - accurac - ETA: 13s - loss: 2.9018 - a - ETA: 12s - loss: 2.9025 - a - ETA: 7s - loss: 2.9040 - accuracy - ETA: 7s - loss: 2.9041 - ac - ETA: 7s - loss: 2.9 - ETA: 6s - loss: 2.9026 - accuracy - ETA: 4s - loss: 2.9023 - accuracy - ETA: 4s - loss: 2.9025 - accuracy: 0. - ETA: 4s - loss: 2.9027 -  - ETA: 3s - loss: 2.9023 - accuracy: 0.17 - ETA: 3s - loss: 2.9023 - accuracy: 0. - ETA: 3s - los - ETA: 2s - loss: 2.9016 - ac - ETA: 2s - loss: 2.9019 - accuracy:  - ETA: 1s - loss: 2.901 - ETA: 1s - loss: 2.9016 - accuracy - ETA: 1s - l\n",
      "Epoch 00122: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 53s 96ms/step - loss: 2.9014 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 123/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784 - ETA: 35s - loss: 2.9110 - acc - ETA: 34s - loss: 2.9054 - accurac - ETA: 33s - loss: 2.9061 - accuracy: - ETA: 32s - loss: 2.9087 - accuracy: 0.1 - ETA: 32s - loss: 2.9097 - accur - ETA: 31s - loss: 2.9129  - ETA: 30s - loss: 2.9073 - accura - ETA: 30s - loss: 2.9062 - accurac - ETA: 29s - loss: 2.9074 -  - ETA: 28s - loss: 2.9030 - ac - ETA: 27s - loss: 2.9027 - accurac - ETA: 26s - loss: 2. - ETA: 25s - loss: 2.9000 - accuracy: 0.178 - ETA: 25s - loss: 2.9003 - accuracy: - ETA: 24s - loss: 2.9009 - accuracy: 0 - ETA: 24s - loss:  - ETA: 22s - loss: 2.8999 - accuracy:  - ETA: 22s - loss: 2.9015 - accuracy: 0. - ETA: 22s - lo - ETA: 20s - loss: 2.8996 - accuracy: 0. - ETA: 20s - loss: 2 - ETA: 18s - loss: 2.9017 - accuracy: 0.1 - ETA: 18s - loss: 2.9019 - accuracy: 0.180 - ETA: 18s - loss: 2.9017 - accuracy: 0.1 - ETA: 18s - loss: 2.9013 - accuracy: 0. - ET - ETA: 15s - loss: 2.9011 - accuracy: 0 - ETA: 15s - loss: 2.9015 - accuracy: 0.180 - ETA: 14s - loss: 2.9010 - accuracy - ETA: 14s - loss: 2.9014 - accuracy: 0. - ETA: 14s - los - ETA: 12s - - ETA - ETA: 4s - l - ETA: 3s - loss: 2.9033 - accura - ETA: 3s - - ETA: 1s - loss: 2.9025  - ETA: 1s - loss: 2.9 - ETA: 0s - loss: 2.9022 \n",
      "Epoch 00123: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 53s 96ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 124/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9016 - accuracy: 0.1784 - ETA: 35s - loss - ETA: 34s - loss: 2.9001  - ETA: 32s - loss: 2.8984 - accur - ETA: 32s - loss: 2.8942 - accuracy: 0.1 - ETA: 31s - loss: 2.8936 - accuracy: 0. - ETA: 3 - ETA: 29s - loss: 2.8948 - accura - ETA: 28s - loss: 2.8981 - accuracy: 0.1 - ETA: 28s - loss: 2.8974 - accuracy: 0.1 - ETA: 28s - loss: 2.8980 - acc - ETA: 27s - loss: 2.8996 - ac - ETA: 23s - loss: 2.9008 - a - ETA: 22s - loss: 2.9011 - accu - ETA: 21s - loss: 2.9000 - accuracy - ETA: 21s - loss: 2.8996 - accuracy:  - ETA: 20s - loss: 2.9008 - accuracy: 0.17 - ETA: 20s - loss: 2.9005 - accuracy: 0.1 - ETA: 20s - loss: 2. - ETA: 19s - loss: 2.8980 - accuracy: 0.1 - ETA: 18s - loss: 2.8990 - accuracy: - ETA: 18s - loss: 2.8987 - accur - ETA: 17s - loss: 2.8985 - accur - ETA: 17s - loss: 2.8980 - accurac - ETA: 16s - loss: 2.8982 - accur - ETA: 15s - loss: 2.8993 - accuracy: 0.1 - ETA: 15s - loss: 2.8994 - accuracy: 0.1 - ETA: 15s - loss: 2.9000 - ac - ETA: 14s - loss: 2.8996 - accuracy - ETA: 13s - loss: 2.9010 - accuracy - ETA: 13s - loss: 2. - ETA: 11s  - E - ETA: 8s - loss: 2.901 - ETA: 7s - loss: 2.9013 - accuracy: 0. - ETA: 7s - loss: 2.901 - ETA: 6s - loss: 2.9010 - accura - ETA: 6s - loss: 2.9 - ETA: 5s - loss: 2 - - ETA: 3s - loss: 2.9003 - accuracy: 0. - ETA: 3s - l - ETA:  -\n",
      "Epoch 00124: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 53s 96ms/step - loss: 2.9016 - accuracy: 0.1784 - val_loss: 2.9008 - val_accuracy: 0.1784\n",
      "Epoch 125/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/555 [============================>.] - ETA: 0s - loss: 2.9016 - accuracy: 0.1784 - ETA: 37s - loss: 2.8938 - accurac - ETA: 36s - loss: 2.890 - ETA: 34s - loss: 2.9071 - accuracy: - ETA: 33s - loss: 2.9035 - accuracy:  - ETA: 33s - loss: 2.9065 - a - ETA: 29s - loss: 2.9014 - accuracy: 0. - ETA: 29s - loss:  - ETA: 24s - loss: 2.8989 - accuracy: - ETA: 24s - loss: 2.8996 - accuracy:  - ETA: 23s - loss: 2.9006 - - ETA: 22s - loss: 2.9037 - accur - ETA: 21s - loss: 2.9035 - accuracy: 0. - ETA: 21s - loss: 2.9040 - accura - ETA: 20s - loss: 2.9048 - accuracy: 0.17 - ETA: 20s - loss: 2.9058 - accuracy: 0 - ETA: 20s - loss: 2.9051 - accuracy: 0.1 - ETA: 20s - loss: 2.9047 - accuracy: 0.175 - ETA: 20s - loss: 2.9045 - acc - ETA: 19s - loss: 2.9050 - accuracy: 0.17 - ETA: 19s - loss: 2.9052 - accuracy: 0 - ETA: 18s - loss: 2.9 - ETA: 17s - loss: 2.9087 - accuracy:  - ETA: 16s - loss: 2.9076 - accu - ETA: 16s - loss: 2.9069 - accuracy: 0. - ETA: 15s - loss: 2.9069 - - ETA: 14s - loss: 2.9054 - accuracy: 0.1 - ETA: 14s - loss: 2.9054 - accuracy: - ETA: 14s - loss: 2.9042 - accuracy: 0 - ETA: 13 - ETA: 11s - loss: 2.9033 - accuracy: - ETA: 10s - loss: 2.9025 - accu - ETA: 10s - loss: 2. - ETA: 9s - loss: 2.9034 - accuracy: 0.17 - ETA: 9s - loss: 2 - ETA: 8s - loss: 2.9 - ETA: 4s - loss: 2.9006 - accura - ETA: 4s - loss: 2.9018 - accuracy - - - ETA: 1s - loss: 2.9020 - accuracy: 0. - ETA\n",
      "Epoch 00125: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 54s 97ms/step - loss: 2.9016 - accuracy: 0.1784 - val_loss: 2.9007 - val_accuracy: 0.1784\n",
      "Epoch 126/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9013 - accuracy: 0.1784 - ETA: 21s - loss: 2.9024 - accura - ETA: 20s - loss: - ETA: 18s - l - ETA: 16s - loss: 2.9036 - a - ETA: 15s - loss - ETA: 13s - loss: 2.9017 - accuracy: 0 - ETA: 12s - loss: 2.9022 - accuracy: 0.1 - ETA: 12s - loss: 2.9022 -  - ETA: 5s - loss: 2.9009 - accu - ETA: 5s - loss: 2.9005 - accura - ETA: 4s - los - ETA: 3s - loss: 2.9024 - accuracy:  - ETA: 3s - loss: 2.9027 - accura - ETA - ETA: 1s - loss: 2.9014 - accuracy: 0.17 - ETA: 1s - loss: 2.9014 - accuracy\n",
      "Epoch 00126: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 56s 101ms/step - loss: 2.9013 - accuracy: 0.1784 - val_loss: 2.9011 - val_accuracy: 0.1784\n",
      "Epoch 127/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784 - ETA: 35s - loss: 2.8674 - accuracy: 0.1 - ETA: 34s - ETA: 33s - loss: 2.8847 - accuracy: 0.18 - ETA: 33s - loss: 2 - ETA: 32s - loss: 2.9002 - accuracy: 0.174 - ETA: 32s - loss: 2.9019 - accuracy: - ETA: 31s - loss: 2.9011 -  - ETA: 30s - loss: 2.9011 - a - ETA: 29s - loss: 2.9030 - accuracy: 0.17 - ETA: 29s - loss: 2.9040 - ETA: 28s - loss: 2.9036 - accuracy: 0.176 - ETA: 28s - loss: 2.9031 - accur - ETA: 27s - loss: 2.9047 - - ETA: 26s - loss: 2.9049 - a - ETA: 25s - loss: 2.9022 - accuracy: 0. - ETA: 24s - loss: - ETA: 23s - loss:  - ETA: 21s - loss: 2.9010 - accuracy: 0 - ETA: 21s - loss: 2.9016 - accuracy: 0.17 - ETA: 21s - loss: 2.9018 - accuracy: 0.17 - ETA: 21s - loss: 2.9021 - accuracy: 0.1 - ETA: 20s - loss: 2.9032 - accuracy: 0. - ETA: 20s - loss: 2.9042 - - ETA: 19s - loss: 2.9029 - accura - ETA: 12s - loss: 2.9039 - accuracy: 0 - ETA: 1 - ETA: 7s - loss: 2.9014 - accu - ETA: 6s - loss: 2.9017 - accuracy:  - ETA: 6s - loss: 2.9017 - accuracy: 0. - ETA: 6s - l - ETA: 5s - - ETA: 4s - ETA: 1s - los - ETA: 0s - loss: 2.9024 - accuracy:  - ETA: 0s - loss: 2.9022 - ac\n",
      "Epoch 00127: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 54s 97ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 128/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9019 - accuracy: 0.1783 - ETA: 36s - loss: 2.8159 - accuracy: 0.1 - ETA: 36s - l - ETA: 34s - loss: 2.8743 - accuracy: 0 - ETA: 34s - loss: 2.8756 - - ETA: 33s - loss: 2.8800 - accura - ETA: 32s - loss: 2.8903 - - ETA: 31s - loss: 2.9018 - accuracy:  - ETA: 30s - loss: 2.9014 - accuracy: 0.1 - ETA: 30s - loss: 2.9006 - accur - ETA: 29s - loss: 2.9047 - accuracy: - ETA: 29s  - ETA: 27s - loss: 2.9028 - ETA: 25s - loss: 2.9038 - accuracy: 0.17 - ETA: 25s - loss: 2.9054 - a - ETA: 24s - loss:  - ETA: 22s - loss:  - ETA: 21s - loss: 2.9024 - accur - ETA: 20s - loss: 2.9026 - accuracy: - ETA:  - ETA: 17s - loss: 2.9019 - accu - ETA: 17s - loss: 2.9030 - accuracy: 0 - ETA: 16s - loss: 2.9 - ETA: 15s - loss: 2.9032 - accuracy: 0.176 - ETA: 15s - loss: 2.9032 - accuracy: 0.17 - ETA: 15s - loss: 2.9031 - accur - ETA: 14s - loss: 2.9032 - accuracy:  - ETA: 13s - loss: 2.9028 - accuracy: - ETA: 13s - loss: 2.9024 - accuracy:  - ETA: 13s - loss: 2.9032 - accur - ETA: 12s - loss: 2.9 - ETA: 10s - loss: 2.9026 - accuracy - ETA: 10s - loss: 2.9021 - accuracy: 0 - ETA: 10s - loss:  - ETA: 9s - loss: 2.9009 - accuracy: 0. - ETA: 9s - loss: 2.9004 - accuracy - ETA: 8s - loss: 2.9006 -  - ETA: 8s - loss: 2.9 - ETA: 7s - ETA: 6s - loss: 2.9024 - accuracy - ETA: 6s - loss: 2.9027 - ac - ETA: 5s - loss: 2.902 - ETA: 4s - loss: 2.9011 - accura - ETA: 4s - loss: 2.9007 - accuracy: 0.17 - ETA: 4s - loss: 2.9 - ETA:  - ETA: 2s - loss: 2.9011 - accuracy:  - - ETA: 1s - loss: 2.9016 - accuracy: 0.17 - ETA: 0s - loss: 2.9018 - accu - ETA: 0s - loss: 2.9016 - accuracy:  - ETA: 0s - loss: 2.9018 - accura - ETA: 0s - loss: 2.9016 - accuracy: 0.1784\n",
      "Epoch 00128: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 54s 97ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 129/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9015 - accuracy: 0.1784 - ETA: 34s - lo - ETA: 35s - loss: 2.8 - ETA: 30s - loss: 2.8952 -  - ETA: 29s - loss: 2.8954 - accuracy: 0.18 - ETA: 28s - loss: 2.8971 - accuracy: - ETA: 28s - loss: 2.8967 - accuracy: 0. - ETA: 28s - loss: 2.8961 - accurac - ETA: 27s - loss: 2.8981 - accura - ETA: 26s - loss: 2.8960 - accura - ETA: 26s - loss: 2.8 - ETA: 24s - loss: 2 - ETA: 23s - loss: 2.8938 - acc - ETA: 22s - loss: 2.8934 - accuracy: 0.1 - ET - ETA: 19s - loss: 2.8966 - a - ETA: 18s - loss: 2.8958 - a - ETA: 17s - loss: 2.8959 - accuracy: 0.179 - ETA: 17s - loss: 2.8957 - accu - ETA: 16s - loss: 2.8977 - accuracy: 0 - ETA: 16s - lo - ETA: 14s - loss: 2.8976 - a - ETA: 13s - loss: 2.8987 - ETA: 12s - loss: 2.8981 - ac - ETA: 11s - - ETA: 9s - loss: 2.9016 - accura - ETA: 9s - loss: 2.9012 - accuracy: 0.17 - ETA: 9s - loss: 2.9014  - ETA: 8s - loss: 2.902 - ETA: 6s - loss: 2.9027 - accura - ETA: 6s - ETA: 5s - loss: 2.9028 - accuracy:  - ETA: 4s - loss: - ETA: 3s - loss: 2.9014 - accura - ETA: 3s - loss: 2.9006  - ETA: 3s - los - ETA: 2s - loss: 2.9002 - accuracy: 0. - ETA: 1s - loss: 2.9003 - accuracy - ETA: 0s - loss: 2.9016 - accuracy: 0. - ETA: 0s - loss: 2.9015 - accuracy: 0.1784\n",
      "Epoch 00129: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 54s 97ms/step - loss: 2.9015 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 130/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.17831- ETA: 36s - loss: 2.8673 - accuracy:  - ETA: 36s - loss: 2.8953 - accura - ETA: 35s - loss: 2.8911 - accuracy: 0 - ETA: 35s - loss: 2.8967  - ETA: 35s - - ETA: 32s - loss: 2.8869 - accuracy:  - ETA: 31s - loss: 2.8836 - ac - ETA: 27s - loss: 2.9 - ETA: 26s - loss: 2.9059 - accuracy: 0.17 - ETA: 26s - - ETA: 23s - loss: 2.9049 - accuracy: 0.17 - ETA: 23s - loss: 2.9045 - accuracy: 0.17 - ETA: 23s - loss: 2.9061 - - ETA: 22s - loss: 2.9082 - accuracy: 0.1 - ETA: 22s - loss: 2.9092 - a - ETA: 18s - loss:  - ETA: 16s - loss: 2.9036 - accuracy: - ETA: 16s - loss: - ETA: 14s - loss: 2.9 - ETA: 12s - loss: - ETA: 11s - loss: 2.9017 - acc - ETA: 10s - loss: 2.9024 - accuracy: 0 - ETA: 9s -  - ETA: 8s - loss: 2.9 - ETA: 8s - l - ETA: 1s - loss: 2.9024 - accuracy - ETA: 1s - los\n",
      "Epoch 00130: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 54s 97ms/step - loss: 2.9016 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 131/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9020 - accuracy: 0.1784 - ETA: 34s - loss: 2.9283 - accuracy: 0 - ETA: 34s - loss: 2.9246 - - ETA: 32s - loss: 2.9221 - acc - ETA: 32s - loss - ETA: 30s - loss: 2.91 - ETA: 26s - loss: 2.9181 - ETA: 24s - loss: 2.9166 - accuracy:  - ETA: 24s - loss: 2.9157 - accuracy: 0.176 - ETA: 24s - loss: 2.9158 - accuracy: - ETA: 24s - loss: 2.9150  - ETA: 22s - loss: 2.9139 - accuracy: 0 - ETA: 22s - loss: 2.9141 - accuracy: 0. - ETA: 22s - loss: 2.9135 - accuracy: 0.175 - ETA: 22s - loss: 2.9138 - ac - ETA: 21s - loss: 2.9133 - accuracy - ETA: 20s - loss: 2.9134 - accuracy: 0 - ETA: 20s - loss: 2.9131 - accuracy:  - ETA: 13s - loss: 2 - ETA: 12s - loss: 2.9049 - accuracy:  - ETA: 11s - loss: 2.9045 - accuracy: - ETA: 11s - loss: 2.9045 - accuracy: 0.1 - ETA: 11s - loss: 2.9047 - accurac - ETA: 10s - loss: 2.9050 - accuracy: 0.1 - ETA: 10s - loss - ETA: 9s - loss: 2.9042 - accuracy: 0.17 - ETA: 9s - loss: 2.9043 - accu - ETA: 8s - loss: 2.9033 - accuracy: 0. - ETA: 7s - loss: 2.9038 - ac - ETA: 6s - los - ETA: 1s - loss: 2.9 - ETA: 0s - loss: 2.902\n",
      "Epoch 00131: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 54s 97ms/step - loss: 2.9019 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 132/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9016 - accuracy: 0.1783 - ETA: 24s - loss: 2.9078 - accuracy: 0.17 - ETA: 24s - loss - ETA: 22s - loss: 2.9107 - accuracy - ETA: 22 - ETA: 20s - loss: 2.9114 - accuracy: 0 - ETA: 19s - loss: - ETA: 17s - loss: 2.9121 - accura - ETA: 17s - loss: 2.9102 - accuracy: 0.175 - ETA: 17s - loss: 2.9099 - a - ETA: 12s - loss - ETA: 10s - loss: 2.9061 - a - ETA: 9s - loss: 2.9052 - accu - ETA: 9s - loss: 2.9045 -  - ETA: 8s - loss: 2.9050 - accura - ETA: 8s - loss: - ETA: 7s - loss: 2.9043 - accuracy - ETA - ETA: 2s - loss: 2.9019 - accuracy: 0.17 - ETA: 2s - loss: 2.9018 - accuracy:  - ETA: 2s - loss: 2.9019 -  - ETA: 2s - loss: - ETA: 1s - loss: 2 - ETA: 0s - loss: 2.9021 - accura\n",
      "Epoch 00132: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 56s 100ms/step - loss: 2.9016 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 133/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9017 - accuracy: 0.1784 - ETA: 36s - loss: 2.9042 - accuracy:  - ETA: 35s - loss: 2.9052 - accu - ETA: 34s - loss: 2.9010 - acc - ETA:  - ETA: 31s - loss: 2.8970 - accuracy - ETA: 30s - loss: 2.8939 - accura - ETA: 30s - loss: 2.8986 - accurac - ETA: 29s - loss:  - ETA: 24s - loss: 2.8936 - accuracy: 0.1 - ETA: 24s - loss: 2.8922 - accura - E - ETA: 21s - loss: 2.8963 - accuracy: - ETA: 20s - loss: 2.8962 - accura - ETA: 20s - loss: 2.8962 - accuracy: 0 - ETA: 19s - loss: 2.8978 - accuracy: 0.18 - ETA: 19s - loss: 2.8976 - accuracy:  - ETA: 19s - loss: 2.8978 - accuracy - ETA - ETA: 16s - loss: 2.8972 - accuracy: 0.181 - ETA: 16s - loss: 2.89 - ETA: 14s - loss: 2.8988 - accuracy: 0.1 - ETA: 14s - loss: 2.8981 -  - ETA: 13s - loss: 2.9005 - accuracy:  - ETA: 12s - loss: 2.9000 - accuracy: 0.18 - ETA: 12s - lo - ETA: 2s - loss: 2.901 - ETA: 2s - loss: 2.9015 - accuracy: 0. - ETA: 1s - loss: 2.9018 - ac\n",
      "Epoch 00133: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 56s 100ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9005 - val_accuracy: 0.1784\n",
      "Epoch 134/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9018 - accuracy: 0.1784\n",
      "Epoch 00134: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 61s 109ms/step - loss: 2.9017 - accuracy: 0.1784 - val_loss: 2.9006 - val_accuracy: 0.1784\n",
      "Epoch 135/1000\n",
      "555/555 [==============================] - ETA: 0s - loss: 2.9016 - accuracy: 0.1784\n",
      "Epoch 00135: accuracy did not improve from 0.17843\n",
      "555/555 [==============================] - 18708s 34s/step - loss: 2.9016 - accuracy: 0.1784 - val_loss: 2.9009 - val_accuracy: 0.1784\n",
      "Epoch 136/1000\n",
      "554/555 [============================>.] - ETA: 0s - loss: 2.9016 - accuracy: 0.1784"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=1000, batch_size=100, verbose=True, callbacks=callbacks_list, validation_data=(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['loss', 'val_loss', 'val_accuracy', 'accuracy']\n",
    "\n",
    "simple_graph(model, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recrear el modelo desde un archivo hd5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './Checkpoints-V2/weights-improvement-epoch_-loss_-accuracy_.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacer Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = generar_semilla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_caracteres(pattern)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oO1t_J7g8Bio",
    "b0beyHLj_Idb"
   ],
   "name": "P3T1_RNN_Leer_Cuento.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
